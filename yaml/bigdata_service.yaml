apiVersion: v1
items:
- apiVersion: v1
  kind: Template
  labels:
    app: alluxiocluster
    template: alluxio-ha-persistent
  metadata:
    annotations:
      description: Create a alluxio HA cluster, with persistent storage.
      iconClass: icon-database
      openshift.io/display-name: AlluxioCluster-HA (Persistent)
      tags: database,alluxio
    creationTimestamp: 2017-10-17T08:43:49Z
    name: alluxio-ha-persistent
    namespace: openshift
    resourceVersion: "27449294"
    selfLink: /oapi/v1/namespaces/openshift/templates/alluxio-ha-persistent
    uid: 4b98dece-b317-11e7-9b95-ecf4bbe2f144
  objects:
  - apiVersion: v1
    kind: Service
    metadata:
      labels:
        name: ${MASTER_NAME}
      name: alluxio-master
    spec:
      clusterIP: None
      ports:
      - name: masterport
        port: 19998
        protocol: TCP
        targetPort: 19998
      - name: masterwebport
        port: 19999
        protocol: TCP
        targetPort: 19999
      selector:
        name: ${MASTER_NAME}
  - apiVersion: v1
    kind: Service
    metadata:
      labels:
        name: ${WORKER_NAME}
      name: alluxio-worker
    spec:
      clusterIP: None
      ports:
      - name: workerport
        port: 29998
        protocol: TCP
        targetPort: 29998
      - name: workerdataport
        port: 29999
        protocol: TCP
        targetPort: 29999
      - name: workerwebport
        port: 11012
        protocol: TCP
        targetPort: 11012
      selector:
        name: ${WORKER_NAME}
  - apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      annotations:
        volume.beta.kubernetes.io/storage-class: nfs
        volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/glusterfs
      name: ${MASTER_NAME}
    spec:
      accessModes:
      - ReadWriteMany
      resources:
        requests:
          storage: ${PV_SIZE}Gi
  - apiVersion: apps/v1beta1
    kind: StatefulSet
    metadata:
      labels:
        name: ${MASTER_NAME}
      name: ${MASTER_NAME}
    spec:
      replicas: 1
      serviceName: alluxio-master
      template:
        metadata:
          labels:
            name: ${MASTER_NAME}
        spec:
          affinity:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: name
                      operator: In
                      values:
                      - ${MASTER_NAME}
                  topologyKey: kubernetes.io/hostname
                weight: 100
          containers:
          - args:
            - master
            env:
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: ALLUXIO_MASTER_HOSTNAME
              value: $(NODE_NAME).alluxio-master.$(NAMESPACE).svc.cluster.local
            - name: ALLUXIO_UNDERFS_ADDRESS
              value: /underStorage
            - name: ALLUXIO_MASTER_PORT
              value: "19998"
            - name: ALLUXIO_MASTER_WEB_PORT
              value: "19999"
            - name: ALLUXIO_MASTER_WORKER_TIMEOUT_MS
              value: "40000"
            - name: ALLUXIO_ZOOKEEPER_ENABLED
              value: "true"
            - name: ALLUXIO_ZOOKEEPER_ADDRESS
              value: ${SERVER_ZOOKEEPER_CONNECT}
            - name: ALLUXIO_MASTER_JOURNAL_FOLDER
              value: /underStorage/DoNotDelete/journal
            image: ${ALLUXIO_IMAGE}
            imagePullPolicy: Always
            livenessProbe:
              failureThreshold: 3
              initialDelaySeconds: 90
              periodSeconds: 10
              successThreshold: 1
              tcpSocket:
                port: 19998
              timeoutSeconds: 5
            name: ${MASTER_NAME}
            ports:
            - containerPort: 19998
              protocol: TCP
            - containerPort: 19999
              protocol: TCP
            resources:
              limits:
                cpu: ${MASTER_CPU_LIMIT}
                memory: ${MASTER_MEM_LIMIT}G
              requests:
                cpu: 200m
                memory: 200M
            securityContext:
              privileged: false
              runAsUser: 0
            volumeMounts:
            - mountPath: /underStorage
              name: alluxiodata
          terminationGracePeriodSeconds: 10
          volumes:
          - name: alluxiodata
            persistentVolumeClaim:
              claimName: ${MASTER_NAME}
  - apiVersion: apps/v1beta1
    kind: StatefulSet
    metadata:
      labels:
        name: ${WORKER_NAME}
      name: ${WORKER_NAME}
    spec:
      replicas: ${REPLICAS}
      serviceName: alluxio-worker
      template:
        metadata:
          labels:
            name: ${WORKER_NAME}
        spec:
          affinity:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: name
                      operator: In
                      values:
                      - ${WORKER_NAME}
                  topologyKey: kubernetes.io/hostname
                weight: 100
          containers:
          - args:
            - worker
            env:
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: ALLUXIO_WORKER_HOSTNAME
              value: $(NODE_NAME).alluxio-worker.$(NAMESPACE).svc.cluster.local
            - name: ALLUXIO_UNDERFS_ADDRESS
              value: /underStorage
            - name: ALLUXIO_WORKER_WEB_PORT
              value: "11012"
            - name: ALLUXIO_WORKER_DATA_PORT
              value: "29999"
            - name: ALLUXIO_WORKER_PORT
              value: "29998"
            - name: ALLUXIO_RAM_FOLDER
              value: /dev/shm
            - name: ALLUXIO_MASTER_PORT
              value: "19998"
            - name: ALLUXIO_MASTER_WEB_PORT
              value: "19999"
            - name: ALLUXIO_WORKER_MEMORY_SIZE
              value: ${WORKER_MEM_LIMIT}GB
            - name: ALLUXIO_USER_FILE_WRITE_LOCATION_POLICY_CLASS
              value: alluxio.client.file.policy.RoundRobinPolicy
            - name: ALLUXIO_USER_FILE_WRITETYPE_DEFAULT
              value: MUST_CACHE
            - name: ALLUXIO_WORKER_ALLOCATOR_CLASS
              value: alluxio.worker.block.allocator.GreedyAllocator
            - name: ALLUXIO_WORKER_TIEREDSTORE_RESERVER_ENABLED
              value: "true"
            - name: ALLUXIO_WORKER_TIEREDSTORE_LEVELS
              value: "2"
            - name: ALLUXIO_WORKER_TIEREDSTORE_LEVEL0_ALIAS
              value: MEM
            - name: ALLUXIO_WORKER_TIEREDSTORE_LEVEL0_DIRS_PATH
              value: /dev/shm
            - name: ALLUXIO_WORKER_TIEREDSTORE_LEVEL0_DIRS_QUOTA
              value: ${WORKER_MEM_LIMIT}GB
            - name: ALLUXIO_WORKER_TIEREDSTORE_LEVEL0_RESERVED_RATIO
              value: "0.1"
            - name: ALLUXIO_WORKER_TIEREDSTORE_LEVEL1_ALIAS
              value: HDD
            - name: ALLUXIO_WORKER_TIEREDSTORE_LEVEL1_DIRS_PATH
              value: /hdd
            - name: ALLUXIO_WORKER_TIEREDSTORE_LEVEL1_RESERVED_RATIO
              value: "0.1"
            - name: ALLUXIO_WORKER_TIEREDSTORE_RETRY
              value: "12"
            - name: ALLUXIO_WORKER_TIEREDSTORE_LEVEL1_DIRS_QUOTA
              value: ${PV_SIZE}GB
            - name: ALLUXIO_ZOOKEEPER_ENABLED
              value: "true"
            - name: ALLUXIO_ZOOKEEPER_ADDRESS
              value: ${SERVER_ZOOKEEPER_CONNECT}
            image: ${ALLUXIO_IMAGE}
            imagePullPolicy: Always
            livenessProbe:
              failureThreshold: 3
              initialDelaySeconds: 30
              periodSeconds: 10
              successThreshold: 1
              tcpSocket:
                port: 29998
              timeoutSeconds: 3
            name: ${WORKER_NAME}
            ports:
            - containerPort: 11012
              protocol: TCP
            - containerPort: 29999
              protocol: TCP
            - containerPort: 29998
              protocol: TCP
            resources:
              limits:
                cpu: ${WORKER_CPU_LIMIT}
                memory: ${WORKER_MEM_LIMIT}G
              requests:
                cpu: 200m
                memory: 200M
            securityContext:
              privileged: false
              runAsUser: 0
            volumeMounts:
            - mountPath: /underStorage
              name: alluxiodata
            - mountPath: /dev/shm
              name: dshm
          terminationGracePeriodSeconds: 10
          volumes:
          - name: alluxiodata
            persistentVolumeClaim:
              claimName: ${MASTER_NAME}
          - emptyDir:
              medium: Memory
            name: dshm
  parameters:
  - description: Name of the Alluxio master/worker image
    name: ALLUXIO_IMAGE
    required: true
    value: openshift/alluxio:v1.5
  - description: master name used as a service name and a selector
    name: MASTER_NAME
    required: true
    value: alluxio-master
  - description: worker name used as a selector
    name: WORKER_NAME
    required: true
    value: alluxio-worker
  - description: Number of replicas.
    name: REPLICAS
    required: true
    value: "3"
  - description: MASTER_NODE_CPU_LIMIT_CORES
    name: MASTER_CPU_LIMIT
    required: true
    value: "2"
  - description: MASTER_NODE_MEM_LIMIT_GB
    name: MASTER_MEM_LIMIT
    required: true
    value: "2"
  - description: WORKER_CPU_LIMIT_CORES
    name: WORKER_CPU_LIMIT
    required: true
    value: "2"
  - description: WORKER_MEM_LIMIT_GB
    name: WORKER_MEM_LIMIT
    required: true
    value: "64"
  - description: persistentVolume_size_Gi
    name: PV_SIZE
    required: true
    value: "10"
  - description: |
      Zookeeper conection string, a list as URL with nodes separated by ','.
    name: SERVER_ZOOKEEPER_CONNECT
    required: true
    value: zk-0.zk:2181,zk-1.zk:2181,zk-2.zk:2181
- apiVersion: v1
  kind: Template
  labels:
    app: alluxiocluster
    template: alluxio-persistent
  metadata:
    annotations:
      description: Create a alluxio cluster, with persistent storage.
      iconClass: icon-database
      openshift.io/display-name: AlluxioCluster (Persistent)
      tags: database,alluxio
    creationTimestamp: 2017-09-25T03:17:22Z
    name: alluxio-persistent
    namespace: openshift
    resourceVersion: "27449602"
    selfLink: /oapi/v1/namespaces/openshift/templates/alluxio-persistent
    uid: 0c2a8668-a1a0-11e7-b83c-ecf4bbe2f144
  objects:
  - apiVersion: v1
    kind: Service
    metadata:
      labels:
        name: ${MASTER_NAME}
      name: alluxio-master
    spec:
      clusterIP: None
      ports:
      - name: masterport
        port: 19998
        protocol: TCP
        targetPort: 19998
      - name: masterwebport
        port: 19999
        protocol: TCP
        targetPort: 19999
      selector:
        name: ${MASTER_NAME}
  - apiVersion: v1
    kind: Service
    metadata:
      labels:
        name: ${WORKER_NAME}
      name: alluxio-worker
    spec:
      clusterIP: None
      ports:
      - name: workerport
        port: 29998
        protocol: TCP
        targetPort: 29998
      - name: workerdataport
        port: 29999
        protocol: TCP
        targetPort: 29999
      - name: workerwebport
        port: 11012
        protocol: TCP
        targetPort: 11012
      selector:
        name: ${WORKER_NAME}
  - apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      annotations:
        volume.beta.kubernetes.io/storage-class: nfs
        volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/glusterfs
      name: ${MASTER_NAME}
    spec:
      accessModes:
      - ReadWriteMany
      resources:
        requests:
          storage: ${PV_SIZE}Gi
  - apiVersion: apps/v1beta1
    kind: StatefulSet
    metadata:
      labels:
        name: ${MASTER_NAME}
      name: ${MASTER_NAME}
    spec:
      replicas: 1
      serviceName: alluxio-master
      template:
        metadata:
          labels:
            name: ${MASTER_NAME}
        spec:
          affinity:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: name
                      operator: In
                      values:
                      - ${MASTER_NAME}
                  topologyKey: kubernetes.io/hostname
                weight: 100
          containers:
          - args:
            - master
            env:
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: ALLUXIO_MASTER_HOSTNAME
              value: $(NODE_NAME).alluxio-master.$(NAMESPACE).svc.cluster.local
            - name: ALLUXIO_UNDERFS_ADDRESS
              value: /underStorage
            - name: ALLUXIO_MASTER_PORT
              value: "19998"
            - name: ALLUXIO_MASTER_WEB_PORT
              value: "19999"
            - name: ALLUXIO_MASTER_WORKER_TIMEOUT_MS
              value: "40000"
            - name: ALLUXIO_MASTER_JOURNAL_FOLDER
              value: /underStorage/DoNotDelete/journal
            image: ${ALLUXIO_IMAGE}
            imagePullPolicy: Always
            livenessProbe:
              failureThreshold: 3
              initialDelaySeconds: 90
              periodSeconds: 10
              successThreshold: 1
              tcpSocket:
                port: 19998
              timeoutSeconds: 5
            name: ${MASTER_NAME}
            ports:
            - containerPort: 19998
              protocol: TCP
            - containerPort: 19999
              protocol: TCP
            resources:
              limits:
                cpu: ${MASTER_CPU_LIMIT}
                memory: ${MASTER_MEM_LIMIT}G
              requests:
                cpu: 200m
                memory: 200M
            securityContext:
              privileged: false
              runAsUser: 0
            volumeMounts:
            - mountPath: /underStorage
              name: alluxiodata
          terminationGracePeriodSeconds: 10
          volumes:
          - name: alluxiodata
            persistentVolumeClaim:
              claimName: ${MASTER_NAME}
  - apiVersion: apps/v1beta1
    kind: StatefulSet
    metadata:
      labels:
        name: ${WORKER_NAME}
      name: ${WORKER_NAME}
    spec:
      replicas: ${REPLICAS}
      serviceName: alluxio-worker
      template:
        metadata:
          labels:
            name: ${WORKER_NAME}
        spec:
          affinity:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: name
                      operator: In
                      values:
                      - ${WORKER_NAME}
                  topologyKey: kubernetes.io/hostname
                weight: 100
          containers:
          - args:
            - worker
            env:
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: ALLUXIO_MASTER_HOSTNAME
              value: ${MASTER_NAME}-0.alluxio-master.$(NAMESPACE).svc.cluster.local
            - name: ALLUXIO_WORKER_HOSTNAME
              value: $(NODE_NAME).alluxio-worker.$(NAMESPACE).svc.cluster.local
            - name: ALLUXIO_UNDERFS_ADDRESS
              value: /underStorage
            - name: ALLUXIO_WORKER_WEB_PORT
              value: "11012"
            - name: ALLUXIO_WORKER_DATA_PORT
              value: "29999"
            - name: ALLUXIO_WORKER_PORT
              value: "29998"
            - name: ALLUXIO_RAM_FOLDER
              value: /dev/shm
            - name: ALLUXIO_MASTER_PORT
              value: "19998"
            - name: ALLUXIO_MASTER_WEB_PORT
              value: "19999"
            - name: ALLUXIO_WORKER_MEMORY_SIZE
              value: ${WORKER_MEM_LIMIT}GB
            - name: ALLUXIO_USER_FILE_WRITE_LOCATION_POLICY_CLASS
              value: alluxio.client.file.policy.RoundRobinPolicy
            - name: ALLUXIO_USER_FILE_WRITETYPE_DEFAULT
              value: MUST_CACHE
            - name: ALLUXIO_WORKER_ALLOCATOR_CLASS
              value: alluxio.worker.block.allocator.GreedyAllocator
            - name: ALLUXIO_WORKER_TIEREDSTORE_RESERVER_ENABLED
              value: "true"
            - name: ALLUXIO_WORKER_TIEREDSTORE_LEVELS
              value: "2"
            - name: ALLUXIO_WORKER_TIEREDSTORE_LEVEL0_ALIAS
              value: MEM
            - name: ALLUXIO_WORKER_TIEREDSTORE_LEVEL0_DIRS_PATH
              value: /dev/shm
            - name: ALLUXIO_WORKER_TIEREDSTORE_LEVEL0_DIRS_QUOTA
              value: ${WORKER_MEM_LIMIT}GB
            - name: ALLUXIO_WORKER_TIEREDSTORE_LEVEL0_RESERVED_RATIO
              value: "0.1"
            - name: ALLUXIO_WORKER_TIEREDSTORE_LEVEL1_ALIAS
              value: HDD
            - name: ALLUXIO_WORKER_TIEREDSTORE_LEVEL1_DIRS_PATH
              value: /hdd
            - name: ALLUXIO_WORKER_TIEREDSTORE_LEVEL1_RESERVED_RATIO
              value: "0.1"
            - name: ALLUXIO_WORKER_TIEREDSTORE_RETRY
              value: "8"
            - name: ALLUXIO_WORKER_TIEREDSTORE_LEVEL1_DIRS_QUOTA
              value: 10GB
            - name: ALLUXIO_WORKER_BLOCK_HEARTBEAT_TIMEOUT_MS
              value: "40000"
            image: ${ALLUXIO_IMAGE}
            imagePullPolicy: Always
            livenessProbe:
              failureThreshold: 3
              initialDelaySeconds: 30
              periodSeconds: 10
              successThreshold: 1
              tcpSocket:
                port: 29998
              timeoutSeconds: 3
            name: ${WORKER_NAME}
            ports:
            - containerPort: 11012
              protocol: TCP
            - containerPort: 29999
              protocol: TCP
            - containerPort: 29998
              protocol: TCP
            resources:
              limits:
                cpu: ${WORKER_CPU_LIMIT}
                memory: ${WORKER_MEM_LIMIT}G
              requests:
                cpu: 200m
                memory: 200M
            securityContext:
              privileged: false
              runAsUser: 0
            volumeMounts:
            - mountPath: /underStorage
              name: alluxiodata
            - mountPath: /dev/shm
              name: dshm
          terminationGracePeriodSeconds: 10
          volumes:
          - name: alluxiodata
            persistentVolumeClaim:
              claimName: ${MASTER_NAME}
          - emptyDir:
              medium: Memory
            name: dshm
  parameters:
  - description: Name of the Alluxio master/worker image
    name: ALLUXIO_IMAGE
    required: true
    value: openshift/alluxio:v1.5
  - description: master name used as a service name and a selector
    name: MASTER_NAME
    required: true
    value: alluxio-master
  - description: worker name used as a selector
    name: WORKER_NAME
    required: true
    value: alluxio-worker
  - description: Number of replicas.
    name: REPLICAS
    required: true
    value: "3"
  - description: MASTER_NODE_CPU_LIMIT_CORES
    name: MASTER_CPU_LIMIT
    required: true
    value: "2"
  - description: MASTER_NODE_MEM_LIMIT_GB
    name: MASTER_MEM_LIMIT
    required: true
    value: "2"
  - description: WORKER_CPU_LIMIT_CORES
    name: WORKER_CPU_LIMIT
    required: true
    value: "2"
  - description: WORKER_MEM_LIMIT_GB
    name: WORKER_MEM_LIMIT
    required: true
    value: "64"
  - description: persistentVolume_size_Gi
    name: PV_SIZE
    required: true
    value: "10"
- apiVersion: v1
  kind: Template
  metadata:
    creationTimestamp: 2017-08-23T07:04:10Z
    name: amp-apicast-wildcard-router
    namespace: openshift
    resourceVersion: "874"
    selfLink: /oapi/v1/namespaces/openshift/templates/amp-apicast-wildcard-router
    uid: 4369143c-87d1-11e7-9a67-ecf4bbe2f144
  objects:
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      name: apicast-router
    spec:
      replicas: 1
      selector:
        deploymentconfig: apicast-router
      strategy:
        type: Rolling
      template:
        metadata:
          labels:
            deploymentconfig: apicast-router
        spec:
          containers:
          - command:
            - bin/apicast
            env:
            - name: APICAST_CONFIGURATION_LOADER
              value: lazy
            - name: APICAST_CONFIGURATION_CACHE
              value: "0"
            image: 3scale-amp20/apicast-gateway:1.0-3
            imagePullPolicy: IfNotPresent
            livenessProbe:
              initialDelaySeconds: 5
              periodSeconds: 10
              tcpSocket:
                port: router
              timeoutSeconds: 5
            name: apicast-router
            ports:
            - containerPort: 8082
              name: router
              protocol: TCP
            - containerPort: 8090
              name: management
              protocol: TCP
            readinessProbe:
              httpGet:
                path: /status/ready
                port: management
              initialDelaySeconds: 5
              periodSeconds: 30
              timeoutSeconds: 5
            volumeMounts:
            - mountPath: /opt/app-root/src/sites.d/
              name: apicast-router-config
              readOnly: true
          volumes:
          - configMap:
              items:
              - key: router.conf
                path: router.conf
              name: apicast-router-config
            name: apicast-router-config
      triggers:
      - type: ConfigChange
  - apiVersion: v1
    kind: Service
    metadata:
      name: apicast-router
    spec:
      ports:
      - name: router
        port: 80
        protocol: TCP
        targetPort: router
      selector:
        deploymentconfig: apicast-router
  - apiVersion: v1
    data:
      router.conf: |-
        upstream wildcard {
          server 0.0.0.1:1;

          balancer_by_lua_block {
            local round_robin = require 'resty.balancer.round_robin'
            local balancer = round_robin.new()
            local peers = balancer:peers(ngx.ctx.apicast)

            local peer, err = balancer:set_peer(peers)

            if not peer then
              ngx.status = ngx.HTTP_SERVICE_UNAVAILABLE
              ngx.log(ngx.ERR, "failed to set current backend peer: ", err)
              ngx.exit(ngx.status)
            end
          }

          keepalive 1024;
        }

        server {
          listen 8082;
          server_name ~-(?<apicast>apicast-(staging|production))\.;
          access_log /dev/stdout combined;

          location / {
            access_by_lua_block {
              local resolver = require('resty.resolver'):instance()
              local servers = resolver:get_servers(ngx.var.apicast, { port = 8080 })

              if #servers == 0 then
                ngx.status = ngx.HTTP_BAD_GATEWAY
                ngx.exit(ngx.HTTP_OK)
              end

              ngx.ctx.apicast = servers
            }
            proxy_http_version 1.1;
            proxy_pass $scheme://wildcard;
            proxy_set_header Host $host;
            proxy_set_header Connection "";
          }
        }
    kind: ConfigMap
    metadata:
      name: apicast-router-config
  - apiVersion: v1
    kind: Route
    metadata:
      labels:
        app: apicast-wildcard-router
      name: apicast-wildcard-router
    spec:
      host: apicast-${TENANT_NAME}.${WILDCARD_DOMAIN}
      port:
        targetPort: router
      tls:
        insecureEdgeTerminationPolicy: Allow
        termination: edge
      to:
        kind: Service
        name: apicast-router
      wildcardPolicy: Subdomain
  parameters:
  - description: AMP release tag.
    name: AMP_RELEASE
    required: true
    value: 2.0.0-CR2-redhat-1
  - description: Root domain for the wildcard routes. Eg. example.com will generate
      3scale-admin.example.com.
    name: WILDCARD_DOMAIN
    required: true
  - description: Domain name under the root that Admin UI will be available with -admin
      suffix.
    name: TENANT_NAME
    required: true
    value: 3scale
- apiVersion: v1
  kind: Template
  labels:
    component: cassandra
    template: cassandra-persistent
  metadata:
    annotations:
      description: Create a Cassandra cluster, with persistent storage.
      iconClass: icon-database
      openshift.io/display-name: Cassandra (Persistent)
      tags: database,cassandra
    creationTimestamp: 2017-10-11T07:30:45Z
    name: cassandra-persistent
    namespace: openshift
    resourceVersion: "27449475"
    selfLink: /oapi/v1/namespaces/openshift/templates/cassandra-persistent
    uid: 1860e2b6-ae56-11e7-b80c-ecf4bbe2f144
  objects:
  - apiVersion: v1
    kind: Service
    metadata:
      annotations:
        service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
      labels:
        component: ${NAME}
      name: cassandra
    spec:
      clusterIP: None
      ports:
      - name: intra-node
        port: 7000
        protocol: TCP
        targetPort: 7000
      - name: tls-intra
        port: 7001
        protocol: TCP
        targetPort: 7001
      - name: cql-port
        port: 9042
        protocol: TCP
        targetPort: 9042
      selector:
        component: ${NAME}
  - apiVersion: apps/v1beta1
    kind: StatefulSet
    metadata:
      labels:
        component: ${NAME}
      name: ${NAME}
    spec:
      replicas: ${REPLICAS}
      serviceName: cassandra
      template:
        metadata:
          annotations:
            l4proxyport: "9042"
            proxy: level4
          labels:
            component: ${NAME}
        spec:
          affinity:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: component
                      operator: In
                      values:
                      - ${NAME}
                  topologyKey: kubernetes.io/hostname
                weight: 100
          containers:
          - command:
            - /opt/apache-cassandra/bin/docker-entrypoint-stateful-sets.sh
            env:
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: CASSANDRA_SEEDS
              value: ${NAME}-0.cassandra.$(NAMESPACE).svc.cluster.local
            - name: CASSANDRA_CLUSTER_NAME
              value: cluster1
            - name: CASSANDRA_DC
              value: dc1
            - name: CASSANDRA_RACK
              value: rack1
            - name: HEAP_NEWSIZE
              value: ${HEAP_NEWSIZE}M
            - name: MAX_HEAP_SIZE
              value: ${MAX_HEAP_SIZE}M
            image: ${SOURCE_IMAGE}
            imagePullPolicy: Always
            livenessProbe:
              failureThreshold: 3
              initialDelaySeconds: 50
              periodSeconds: 10
              successThreshold: 1
              tcpSocket:
                port: 9042
              timeoutSeconds: 5
            name: ${NAME}
            ports:
            - containerPort: 9042
              name: cql-port
              protocol: TCP
            - containerPort: 9160
              name: thift-port
              protocol: TCP
            - containerPort: 7000
              name: intra-node
              protocol: TCP
            - containerPort: 7001
              name: tls-intra
              protocol: TCP
            resources:
              limits:
                cpu: ${RESOURCE_CPU_LIMIT}
                memory: ${RESOURCE_MEMORY_LIMIT}G
              requests:
                cpu: 300m
                memory: 300M
            securityContext:
              privileged: true
              runAsUser: 0
            volumeMounts:
            - mountPath: /var/lib/cassandra/data
              name: cassandra-storage
          dnsPolicy: ClusterFirst
          restartPolicy: Always
          securityContext: {}
          terminationGracePeriodSeconds: 10
      volumeClaimTemplates:
      - metadata:
          annotations:
            volume.alpha.kubernetes.io/storage-class: slow
            volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/rbd
          name: cassandra-storage
        spec:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: ${PV_SIZE}Gi
  parameters:
  - description: Name.
    name: NAME
    required: true
    value: cassandra
  - description: Container image source.
    name: SOURCE_IMAGE
    required: true
    value: cassandra-state:3.11
  - description: Number of replicas.
    name: REPLICAS
    required: true
    value: "3"
  - description: cassandra storage capacity GB.
    name: PV_SIZE
    required: true
    value: "30"
  - description: The HEAP_NEWSIZE MB = 1/4 MAX_HEAP_SIZE .
    name: HEAP_NEWSIZE
    value: "400"
  - description: The MAX_HEAP_SIZE MB.
    name: MAX_HEAP_SIZE
    value: "16000"
  - description: The limits for memory resource.
    name: RESOURCE_MEMORY_LIMIT
    value: "18"
  - description: The limits for CPU resource.
    name: RESOURCE_CPU_LIMIT
    value: "8"
- apiVersion: v1
  kind: Template
  labels:
    template: dancer-mysql-example
  message: |-
    The following service(s) have been created in your project: ${NAME}, ${DATABASE_SERVICE_NAME}.

    For more information about using this template, including OpenShift considerations, see https://github.com/openshift/dancer-ex/blob/master/README.md.
  metadata:
    annotations:
      description: |-
        An example Dancer application with a MySQL database. For more information about using this template, including OpenShift considerations, see https://github.com/openshift/dancer-ex/blob/master/README.md.

        WARNING: Any data stored will be lost upon pod destruction. Only use this template for testing.
      iconClass: icon-perl
      openshift.io/display-name: Dancer + MySQL (Ephemeral)
      tags: quickstart,perl,dancer
      template.openshift.io/documentation-url: https://github.com/openshift/dancer-ex
      template.openshift.io/long-description: This template defines resources needed
        to develop a Dancer based application, including a build configuration, application
        deployment configuration, and database deployment configuration.  The database
        is stored in non-persistent storage, so this configuration should be used
        for experimental purposes only.
      template.openshift.io/provider-display-name: Red Hat, Inc.
      template.openshift.io/support-url: https://access.redhat.com
    creationTimestamp: 2017-08-23T07:04:10Z
    name: dancer-mysql-example
    namespace: openshift
    resourceVersion: "861"
    selfLink: /oapi/v1/namespaces/openshift/templates/dancer-mysql-example
    uid: 4357d789-87d1-11e7-9a67-ecf4bbe2f144
  objects:
  - apiVersion: v1
    kind: Secret
    metadata:
      name: ${NAME}
    stringData:
      database-password: ${DATABASE_PASSWORD}
      database-user: ${DATABASE_USER}
      keybase: ${SECRET_KEY_BASE}
  - apiVersion: v1
    kind: Service
    metadata:
      annotations:
        description: Exposes and load balances the application pods
        service.alpha.openshift.io/dependencies: '[{"name": "${DATABASE_SERVICE_NAME}",
          "kind": "Service"}]'
      name: ${NAME}
    spec:
      ports:
      - name: web
        port: 8080
        targetPort: 8080
      selector:
        name: ${NAME}
  - apiVersion: v1
    kind: Route
    metadata:
      annotations:
        template.openshift.io/expose-uri: http://{.spec.host}{.spec.path}
      name: ${NAME}
    spec:
      host: ${APPLICATION_DOMAIN}
      to:
        kind: Service
        name: ${NAME}
  - apiVersion: v1
    kind: ImageStream
    metadata:
      annotations:
        description: Keeps track of changes in the application image
      name: ${NAME}
  - apiVersion: v1
    kind: BuildConfig
    metadata:
      annotations:
        description: Defines how to build the application
      name: ${NAME}
    spec:
      output:
        to:
          kind: ImageStreamTag
          name: ${NAME}:latest
      postCommit:
        script: perl -I extlib/lib/perl5 -I lib t/*
      source:
        contextDir: ${CONTEXT_DIR}
        git:
          ref: ${SOURCE_REPOSITORY_REF}
          uri: ${SOURCE_REPOSITORY_URL}
        type: Git
      strategy:
        sourceStrategy:
          env:
          - name: CPAN_MIRROR
            value: ${CPAN_MIRROR}
          from:
            kind: ImageStreamTag
            name: perl:5.24
            namespace: ${NAMESPACE}
        type: Source
      triggers:
      - type: ImageChange
      - type: ConfigChange
      - github:
          secret: ${GITHUB_WEBHOOK_SECRET}
        type: GitHub
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      annotations:
        description: Defines how to deploy the application server
      name: ${NAME}
    spec:
      replicas: 1
      selector:
        name: ${NAME}
      strategy:
        type: Recreate
      template:
        metadata:
          labels:
            name: ${NAME}
          name: ${NAME}
        spec:
          containers:
          - env:
            - name: DATABASE_SERVICE_NAME
              value: ${DATABASE_SERVICE_NAME}
            - name: MYSQL_USER
              valueFrom:
                secretKeyRef:
                  key: database-user
                  name: ${NAME}
            - name: MYSQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: database-password
                  name: ${NAME}
            - name: MYSQL_DATABASE
              value: ${DATABASE_NAME}
            - name: SECRET_KEY_BASE
              valueFrom:
                secretKeyRef:
                  key: keybase
                  name: ${NAME}
            - name: PERL_APACHE2_RELOAD
              value: ${PERL_APACHE2_RELOAD}
            image: ' '
            livenessProbe:
              httpGet:
                path: /
                port: 8080
              initialDelaySeconds: 30
              timeoutSeconds: 3
            name: dancer-mysql-example
            ports:
            - containerPort: 8080
            readinessProbe:
              httpGet:
                path: /health
                port: 8080
              initialDelaySeconds: 3
              timeoutSeconds: 3
            resources:
              limits:
                memory: ${MEMORY_LIMIT}
      triggers:
      - imageChangeParams:
          automatic: true
          containerNames:
          - dancer-mysql-example
          from:
            kind: ImageStreamTag
            name: ${NAME}:latest
        type: ImageChange
      - type: ConfigChange
  - apiVersion: v1
    kind: Service
    metadata:
      annotations:
        description: Exposes the database server
      name: ${DATABASE_SERVICE_NAME}
    spec:
      ports:
      - name: mysql
        port: 3306
        targetPort: 3306
      selector:
        name: ${DATABASE_SERVICE_NAME}
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      annotations:
        description: Defines how to deploy the database
      name: ${DATABASE_SERVICE_NAME}
    spec:
      replicas: 1
      selector:
        name: ${DATABASE_SERVICE_NAME}
      strategy:
        type: Recreate
      template:
        metadata:
          labels:
            name: ${DATABASE_SERVICE_NAME}
          name: ${DATABASE_SERVICE_NAME}
        spec:
          containers:
          - env:
            - name: MYSQL_USER
              valueFrom:
                secretKeyRef:
                  key: database-user
                  name: ${NAME}
            - name: MYSQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: database-password
                  name: ${NAME}
            - name: MYSQL_DATABASE
              value: ${DATABASE_NAME}
            image: ' '
            livenessProbe:
              initialDelaySeconds: 30
              tcpSocket:
                port: 3306
              timeoutSeconds: 1
            name: mysql
            ports:
            - containerPort: 3306
            readinessProbe:
              exec:
                command:
                - /bin/sh
                - -i
                - -c
                - MYSQL_PWD='${DATABASE_PASSWORD}' mysql -h 127.0.0.1 -u ${DATABASE_USER}
                  -D ${DATABASE_NAME} -e 'SELECT 1'
              initialDelaySeconds: 5
              timeoutSeconds: 1
            resources:
              limits:
                memory: ${MEMORY_MYSQL_LIMIT}
            volumeMounts:
            - mountPath: /var/lib/mysql/data
              name: data
          volumes:
          - emptyDir: {}
            name: data
      triggers:
      - imageChangeParams:
          automatic: true
          containerNames:
          - mysql
          from:
            kind: ImageStreamTag
            name: mysql:5.7
            namespace: ${NAMESPACE}
        type: ImageChange
      - type: ConfigChange
  parameters:
  - description: The name assigned to all of the frontend objects defined in this
      template.
    displayName: Name
    name: NAME
    required: true
    value: dancer-mysql-example
  - description: The OpenShift Namespace where the ImageStream resides.
    displayName: Namespace
    name: NAMESPACE
    required: true
    value: openshift
  - description: Maximum amount of memory the Perl Dancer container can use.
    displayName: Memory Limit
    name: MEMORY_LIMIT
    required: true
    value: 512Mi
  - description: Maximum amount of memory the MySQL container can use.
    displayName: Memory Limit (MySQL)
    name: MEMORY_MYSQL_LIMIT
    required: true
    value: 512Mi
  - description: The URL of the repository with your application source code.
    displayName: Git Repository URL
    name: SOURCE_REPOSITORY_URL
    required: true
    value: https://github.com/openshift/dancer-ex.git
  - description: Set this to a branch name, tag or other ref of your repository if
      you are not using the default branch.
    displayName: Git Reference
    name: SOURCE_REPOSITORY_REF
  - description: Set this to the relative path to your project if it is not in the
      root of your repository.
    displayName: Context Directory
    name: CONTEXT_DIR
  - description: The exposed hostname that will route to the Dancer service, if left
      blank a value will be defaulted.
    displayName: Application Hostname
    name: APPLICATION_DOMAIN
  - description: Github trigger secret.  A difficult to guess string encoded as part
      of the webhook URL.  Not encrypted.
    displayName: GitHub Webhook Secret
    from: '[a-zA-Z0-9]{40}'
    generate: expression
    name: GITHUB_WEBHOOK_SECRET
  - displayName: Database Service Name
    name: DATABASE_SERVICE_NAME
    required: true
    value: database
  - displayName: Database Username
    from: user[A-Z0-9]{3}
    generate: expression
    name: DATABASE_USER
  - displayName: Database Password
    from: '[a-zA-Z0-9]{8}'
    generate: expression
    name: DATABASE_PASSWORD
  - displayName: Database Name
    name: DATABASE_NAME
    required: true
    value: sampledb
  - description: Set this to "true" to enable automatic reloading of modified Perl
      modules.
    displayName: Perl Module Reload
    name: PERL_APACHE2_RELOAD
  - description: Your secret key for verifying the integrity of signed cookies.
    displayName: Secret Key
    from: '[a-z0-9]{127}'
    generate: expression
    name: SECRET_KEY_BASE
  - description: The custom CPAN mirror URL
    displayName: Custom CPAN Mirror URL
    name: CPAN_MIRROR
- apiVersion: v1
  kind: Template
  labels:
    template: dancer-mysql-persistent
  message: |-
    The following service(s) have been created in your project: ${NAME}, ${DATABASE_SERVICE_NAME}.

    For more information about using this template, including OpenShift considerations, see https://github.com/openshift/dancer-ex/blob/master/README.md.
  metadata:
    annotations:
      description: An example Dancer application with a MySQL database. For more information
        about using this template, including OpenShift considerations, see https://github.com/openshift/dancer-ex/blob/master/README.md.
      iconClass: icon-perl
      openshift.io/display-name: Dancer + MySQL (Persistent)
      tags: quickstart,perl,dancer
      template.openshift.io/documentation-url: https://github.com/openshift/dancer-ex
      template.openshift.io/long-description: This template defines resources needed
        to develop a Dancer based application, including a build configuration, application
        deployment configuration, and database deployment configuration.
      template.openshift.io/provider-display-name: Red Hat, Inc.
      template.openshift.io/support-url: https://access.redhat.com
    creationTimestamp: 2017-08-23T07:04:10Z
    name: dancer-mysql-persistent
    namespace: openshift
    resourceVersion: "860"
    selfLink: /oapi/v1/namespaces/openshift/templates/dancer-mysql-persistent
    uid: 43566979-87d1-11e7-9a67-ecf4bbe2f144
  objects:
  - apiVersion: v1
    kind: Secret
    metadata:
      name: ${NAME}
    stringData:
      database-password: ${DATABASE_PASSWORD}
      database-user: ${DATABASE_USER}
      keybase: ${SECRET_KEY_BASE}
  - apiVersion: v1
    kind: Service
    metadata:
      annotations:
        description: Exposes and load balances the application pods
        service.alpha.openshift.io/dependencies: '[{"name": "${DATABASE_SERVICE_NAME}",
          "kind": "Service"}]'
      name: ${NAME}
    spec:
      ports:
      - name: web
        port: 8080
        targetPort: 8080
      selector:
        name: ${NAME}
  - apiVersion: v1
    kind: Route
    metadata:
      annotations:
        template.openshift.io/expose-uri: http://{.spec.host}{.spec.path}
      name: ${NAME}
    spec:
      host: ${APPLICATION_DOMAIN}
      to:
        kind: Service
        name: ${NAME}
  - apiVersion: v1
    kind: ImageStream
    metadata:
      annotations:
        description: Keeps track of changes in the application image
      name: ${NAME}
  - apiVersion: v1
    kind: BuildConfig
    metadata:
      annotations:
        description: Defines how to build the application
      name: ${NAME}
    spec:
      output:
        to:
          kind: ImageStreamTag
          name: ${NAME}:latest
      postCommit:
        script: perl -I extlib/lib/perl5 -I lib t/*
      source:
        contextDir: ${CONTEXT_DIR}
        git:
          ref: ${SOURCE_REPOSITORY_REF}
          uri: ${SOURCE_REPOSITORY_URL}
        type: Git
      strategy:
        sourceStrategy:
          env:
          - name: CPAN_MIRROR
            value: ${CPAN_MIRROR}
          from:
            kind: ImageStreamTag
            name: perl:5.24
            namespace: ${NAMESPACE}
        type: Source
      triggers:
      - type: ImageChange
      - type: ConfigChange
      - github:
          secret: ${GITHUB_WEBHOOK_SECRET}
        type: GitHub
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      annotations:
        description: Defines how to deploy the application server
      name: ${NAME}
    spec:
      replicas: 1
      selector:
        name: ${NAME}
      strategy:
        type: Recreate
      template:
        metadata:
          labels:
            name: ${NAME}
          name: ${NAME}
        spec:
          containers:
          - env:
            - name: DATABASE_SERVICE_NAME
              value: ${DATABASE_SERVICE_NAME}
            - name: MYSQL_USER
              valueFrom:
                secretKeyRef:
                  key: database-user
                  name: ${NAME}
            - name: MYSQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: database-password
                  name: ${NAME}
            - name: MYSQL_DATABASE
              value: ${DATABASE_NAME}
            - name: SECRET_KEY_BASE
              valueFrom:
                secretKeyRef:
                  key: keybase
                  name: ${NAME}
            - name: PERL_APACHE2_RELOAD
              value: ${PERL_APACHE2_RELOAD}
            image: ' '
            livenessProbe:
              httpGet:
                path: /
                port: 8080
              initialDelaySeconds: 30
              timeoutSeconds: 3
            name: dancer-mysql-persistent
            ports:
            - containerPort: 8080
            readinessProbe:
              httpGet:
                path: /health
                port: 8080
              initialDelaySeconds: 3
              timeoutSeconds: 3
            resources:
              limits:
                memory: ${MEMORY_LIMIT}
      triggers:
      - imageChangeParams:
          automatic: true
          containerNames:
          - dancer-mysql-persistent
          from:
            kind: ImageStreamTag
            name: ${NAME}:latest
        type: ImageChange
      - type: ConfigChange
  - apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: ${DATABASE_SERVICE_NAME}
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: ${VOLUME_CAPACITY}
  - apiVersion: v1
    kind: Service
    metadata:
      annotations:
        description: Exposes the database server
      name: ${DATABASE_SERVICE_NAME}
    spec:
      ports:
      - name: mysql
        port: 3306
        targetPort: 3306
      selector:
        name: ${DATABASE_SERVICE_NAME}
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      annotations:
        description: Defines how to deploy the database
      name: ${DATABASE_SERVICE_NAME}
    spec:
      replicas: 1
      selector:
        name: ${DATABASE_SERVICE_NAME}
      strategy:
        type: Recreate
      template:
        metadata:
          labels:
            name: ${DATABASE_SERVICE_NAME}
          name: ${DATABASE_SERVICE_NAME}
        spec:
          containers:
          - env:
            - name: MYSQL_USER
              valueFrom:
                secretKeyRef:
                  key: database-user
                  name: ${NAME}
            - name: MYSQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: database-password
                  name: ${NAME}
            - name: MYSQL_DATABASE
              value: ${DATABASE_NAME}
            image: ' '
            livenessProbe:
              initialDelaySeconds: 30
              tcpSocket:
                port: 3306
              timeoutSeconds: 1
            name: mysql
            ports:
            - containerPort: 3306
            readinessProbe:
              exec:
                command:
                - /bin/sh
                - -i
                - -c
                - MYSQL_PWD='${DATABASE_PASSWORD}' mysql -h 127.0.0.1 -u ${DATABASE_USER}
                  -D ${DATABASE_NAME} -e 'SELECT 1'
              initialDelaySeconds: 5
              timeoutSeconds: 1
            resources:
              limits:
                memory: ${MEMORY_MYSQL_LIMIT}
            volumeMounts:
            - mountPath: /var/lib/mysql/data
              name: ${DATABASE_SERVICE_NAME}-data
          volumes:
          - name: ${DATABASE_SERVICE_NAME}-data
            persistentVolumeClaim:
              claimName: ${DATABASE_SERVICE_NAME}
      triggers:
      - imageChangeParams:
          automatic: true
          containerNames:
          - mysql
          from:
            kind: ImageStreamTag
            name: mysql:5.7
            namespace: ${NAMESPACE}
        type: ImageChange
      - type: ConfigChange
  parameters:
  - description: The name assigned to all of the frontend objects defined in this
      template.
    displayName: Name
    name: NAME
    required: true
    value: dancer-mysql-persistent
  - description: The OpenShift Namespace where the ImageStream resides.
    displayName: Namespace
    name: NAMESPACE
    required: true
    value: openshift
  - description: Maximum amount of memory the Perl Dancer container can use.
    displayName: Memory Limit
    name: MEMORY_LIMIT
    required: true
    value: 512Mi
  - description: Maximum amount of memory the MySQL container can use.
    displayName: Memory Limit (MySQL)
    name: MEMORY_MYSQL_LIMIT
    required: true
    value: 512Mi
  - description: Volume space available for data, e.g. 512Mi, 2Gi
    displayName: Volume Capacity
    name: VOLUME_CAPACITY
    required: true
    value: 1Gi
  - description: The URL of the repository with your application source code.
    displayName: Git Repository URL
    name: SOURCE_REPOSITORY_URL
    required: true
    value: https://github.com/openshift/dancer-ex.git
  - description: Set this to a branch name, tag or other ref of your repository if
      you are not using the default branch.
    displayName: Git Reference
    name: SOURCE_REPOSITORY_REF
  - description: Set this to the relative path to your project if it is not in the
      root of your repository.
    displayName: Context Directory
    name: CONTEXT_DIR
  - description: The exposed hostname that will route to the Dancer service, if left
      blank a value will be defaulted.
    displayName: Application Hostname
    name: APPLICATION_DOMAIN
  - description: Github trigger secret.  A difficult to guess string encoded as part
      of the webhook URL.  Not encrypted.
    displayName: GitHub Webhook Secret
    from: '[a-zA-Z0-9]{40}'
    generate: expression
    name: GITHUB_WEBHOOK_SECRET
  - displayName: Database Service Name
    name: DATABASE_SERVICE_NAME
    required: true
    value: database
  - displayName: Database Username
    from: user[A-Z0-9]{3}
    generate: expression
    name: DATABASE_USER
  - displayName: Database Password
    from: '[a-zA-Z0-9]{8}'
    generate: expression
    name: DATABASE_PASSWORD
  - displayName: Database Name
    name: DATABASE_NAME
    required: true
    value: sampledb
  - description: Set this to "true" to enable automatic reloading of modified Perl
      modules.
    displayName: Perl Module Reload
    name: PERL_APACHE2_RELOAD
  - description: Your secret key for verifying the integrity of signed cookies.
    displayName: Secret Key
    from: '[a-z0-9]{127}'
    generate: expression
    name: SECRET_KEY_BASE
  - description: The custom CPAN mirror URL
    displayName: Custom CPAN Mirror URL
    name: CPAN_MIRROR
- apiVersion: v1
  kind: Template
  labels:
    template: django-psql-example
  message: |-
    The following service(s) have been created in your project: ${NAME}, ${DATABASE_SERVICE_NAME}.

    For more information about using this template, including OpenShift considerations, see https://github.com/openshift/django-ex/blob/master/README.md.
  metadata:
    annotations:
      description: |-
        An example Django application with a PostgreSQL database. For more information about using this template, including OpenShift considerations, see https://github.com/openshift/django-ex/blob/master/README.md.

        WARNING: Any data stored will be lost upon pod destruction. Only use this template for testing.
      iconClass: icon-python
      openshift.io/display-name: Django + PostgreSQL (Ephemeral)
      tags: quickstart,python,django
      template.openshift.io/documentation-url: https://github.com/openshift/django-ex
      template.openshift.io/long-description: This template defines resources needed
        to develop a Django based application, including a build configuration, application
        deployment configuration, and database deployment configuration.  The database
        is stored in non-persistent storage, so this configuration should be used
        for experimental purposes only.
      template.openshift.io/provider-display-name: Red Hat, Inc.
      template.openshift.io/support-url: https://access.redhat.com
    creationTimestamp: 2017-08-23T07:04:10Z
    name: django-psql-example
    namespace: openshift
    resourceVersion: "863"
    selfLink: /oapi/v1/namespaces/openshift/templates/django-psql-example
    uid: 435abe3e-87d1-11e7-9a67-ecf4bbe2f144
  objects:
  - apiVersion: v1
    kind: Secret
    metadata:
      name: ${NAME}
    stringData:
      database-password: ${DATABASE_PASSWORD}
      database-user: ${DATABASE_USER}
      django-secret-key: ${DJANGO_SECRET_KEY}
  - apiVersion: v1
    kind: Service
    metadata:
      annotations:
        description: Exposes and load balances the application pods
        service.alpha.openshift.io/dependencies: '[{"name": "${DATABASE_SERVICE_NAME}",
          "kind": "Service"}]'
      name: ${NAME}
    spec:
      ports:
      - name: web
        port: 8080
        targetPort: 8080
      selector:
        name: ${NAME}
  - apiVersion: v1
    kind: Route
    metadata:
      annotations:
        template.openshift.io/expose-uri: http://{.spec.host}{.spec.path}
      name: ${NAME}
    spec:
      host: ${APPLICATION_DOMAIN}
      to:
        kind: Service
        name: ${NAME}
  - apiVersion: v1
    kind: ImageStream
    metadata:
      annotations:
        description: Keeps track of changes in the application image
      name: ${NAME}
  - apiVersion: v1
    kind: BuildConfig
    metadata:
      annotations:
        description: Defines how to build the application
      name: ${NAME}
    spec:
      output:
        to:
          kind: ImageStreamTag
          name: ${NAME}:latest
      postCommit:
        script: ./manage.py test
      source:
        contextDir: ${CONTEXT_DIR}
        git:
          ref: ${SOURCE_REPOSITORY_REF}
          uri: ${SOURCE_REPOSITORY_URL}
        type: Git
      strategy:
        sourceStrategy:
          env:
          - name: PIP_INDEX_URL
            value: ${PIP_INDEX_URL}
          from:
            kind: ImageStreamTag
            name: python:3.5
            namespace: ${NAMESPACE}
        type: Source
      triggers:
      - type: ImageChange
      - type: ConfigChange
      - github:
          secret: ${GITHUB_WEBHOOK_SECRET}
        type: GitHub
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      annotations:
        description: Defines how to deploy the application server
      name: ${NAME}
    spec:
      replicas: 1
      selector:
        name: ${NAME}
      strategy:
        type: Recreate
      template:
        metadata:
          labels:
            name: ${NAME}
          name: ${NAME}
        spec:
          containers:
          - env:
            - name: DATABASE_SERVICE_NAME
              value: ${DATABASE_SERVICE_NAME}
            - name: DATABASE_ENGINE
              value: ${DATABASE_ENGINE}
            - name: DATABASE_NAME
              value: ${DATABASE_NAME}
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  key: database-user
                  name: ${NAME}
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: database-password
                  name: ${NAME}
            - name: APP_CONFIG
              value: ${APP_CONFIG}
            - name: DJANGO_SECRET_KEY
              valueFrom:
                secretKeyRef:
                  key: django-secret-key
                  name: ${NAME}
            image: ' '
            livenessProbe:
              httpGet:
                path: /health
                port: 8080
              initialDelaySeconds: 30
              timeoutSeconds: 3
            name: django-psql-example
            ports:
            - containerPort: 8080
            readinessProbe:
              httpGet:
                path: /health
                port: 8080
              initialDelaySeconds: 3
              timeoutSeconds: 3
            resources:
              limits:
                memory: ${MEMORY_LIMIT}
      triggers:
      - imageChangeParams:
          automatic: true
          containerNames:
          - django-psql-example
          from:
            kind: ImageStreamTag
            name: ${NAME}:latest
        type: ImageChange
      - type: ConfigChange
  - apiVersion: v1
    kind: Service
    metadata:
      annotations:
        description: Exposes the database server
      name: ${DATABASE_SERVICE_NAME}
    spec:
      ports:
      - name: postgresql
        port: 5432
        targetPort: 5432
      selector:
        name: ${DATABASE_SERVICE_NAME}
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      annotations:
        description: Defines how to deploy the database
      name: ${DATABASE_SERVICE_NAME}
    spec:
      replicas: 1
      selector:
        name: ${DATABASE_SERVICE_NAME}
      strategy:
        type: Recreate
      template:
        metadata:
          labels:
            name: ${DATABASE_SERVICE_NAME}
          name: ${DATABASE_SERVICE_NAME}
        spec:
          containers:
          - env:
            - name: POSTGRESQL_USER
              valueFrom:
                secretKeyRef:
                  key: database-user
                  name: ${NAME}
            - name: POSTGRESQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: database-password
                  name: ${NAME}
            - name: POSTGRESQL_DATABASE
              value: ${DATABASE_NAME}
            image: ' '
            livenessProbe:
              initialDelaySeconds: 30
              tcpSocket:
                port: 5432
              timeoutSeconds: 1
            name: postgresql
            ports:
            - containerPort: 5432
            readinessProbe:
              exec:
                command:
                - /bin/sh
                - -i
                - -c
                - psql -h 127.0.0.1 -U ${POSTGRESQL_USER} -q -d ${POSTGRESQL_DATABASE}
                  -c 'SELECT 1'
              initialDelaySeconds: 5
              timeoutSeconds: 1
            resources:
              limits:
                memory: ${MEMORY_POSTGRESQL_LIMIT}
            volumeMounts:
            - mountPath: /var/lib/pgsql/data
              name: data
          volumes:
          - emptyDir: {}
            name: data
      triggers:
      - imageChangeParams:
          automatic: true
          containerNames:
          - postgresql
          from:
            kind: ImageStreamTag
            name: postgresql:9.5
            namespace: ${NAMESPACE}
        type: ImageChange
      - type: ConfigChange
  parameters:
  - description: The name assigned to all of the frontend objects defined in this
      template.
    displayName: Name
    name: NAME
    required: true
    value: django-psql-example
  - description: The OpenShift Namespace where the ImageStream resides.
    displayName: Namespace
    name: NAMESPACE
    required: true
    value: openshift
  - description: Maximum amount of memory the Django container can use.
    displayName: Memory Limit
    name: MEMORY_LIMIT
    required: true
    value: 512Mi
  - description: Maximum amount of memory the PostgreSQL container can use.
    displayName: Memory Limit (PostgreSQL)
    name: MEMORY_POSTGRESQL_LIMIT
    required: true
    value: 512Mi
  - description: The URL of the repository with your application source code.
    displayName: Git Repository URL
    name: SOURCE_REPOSITORY_URL
    required: true
    value: https://github.com/openshift/django-ex.git
  - description: Set this to a branch name, tag or other ref of your repository if
      you are not using the default branch.
    displayName: Git Reference
    name: SOURCE_REPOSITORY_REF
  - description: Set this to the relative path to your project if it is not in the
      root of your repository.
    displayName: Context Directory
    name: CONTEXT_DIR
  - description: The exposed hostname that will route to the Django service, if left
      blank a value will be defaulted.
    displayName: Application Hostname
    name: APPLICATION_DOMAIN
  - description: Github trigger secret.  A difficult to guess string encoded as part
      of the webhook URL.  Not encrypted.
    displayName: GitHub Webhook Secret
    from: '[a-zA-Z0-9]{40}'
    generate: expression
    name: GITHUB_WEBHOOK_SECRET
  - displayName: Database Service Name
    name: DATABASE_SERVICE_NAME
    required: true
    value: postgresql
  - description: 'Database engine: postgresql, mysql or sqlite (default).'
    displayName: Database Engine
    name: DATABASE_ENGINE
    required: true
    value: postgresql
  - displayName: Database Name
    name: DATABASE_NAME
    required: true
    value: default
  - displayName: Database Username
    name: DATABASE_USER
    required: true
    value: django
  - displayName: Database User Password
    from: '[a-zA-Z0-9]{16}'
    generate: expression
    name: DATABASE_PASSWORD
  - description: Relative path to Gunicorn configuration file (optional).
    displayName: Application Configuration File Path
    name: APP_CONFIG
  - description: Set this to a long random string.
    displayName: Django Secret Key
    from: '[\w]{50}'
    generate: expression
    name: DJANGO_SECRET_KEY
  - description: The custom PyPi index URL
    displayName: Custom PyPi Index URL
    name: PIP_INDEX_URL
- apiVersion: v1
  kind: Template
  labels:
    template: django-psql-persistent
  message: |-
    The following service(s) have been created in your project: ${NAME}, ${DATABASE_SERVICE_NAME}.

    For more information about using this template, including OpenShift considerations, see https://github.com/openshift/django-ex/blob/master/README.md.
  metadata:
    annotations:
      description: An example Django application with a PostgreSQL database. For more
        information about using this template, including OpenShift considerations,
        see https://github.com/openshift/django-ex/blob/master/README.md.
      iconClass: icon-python
      openshift.io/display-name: Django + PostgreSQL (Persistent)
      tags: quickstart,python,django
      template.openshift.io/documentation-url: https://github.com/openshift/django-ex
      template.openshift.io/long-description: This template defines resources needed
        to develop a Django based application, including a build configuration, application
        deployment configuration, and database deployment configuration.
      template.openshift.io/provider-display-name: Red Hat, Inc.
      template.openshift.io/support-url: https://access.redhat.com
    creationTimestamp: 2017-08-23T07:04:10Z
    name: django-psql-persistent
    namespace: openshift
    resourceVersion: "862"
    selfLink: /oapi/v1/namespaces/openshift/templates/django-psql-persistent
    uid: 4359438d-87d1-11e7-9a67-ecf4bbe2f144
  objects:
  - apiVersion: v1
    kind: Secret
    metadata:
      name: ${NAME}
    stringData:
      database-password: ${DATABASE_PASSWORD}
      database-user: ${DATABASE_USER}
      django-secret-key: ${DJANGO_SECRET_KEY}
  - apiVersion: v1
    kind: Service
    metadata:
      annotations:
        description: Exposes and load balances the application pods
        service.alpha.openshift.io/dependencies: '[{"name": "${DATABASE_SERVICE_NAME}",
          "kind": "Service"}]'
      name: ${NAME}
    spec:
      ports:
      - name: web
        port: 8080
        targetPort: 8080
      selector:
        name: ${NAME}
  - apiVersion: v1
    kind: Route
    metadata:
      annotations:
        template.openshift.io/expose-uri: http://{.spec.host}{.spec.path}
      name: ${NAME}
    spec:
      host: ${APPLICATION_DOMAIN}
      to:
        kind: Service
        name: ${NAME}
  - apiVersion: v1
    kind: ImageStream
    metadata:
      annotations:
        description: Keeps track of changes in the application image
      name: ${NAME}
  - apiVersion: v1
    kind: BuildConfig
    metadata:
      annotations:
        description: Defines how to build the application
      name: ${NAME}
    spec:
      output:
        to:
          kind: ImageStreamTag
          name: ${NAME}:latest
      postCommit:
        script: ./manage.py test
      source:
        contextDir: ${CONTEXT_DIR}
        git:
          ref: ${SOURCE_REPOSITORY_REF}
          uri: ${SOURCE_REPOSITORY_URL}
        type: Git
      strategy:
        sourceStrategy:
          env:
          - name: PIP_INDEX_URL
            value: ${PIP_INDEX_URL}
          from:
            kind: ImageStreamTag
            name: python:3.5
            namespace: ${NAMESPACE}
        type: Source
      triggers:
      - type: ImageChange
      - type: ConfigChange
      - github:
          secret: ${GITHUB_WEBHOOK_SECRET}
        type: GitHub
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      annotations:
        description: Defines how to deploy the application server
      name: ${NAME}
    spec:
      replicas: 1
      selector:
        name: ${NAME}
      strategy:
        type: Recreate
      template:
        metadata:
          labels:
            name: ${NAME}
          name: ${NAME}
        spec:
          containers:
          - env:
            - name: DATABASE_SERVICE_NAME
              value: ${DATABASE_SERVICE_NAME}
            - name: DATABASE_ENGINE
              value: ${DATABASE_ENGINE}
            - name: DATABASE_NAME
              value: ${DATABASE_NAME}
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  key: database-user
                  name: ${NAME}
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: database-password
                  name: ${NAME}
            - name: APP_CONFIG
              value: ${APP_CONFIG}
            - name: DJANGO_SECRET_KEY
              valueFrom:
                secretKeyRef:
                  key: django-secret-key
                  name: ${NAME}
            image: ' '
            livenessProbe:
              httpGet:
                path: /health
                port: 8080
              initialDelaySeconds: 30
              timeoutSeconds: 3
            name: django-psql-persistent
            ports:
            - containerPort: 8080
            readinessProbe:
              httpGet:
                path: /health
                port: 8080
              initialDelaySeconds: 3
              timeoutSeconds: 3
            resources:
              limits:
                memory: ${MEMORY_LIMIT}
      triggers:
      - imageChangeParams:
          automatic: true
          containerNames:
          - django-psql-persistent
          from:
            kind: ImageStreamTag
            name: ${NAME}:latest
        type: ImageChange
      - type: ConfigChange
  - apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: ${DATABASE_SERVICE_NAME}
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: ${VOLUME_CAPACITY}
  - apiVersion: v1
    kind: Service
    metadata:
      annotations:
        description: Exposes the database server
      name: ${DATABASE_SERVICE_NAME}
    spec:
      ports:
      - name: postgresql
        port: 5432
        targetPort: 5432
      selector:
        name: ${DATABASE_SERVICE_NAME}
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      annotations:
        description: Defines how to deploy the database
      name: ${DATABASE_SERVICE_NAME}
    spec:
      replicas: 1
      selector:
        name: ${DATABASE_SERVICE_NAME}
      strategy:
        type: Recreate
      template:
        metadata:
          labels:
            name: ${DATABASE_SERVICE_NAME}
          name: ${DATABASE_SERVICE_NAME}
        spec:
          containers:
          - env:
            - name: POSTGRESQL_USER
              valueFrom:
                secretKeyRef:
                  key: database-user
                  name: ${NAME}
            - name: POSTGRESQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: database-password
                  name: ${NAME}
            - name: POSTGRESQL_DATABASE
              value: ${DATABASE_NAME}
            image: ' '
            livenessProbe:
              initialDelaySeconds: 30
              tcpSocket:
                port: 5432
              timeoutSeconds: 1
            name: postgresql
            ports:
            - containerPort: 5432
            readinessProbe:
              exec:
                command:
                - /bin/sh
                - -i
                - -c
                - psql -h 127.0.0.1 -U ${POSTGRESQL_USER} -q -d ${POSTGRESQL_DATABASE}
                  -c 'SELECT 1'
              initialDelaySeconds: 5
              timeoutSeconds: 1
            resources:
              limits:
                memory: ${MEMORY_POSTGRESQL_LIMIT}
            volumeMounts:
            - mountPath: /var/lib/pgsql/data
              name: ${DATABASE_SERVICE_NAME}-data
          volumes:
          - name: ${DATABASE_SERVICE_NAME}-data
            persistentVolumeClaim:
              claimName: ${DATABASE_SERVICE_NAME}
      triggers:
      - imageChangeParams:
          automatic: true
          containerNames:
          - postgresql
          from:
            kind: ImageStreamTag
            name: postgresql:9.5
            namespace: ${NAMESPACE}
        type: ImageChange
      - type: ConfigChange
  parameters:
  - description: The name assigned to all of the frontend objects defined in this
      template.
    displayName: Name
    name: NAME
    required: true
    value: django-psql-persistent
  - description: The OpenShift Namespace where the ImageStream resides.
    displayName: Namespace
    name: NAMESPACE
    required: true
    value: openshift
  - description: Maximum amount of memory the Django container can use.
    displayName: Memory Limit
    name: MEMORY_LIMIT
    required: true
    value: 512Mi
  - description: Maximum amount of memory the PostgreSQL container can use.
    displayName: Memory Limit (PostgreSQL)
    name: MEMORY_POSTGRESQL_LIMIT
    required: true
    value: 512Mi
  - description: Volume space available for data, e.g. 512Mi, 2Gi
    displayName: Volume Capacity
    name: VOLUME_CAPACITY
    required: true
    value: 1Gi
  - description: The URL of the repository with your application source code.
    displayName: Git Repository URL
    name: SOURCE_REPOSITORY_URL
    required: true
    value: https://github.com/openshift/django-ex.git
  - description: Set this to a branch name, tag or other ref of your repository if
      you are not using the default branch.
    displayName: Git Reference
    name: SOURCE_REPOSITORY_REF
  - description: Set this to the relative path to your project if it is not in the
      root of your repository.
    displayName: Context Directory
    name: CONTEXT_DIR
  - description: The exposed hostname that will route to the Django service, if left
      blank a value will be defaulted.
    displayName: Application Hostname
    name: APPLICATION_DOMAIN
  - description: Github trigger secret.  A difficult to guess string encoded as part
      of the webhook URL.  Not encrypted.
    displayName: GitHub Webhook Secret
    from: '[a-zA-Z0-9]{40}'
    generate: expression
    name: GITHUB_WEBHOOK_SECRET
  - displayName: Database Service Name
    name: DATABASE_SERVICE_NAME
    required: true
    value: postgresql
  - description: 'Database engine: postgresql, mysql or sqlite (default).'
    displayName: Database Engine
    name: DATABASE_ENGINE
    required: true
    value: postgresql
  - displayName: Database Name
    name: DATABASE_NAME
    required: true
    value: default
  - displayName: Database Username
    name: DATABASE_USER
    required: true
    value: django
  - displayName: Database User Password
    from: '[a-zA-Z0-9]{16}'
    generate: expression
    name: DATABASE_PASSWORD
  - description: Relative path to Gunicorn configuration file (optional).
    displayName: Application Configuration File Path
    name: APP_CONFIG
  - description: Set this to a long random string.
    displayName: Django Secret Key
    from: '[\w]{50}'
    generate: expression
    name: DJANGO_SECRET_KEY
  - description: The custom PyPi index URL
    displayName: Custom PyPi Index URL
    name: PIP_INDEX_URL
- apiVersion: v1
  kind: Template
  metadata:
    annotations:
      description: An example .NET Core application.
      iconClass: icon-dotnet
      openshift.io/display-name: .NET Core Example
      tags: quickstart,dotnet,.net
      template.openshift.io/documentation-url: https://github.com/redhat-developer/s2i-dotnetcore
      template.openshift.io/provider-display-name: Red Hat, Inc.
      template.openshift.io/support-url: https://access.redhat.com
    creationTimestamp: 2017-08-23T07:04:10Z
    name: dotnet-example
    namespace: openshift
    resourceVersion: "864"
    selfLink: /oapi/v1/namespaces/openshift/templates/dotnet-example
    uid: 435c0b64-87d1-11e7-9a67-ecf4bbe2f144
  objects:
  - apiVersion: v1
    kind: Route
    metadata:
      name: ${NAME}
    spec:
      host: ${APPLICATION_DOMAIN}
      to:
        kind: Service
        name: ${NAME}
  - apiVersion: v1
    kind: Service
    metadata:
      annotations:
        description: Exposes and load balances the application pods
      name: ${NAME}
    spec:
      ports:
      - name: web
        port: 8080
        targetPort: 8080
      selector:
        name: ${NAME}
  - apiVersion: v1
    kind: ImageStream
    metadata:
      annotations:
        description: Keeps track of changes in the application image
      name: ${NAME}
  - apiVersion: v1
    kind: BuildConfig
    metadata:
      annotations:
        description: Defines how to build the application
      name: ${NAME}
    spec:
      output:
        to:
          kind: ImageStreamTag
          name: ${NAME}:latest
      source:
        contextDir: ${CONTEXT_DIR}
        git:
          ref: ${SOURCE_REPOSITORY_REF}
          uri: ${SOURCE_REPOSITORY_URL}
        type: Git
      strategy:
        sourceStrategy:
          env:
          - name: DOTNET_STARTUP_PROJECT
            value: ${DOTNET_STARTUP_PROJECT}
          - name: DOTNET_ASSEMBLY_NAME
            value: ${DOTNET_ASSEMBLY_NAME}
          - name: DOTNET_NPM_TOOLS
            value: ${DOTNET_NPM_TOOLS}
          - name: DOTNET_TEST_PROJECTS
            value: ${DOTNET_TEST_PROJECTS}
          - name: DOTNET_CONFIGURATION
            value: ${DOTNET_CONFIGURATION}
          - name: DOTNET_PUBLISH
            value: "true"
          - name: DOTNET_RESTORE_SOURCES
            value: ${DOTNET_RESTORE_SOURCES}
          from:
            kind: ImageStreamTag
            name: ${DOTNET_IMAGE_STREAM_TAG}
            namespace: ${NAMESPACE}
        type: Source
      triggers:
      - type: ImageChange
      - type: ConfigChange
      - github:
          secret: ${GITHUB_WEBHOOK_SECRET}
        type: GitHub
      - generic:
          secret: ${GENERIC_WEBHOOK_SECRET}
        type: Generic
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      annotations:
        description: Defines how to deploy the application server
      name: ${NAME}
    spec:
      replicas: 1
      selector:
        name: ${NAME}
      strategy:
        type: Rolling
      template:
        metadata:
          labels:
            name: ${NAME}
          name: ${NAME}
        spec:
          containers:
          - env: []
            image: ' '
            livenessProbe:
              httpGet:
                path: /
                port: 8080
                scheme: HTTP
              initialDelaySeconds: 40
              timeoutSeconds: 15
            name: dotnet-app
            ports:
            - containerPort: 8080
            readinessProbe:
              httpGet:
                path: /
                port: 8080
                scheme: HTTP
              initialDelaySeconds: 10
              timeoutSeconds: 30
            resources:
              limits:
                memory: ${MEMORY_LIMIT}
      triggers:
      - imageChangeParams:
          automatic: true
          containerNames:
          - dotnet-app
          from:
            kind: ImageStreamTag
            name: ${NAME}:latest
        type: ImageChange
      - type: ConfigChange
  parameters:
  - description: The name assigned to all of the frontend objects defined in this
      template.
    displayName: Name
    name: NAME
    required: true
    value: dotnet-example
  - description: Maximum amount of memory the container can use.
    displayName: Memory Limit
    name: MEMORY_LIMIT
    required: true
    value: 512Mi
  - description: The image stream tag which is used to build the code.
    displayName: .NET builder
    name: DOTNET_IMAGE_STREAM_TAG
    required: true
    value: dotnet:1.0
  - description: The OpenShift Namespace where the ImageStream resides.
    displayName: Namespace
    name: NAMESPACE
    required: true
    value: openshift
  - description: The URL of the repository with your application source code.
    displayName: Git Repository URL
    name: SOURCE_REPOSITORY_URL
    required: true
    value: https://github.com/redhat-developer/s2i-dotnetcore-ex.git
  - description: Set this to a branch name, tag or other ref of your repository if
      you are not using the default branch.
    displayName: Git Reference
    name: SOURCE_REPOSITORY_REF
    value: dotnetcore-1.0
  - description: Set this to use a subdirectory of the source code repository
    displayName: Context Directory
    name: CONTEXT_DIR
  - description: The exposed hostname that will route to the .NET Core service, if
      left blank a value will be defaulted.
    displayName: Application Hostname
    name: APPLICATION_DOMAIN
  - description: A secret string used to configure the GitHub webhook.
    displayName: GitHub Webhook Secret
    from: '[a-zA-Z0-9]{40}'
    generate: expression
    name: GITHUB_WEBHOOK_SECRET
  - description: A secret string used to configure the Generic webhook.
    displayName: Generic Webhook Secret
    from: '[a-zA-Z0-9]{40}'
    generate: expression
    name: GENERIC_WEBHOOK_SECRET
  - description: Set this to the folder containing your startup project.
    displayName: Startup Project
    name: DOTNET_STARTUP_PROJECT
    value: app
  - description: Set this when the assembly name is overridden in the project file.
    displayName: Startup Assembly
    name: DOTNET_ASSEMBLY_NAME
  - description: Set this to a space separated list of npm tools needed to publish.
    displayName: Npm Tools
    name: DOTNET_NPM_TOOLS
    value: bower gulp
  - description: Set this to a space separated list of test projects to run before
      publishing.
    displayName: Test projects
    name: DOTNET_TEST_PROJECTS
  - description: Set this to configuration (Release/Debug).
    displayName: Configuration
    name: DOTNET_CONFIGURATION
    value: Release
  - description: Set this to override the NuGet.config sources.
    displayName: NuGet package sources
    name: DOTNET_RESTORE_SOURCES
- apiVersion: v1
  kind: Template
  labels:
    template: dotnet-pgsql-persistent
  message: |-
    The following service(s) have been created in your project: ${NAME}, ${DATABASE_SERVICE_NAME}.

    For more information about using this template, including OpenShift considerations, see https://github.com/redhat-developer/s2i-dotnetcore.
  metadata:
    annotations:
      description: An example .NET Core application with a PostgreSQL database. For
        more information about using this template, including OpenShift considerations,
        see https://github.com/redhat-developer/s2i-dotnetcore.
      iconClass: icon-dotnet
      openshift.io/display-name: .NET Core + PostgreSQL (Persistent)
      tags: quickstart,dotnet
      template.openshift.io/documentation-url: https://github.com/redhat-developer/s2i-dotnetcore
      template.openshift.io/provider-display-name: Red Hat, Inc.
      template.openshift.io/support-url: https://access.redhat.com
    creationTimestamp: 2017-08-23T07:04:10Z
    name: dotnet-pgsql-persistent
    namespace: openshift
    resourceVersion: "865"
    selfLink: /oapi/v1/namespaces/openshift/templates/dotnet-pgsql-persistent
    uid: 435d6ab5-87d1-11e7-9a67-ecf4bbe2f144
  objects:
  - apiVersion: v1
    kind: Secret
    metadata:
      name: ${NAME}
    stringData:
      connect-string: Host=${DATABASE_SERVICE_NAME};Database=${DATABASE_NAME};Username=${DATABASE_USER};Password=${DATABASE_PASSWORD}
      database-password: ${DATABASE_PASSWORD}
  - apiVersion: v1
    kind: Service
    metadata:
      annotations:
        description: Exposes and load balances the application pods
        service.alpha.openshift.io/dependencies: '[{"name": "${DATABASE_SERVICE_NAME}",
          "kind": "Service"}]'
      name: ${NAME}
    spec:
      ports:
      - name: web
        port: 8080
        targetPort: 8080
      selector:
        name: ${NAME}
  - apiVersion: v1
    kind: Route
    metadata:
      name: ${NAME}
    spec:
      host: ${APPLICATION_DOMAIN}
      to:
        kind: Service
        name: ${NAME}
  - apiVersion: v1
    kind: ImageStream
    metadata:
      annotations:
        description: Keeps track of changes in the application image
      name: ${NAME}
  - apiVersion: v1
    kind: BuildConfig
    metadata:
      annotations:
        description: Defines how to build the application
      name: ${NAME}
    spec:
      output:
        to:
          kind: ImageStreamTag
          name: ${NAME}:latest
      postCommit: {}
      source:
        contextDir: ${CONTEXT_DIR}
        git:
          ref: ${SOURCE_REPOSITORY_REF}
          uri: ${SOURCE_REPOSITORY_URL}
        type: Git
      strategy:
        sourceStrategy:
          env:
          - name: DOTNET_STARTUP_PROJECT
            value: ${DOTNET_STARTUP_PROJECT}
          - name: DOTNET_ASSEMBLY_NAME
            value: ${DOTNET_ASSEMBLY_NAME}
          - name: DOTNET_NPM_TOOLS
            value: ${DOTNET_NPM_TOOLS}
          - name: DOTNET_TEST_PROJECTS
            value: ${DOTNET_TEST_PROJECTS}
          - name: DOTNET_CONFIGURATION
            value: ${DOTNET_CONFIGURATION}
          - name: DOTNET_PUBLISH
            value: "true"
          - name: DOTNET_RESTORE_SOURCES
            value: ${DOTNET_RESTORE_SOURCES}
          from:
            kind: ImageStreamTag
            name: ${DOTNET_IMAGE_STREAM_TAG}
            namespace: ${NAMESPACE}
        type: Source
      triggers:
      - type: ImageChange
      - type: ConfigChange
      - github:
          secret: ${GITHUB_WEBHOOK_SECRET}
        type: GitHub
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      annotations:
        description: Defines how to deploy the application server
      name: ${NAME}
    spec:
      replicas: 1
      selector:
        name: ${NAME}
      strategy:
        resources: {}
        rollingParams:
          intervalSeconds: 1
          maxSurge: 25%
          maxUnavailable: 25%
          timeoutSeconds: 600
          updatePeriodSeconds: 1
        type: Rolling
      template:
        metadata:
          labels:
            name: ${NAME}
          name: ${NAME}
        spec:
          containers:
          - env:
            - name: ConnectionString
              valueFrom:
                secretKeyRef:
                  key: connect-string
                  name: ${NAME}
            image: ' '
            livenessProbe:
              httpGet:
                path: /
                port: 8080
                scheme: HTTP
              initialDelaySeconds: 40
              timeoutSeconds: 10
            name: dotnet-pgsql-persistent
            ports:
            - containerPort: 8080
            readinessProbe:
              httpGet:
                path: /
                port: 8080
                scheme: HTTP
              initialDelaySeconds: 10
              timeoutSeconds: 30
            resources:
              limits:
                memory: ${MEMORY_LIMIT}
      triggers:
      - imageChangeParams:
          automatic: true
          containerNames:
          - dotnet-pgsql-persistent
          from:
            kind: ImageStreamTag
            name: ${NAME}:latest
        type: ImageChange
      - type: ConfigChange
  - apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: ${DATABASE_SERVICE_NAME}
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: ${VOLUME_CAPACITY}
  - apiVersion: v1
    kind: Service
    metadata:
      annotations:
        description: Exposes the database server
      name: ${DATABASE_SERVICE_NAME}
    spec:
      ports:
      - name: postgresql
        port: 5432
        targetPort: 5432
      selector:
        name: ${DATABASE_SERVICE_NAME}
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      annotations:
        description: Defines how to deploy the database
      name: ${DATABASE_SERVICE_NAME}
    spec:
      replicas: 1
      selector:
        name: ${DATABASE_SERVICE_NAME}
      strategy:
        type: Recreate
      template:
        metadata:
          labels:
            name: ${DATABASE_SERVICE_NAME}
          name: ${DATABASE_SERVICE_NAME}
        spec:
          containers:
          - env:
            - name: POSTGRESQL_USER
              value: ${DATABASE_USER}
            - name: POSTGRESQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: database-password
                  name: ${NAME}
            - name: POSTGRESQL_DATABASE
              value: ${DATABASE_NAME}
            - name: POSTGRESQL_MAX_CONNECTIONS
              value: ${POSTGRESQL_MAX_CONNECTIONS}
            - name: POSTGRESQL_SHARED_BUFFERS
              value: ${POSTGRESQL_SHARED_BUFFERS}
            image: ' '
            livenessProbe:
              initialDelaySeconds: 30
              tcpSocket:
                port: 5432
              timeoutSeconds: 1
            name: postgresql
            ports:
            - containerPort: 5432
            readinessProbe:
              exec:
                command:
                - /bin/sh
                - -i
                - -c
                - psql -h 127.0.0.1 -U ${POSTGRESQL_USER} -q -d ${POSTGRESQL_DATABASE}
                  -c 'SELECT 1'
              initialDelaySeconds: 5
              timeoutSeconds: 1
            resources:
              limits:
                memory: ${MEMORY_POSTGRESQL_LIMIT}
            volumeMounts:
            - mountPath: /var/lib/pgsql/data
              name: ${DATABASE_SERVICE_NAME}-data
          volumes:
          - name: ${DATABASE_SERVICE_NAME}-data
            persistentVolumeClaim:
              claimName: ${DATABASE_SERVICE_NAME}
      triggers:
      - imageChangeParams:
          automatic: true
          containerNames:
          - postgresql
          from:
            kind: ImageStreamTag
            name: postgresql:9.5
            namespace: openshift
        type: ImageChange
      - type: ConfigChange
  parameters:
  - description: The name assigned to all of the frontend objects defined in this
      template.
    displayName: Name
    name: NAME
    required: true
    value: musicstore
  - description: Maximum amount of memory the .NET Core container can use.
    displayName: Memory Limit
    name: MEMORY_LIMIT
    required: true
    value: 512Mi
  - description: Maximum amount of memory the PostgreSQL container can use.
    displayName: Memory Limit (PostgreSQL)
    name: MEMORY_POSTGRESQL_LIMIT
    required: true
    value: 512Mi
  - description: Volume space available for data, e.g. 512Mi, 2Gi
    displayName: Volume Capacity
    name: VOLUME_CAPACITY
    required: true
    value: 1Gi
  - description: The image stream tag which is used to build the code.
    displayName: .NET builder
    name: DOTNET_IMAGE_STREAM_TAG
    required: true
    value: dotnet:1.1
  - description: The OpenShift Namespace where the .NET builder ImageStream resides.
    displayName: Namespace
    name: NAMESPACE
    required: true
    value: openshift
  - description: The URL of the repository with your application source code.
    displayName: Git Repository URL
    name: SOURCE_REPOSITORY_URL
    required: true
    value: https://github.com/redhat-developer/s2i-aspnet-musicstore-ex.git
  - description: Set this to a branch name, tag or other ref of your repository if
      you are not using the default branch.
    displayName: Git Reference
    name: SOURCE_REPOSITORY_REF
    value: rel/1.1-example
  - description: Set this to the relative path to your project if it is not in the
      root of your repository.
    displayName: Context Directory
    name: CONTEXT_DIR
  - description: Set this to the folder containing your startup project.
    displayName: Startup Project
    name: DOTNET_STARTUP_PROJECT
    value: samples/MusicStore
  - description: Set this when the assembly name is overridden in the project file.
    displayName: Startup Assembly
    name: DOTNET_ASSEMBLY_NAME
  - description: Set this to a space separated list of npm tools needed to publish.
    displayName: Npm Tools
    name: DOTNET_NPM_TOOLS
  - description: Set this to a space separated list of test projects to run before
      publishing.
    displayName: Test projects
    name: DOTNET_TEST_PROJECTS
  - description: Set this to configuration (Release/Debug).
    displayName: Configuration
    name: DOTNET_CONFIGURATION
    value: Release
  - description: Set this to override the NuGet.config sources.
    displayName: NuGet package sources
    name: DOTNET_RESTORE_SOURCES
  - description: The exposed hostname that will route to the .NET Core service, if
      left blank a value will be defaulted.
    displayName: Application Hostname
    name: APPLICATION_DOMAIN
  - description: A secret string used to configure the GitHub webhook.
    displayName: GitHub Webhook Secret
    from: '[a-zA-Z0-9]{40}'
    generate: expression
    name: GITHUB_WEBHOOK_SECRET
  - displayName: Database Service Name
    name: DATABASE_SERVICE_NAME
    required: true
    value: postgresql
  - displayName: Database Username
    from: user[A-Z0-9]{3}
    generate: expression
    name: DATABASE_USER
  - displayName: Database Password
    from: '[a-zA-Z0-9]{8}'
    generate: expression
    name: DATABASE_PASSWORD
  - displayName: Database Name
    name: DATABASE_NAME
    required: true
    value: musicstore
  - displayName: Maximum Database Connections
    name: POSTGRESQL_MAX_CONNECTIONS
    value: "100"
  - displayName: Shared Buffer Amount
    name: POSTGRESQL_SHARED_BUFFERS
    value: 12MB
- apiVersion: v1
  kind: Template
  labels:
    component: es_single
    template: elasticsearch-single-persistent
  metadata:
    annotations:
      description: Create a single elasticsearch, with persistent storage.
      iconClass: icon-database
      openshift.io/display-name: elasticsearch_single (Persistent)
      tags: database,elasticsearch_single
    creationTimestamp: 2017-10-16T04:18:19Z
    name: elasticsearch-single-persistent
    namespace: openshift
    resourceVersion: "27449453"
    selfLink: /oapi/v1/namespaces/openshift/templates/elasticsearch-single-persistent
    uid: 0aa2a25b-b229-11e7-9b95-ecf4bbe2f144
  objects:
  - apiVersion: v1
    kind: Service
    metadata:
      labels:
        es-name: ${NAME}
      name: elasticsearch
    spec:
      clusterIP: None
      ports:
      - name: http
        port: 9200
        protocol: TCP
      - name: transport
        port: 9300
        protocol: TCP
      selector:
        es-name: ${NAME}
  - apiVersion: apps/v1beta1
    kind: StatefulSet
    metadata:
      labels:
        es-name: ${NAME}
      name: ${NAME}
    spec:
      replicas: "1"
      serviceName: elasticsearch
      template:
        metadata:
          labels:
            component: es_single
            es-name: ${NAME}
            template: elasticsearch-single-persistent
        spec:
          affinity:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: es-name
                      operator: In
                      values:
                      - ${NAME}
                  topologyKey: kubernetes.io/hostname
                weight: 100
          containers:
          - env:
            - name: KUBERNETES_CA_CERTIFICATE_FILE
              value: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: CLUSTER_NAME
              value: es.ycsb.cluster
            - name: DISCOVERY_SERVICE
              value: es_single
            - name: NODE_MASTER
              value: "true"
            - name: NODE_DATA
              value: "true"
            - name: HTTP_ENABLE
              value: "true"
            - name: ES_JAVA_OPTS
              value: -Xmx${ES_JAVA_OPTS_MEM_SIZE}m -Xms${ES_JAVA_OPTS_MEM_SIZE}m
            image: ${SOURCE_IMAGE}
            imagePullPolicy: Always
            name: ${NAME}
            ports:
            - containerPort: 9200
              name: http
              protocol: TCP
            - containerPort: 9300
              name: transport
              protocol: TCP
            resources:
              limits:
                cpu: ${RESOURCE_CPU_LIMIT}
                memory: ${RESOURCE_MEMORY_LIMIT}G
              requests:
                cpu: 300m
                memory: 300M
            securityContext:
              privileged: true
              runAsUser: 0
            volumeMounts:
            - mountPath: /data
              name: storage
          terminationGracePeriodSeconds: 10
      volumeClaimTemplates:
      - metadata:
          annotations:
            volume.alpha.kubernetes.io/storage-class: slow
            volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/rbd
          name: storage
        spec:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: ${VOLUME_ES_CAPACITY}Gi
  parameters:
  - description: Name.
    name: NAME
    required: true
    value: es-single
  - description: Container image source.
    name: SOURCE_IMAGE
    required: true
    value: openshift/elasticsearch-kubernetes-single:v5.6.0
  - description: ES_JAVA_MEMSize_MB
    name: ES_JAVA_OPTS_MEM_SIZE
    required: true
    value: "9000"
  - description: ES Data capacity.
    name: VOLUME_ES_CAPACITY
    required: true
    value: "30"
  - description: The limits for memory resource.
    name: RESOURCE_MEMORY_LIMIT
    value: "12"
  - description: The limits for CPU resource.
    name: RESOURCE_CPU_LIMIT
    value: "8"
- apiVersion: v1
  kind: Template
  labels:
    app: es-client
    template: es-client
  metadata:
    annotations:
      description: Create a es-client cluster.
      iconClass: icon-database
      openshift.io/display-name: ES-Client
      tags: database,es-client
    creationTimestamp: 2017-10-10T07:40:48Z
    name: es-client
    namespace: openshift
    resourceVersion: "26411895"
    selfLink: /oapi/v1/namespaces/openshift/templates/es-client
    uid: 55140b1a-ad8e-11e7-b80c-ecf4bbe2f144
  objects:
  - apiVersion: v1
    data:
      elasticsearch.yml: |-
        cluster:
          name: ${CLUSTER_NAME}

        node:
          master: ${NODE_MASTER}
          data: ${NODE_DATA}
          name: ${NODE_NAME}
          ingest: ${NODE_INGEST}
          max_local_storage_nodes: ${MAX_LOCAL_STORAGE_NODES}

        network.host: ${NETWORK_HOST}
        action.auto_create_index: false

        path:
          data: /data/data
          logs: /data/log

        bootstrap:
          memory_lock: ${MEMORY_LOCK}

        http:
          enabled: ${HTTP_ENABLE}
          compression: true
          cors:
            enabled: ${HTTP_CORS_ENABLE}
            allow-origin: ${HTTP_CORS_ALLOW_ORIGIN}

        discovery:
          zen:
            ping.unicast.hosts: ${DISCOVERY_SERVICE}
            minimum_master_nodes: ${NUMBER_OF_MASTERS}
    kind: ConfigMap
    metadata:
      name: es-config
  - apiVersion: v1
    kind: Service
    metadata:
      labels:
        component: elasticsearch
        role: ${ES_CLIENT_NAME}
      name: elasticsearch
    spec:
      ports:
      - name: http
        port: 9200
        protocol: TCP
      - name: tran
        port: 9300
        protocol: TCP
      selector:
        component: elasticsearch
        role: ${ES_CLIENT_NAME}
      type: LoadBalancer
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      labels:
        component: elasticsearch
        role: ${ES_CLIENT_NAME}
      name: ${ES_CLIENT_NAME}
    spec:
      replicas: ${REPLICAS}
      selector:
        component: elasticsearch
        role: ${ES_CLIENT_NAME}
      strategy:
        type: Rolling
      template:
        metadata:
          creationTimestamp: null
          labels:
            component: elasticsearch
            role: ${ES_CLIENT_NAME}
        spec:
          affinity:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: role
                      operator: In
                      values:
                      - ${ES_CLIENT_NAME}
                  topologyKey: kubernetes.io/hostname
                weight: 100
          containers:
          - env:
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
            - name: CLUSTER_NAME
              value: es.ycsb.cluster
            - name: NODE_MASTER
              value: "false"
            - name: NODE_DATA
              value: "false"
            - name: HTTP_ENABLE
              value: "true"
            - name: ES_JAVA_OPTS
              value: -Xmx${ES_JAVA_OPTS_MEM_SIZE}m -Xms${ES_JAVA_OPTS_MEM_SIZE}m
            image: ${ES_CLIENT_IMAGE}
            imagePullPolicy: Always
            livenessProbe:
              tcpSocket:
                port: 9200
            name: ${ES_CLIENT_NAME}
            ports:
            - containerPort: 9200
              name: http
              protocol: TCP
            - containerPort: 9300
              name: transport
              protocol: TCP
            resources:
              limits:
                cpu: ${CLIENT_CPU_LIMIT}
                memory: ${CLIENT_MEM_LIMIT}G
              requests:
                cpu: 100m
                memory: 100M
            securityContext:
              privileged: true
              runAsUser: 0
            terminationMessagePath: /dev/termination-log
            volumeMounts:
            - mountPath: /data
              name: storage
            - mountPath: /elasticsearch/config/elasticsearch.yml
              name: conf-volume
              subPath: elasticsearch.yml
          dnsPolicy: ClusterFirst
          initContainers:
          - command:
            - sysctl
            - -w
            - vm.max_map_count=262144
            image: busybox
            imagePullPolicy: Always
            name: init-sysctl
            securityContext:
              privileged: true
          restartPolicy: Always
          securityContext: {}
          terminationGracePeriodSeconds: 10
          volumes:
          - emptyDir: {}
            name: storage
          - configMap:
              items:
              - key: elasticsearch.yml
                path: elasticsearch.yml
              name: es-config
            name: conf-volume
      triggers:
      - type: ConfigChange
  parameters:
  - description: Name of the es-client image
    name: ES_CLIENT_IMAGE
    required: true
    value: docker-elasticsearch-kubernetes:5.6.0
  - description: es-client name used as a service name and a selector
    name: ES_CLIENT_NAME
    required: true
    value: esclient
  - description: ES_JAVA_MEMSize MB
    name: ES_JAVA_OPTS_MEM_SIZE
    required: true
    value: "2000"
  - description: CLIENT_NODE_CPU_LIMIT_CORES
    name: CLIENT_CPU_LIMIT
    required: true
    value: "2"
  - description: CLIENT_NODE_MEM_LIMIT_GB
    name: CLIENT_MEM_LIMIT
    required: true
    value: "4"
  - description: Number of replicas.
    name: REPLICAS
    required: true
    value: "2"
- apiVersion: v1
  kind: Template
  labels:
    app: es-data
    template: es-data-persistent
  metadata:
    annotations:
      description: Create a es-data cluster, with persistent storage.
      iconClass: icon-database
      openshift.io/display-name: ES-DATA (Persistent)
      tags: database,es-data
    creationTimestamp: 2017-10-10T07:48:39Z
    name: es-data-persistent
    namespace: openshift
    resourceVersion: "26411825"
    selfLink: /oapi/v1/namespaces/openshift/templates/es-data-persistent
    uid: 6e2a7d8c-ad8f-11e7-b80c-ecf4bbe2f144
  objects:
  - apiVersion: v1
    data:
      elasticsearch.yml: |-
        cluster:
          name: ${CLUSTER_NAME}

        node:
          master: ${NODE_MASTER}
          data: ${NODE_DATA}
          name: ${NODE_NAME}
          ingest: ${NODE_INGEST}
          max_local_storage_nodes: ${MAX_LOCAL_STORAGE_NODES}

        network.host: ${NETWORK_HOST}
        action.auto_create_index: false

        path:
          data: /data/data
          logs: /data/log

        bootstrap:
          memory_lock: ${MEMORY_LOCK}

        http:
          enabled: ${HTTP_ENABLE}
          compression: true
          cors:
            enabled: ${HTTP_CORS_ENABLE}
            allow-origin: ${HTTP_CORS_ALLOW_ORIGIN}

        discovery:
          zen:
            ping.unicast.hosts: ${DISCOVERY_SERVICE}
            minimum_master_nodes: ${NUMBER_OF_MASTERS}
    kind: ConfigMap
    metadata:
      name: es-config
  - apiVersion: v1
    kind: Service
    metadata:
      labels:
        component: elasticsearch
        role: ${ES_DATA_NAME}
      name: elasticsearch-data
    spec:
      clusterIP: None
      ports:
      - name: transport
        port: 9300
        protocol: TCP
        targetPort: 9300
      selector:
        component: elasticsearch
        role: ${ES_DATA_NAME}
      sessionAffinity: None
      type: ClusterIP
  - apiVersion: apps/v1beta1
    kind: StatefulSet
    metadata:
      labels:
        component: elasticsearch
        role: ${ES_DATA_NAME}
      name: ${ES_DATA_NAME}
    spec:
      replicas: ${REPLICAS}
      selector:
        matchLabels:
          component: elasticsearch
          role: ${ES_DATA_NAME}
      serviceName: elasticsearch-data
      template:
        metadata:
          creationTimestamp: null
          labels:
            component: elasticsearch
            role: ${ES_DATA_NAME}
        spec:
          affinity:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: role
                      operator: In
                      values:
                      - ${ES_DATA_NAME}
                  topologyKey: kubernetes.io/hostname
                weight: 100
          containers:
          - env:
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
            - name: CLUSTER_NAME
              value: es.ycsb.cluster
            - name: NODE_MASTER
              value: "false"
            - name: NODE_DATA
              value: "true"
            - name: NODE_INGEST
              value: "false"
            - name: HTTP_ENABLE
              value: "false"
            - name: ES_JAVA_OPTS
              value: -Xmx${ES_JAVA_OPTS_MEM_SIZE}m -Xms${ES_JAVA_OPTS_MEM_SIZE}m
            image: ${ES_DATA_IMAGE}
            imagePullPolicy: Always
            livenessProbe:
              failureThreshold: 3
              initialDelaySeconds: 30
              periodSeconds: 10
              successThreshold: 1
              tcpSocket:
                port: 9300
              timeoutSeconds: 5
            name: ${ES_DATA_NAME}
            ports:
            - containerPort: 9300
              name: transport
              protocol: TCP
            resources:
              limits:
                cpu: ${DATA_CPU_LIMIT}
                memory: ${DATA_MEM_LIMIT}G
              requests:
                cpu: 100m
                memory: 100M
            securityContext:
              privileged: true
              runAsUser: 0
            terminationMessagePath: /dev/termination-log
            volumeMounts:
            - mountPath: /data
              name: storage
            - mountPath: /elasticsearch/config/elasticsearch.yml
              name: conf-volume
              subPath: elasticsearch.yml
          dnsPolicy: ClusterFirst
          initContainers:
          - command:
            - sysctl
            - -w
            - vm.max_map_count=262144
            image: busybox
            imagePullPolicy: IfNotPresent
            name: init-sysctl
            securityContext:
              privileged: true
          restartPolicy: Always
          securityContext: {}
          terminationGracePeriodSeconds: 10
          volumes:
          - configMap:
              items:
              - key: elasticsearch.yml
                path: elasticsearch.yml
              name: es-config
            name: conf-volume
      volumeClaimTemplates:
      - metadata:
          annotations:
            volume.beta.kubernetes.io/storage-class: slow
          creationTimestamp: null
          name: storage
        spec:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: ${PV_SIZE}Gi
  parameters:
  - description: Name of the es-data image
    name: ES_DATA_IMAGE
    required: true
    value: docker-elasticsearch-kubernetes:5.6.0
  - description: es data name used as a service name and a selector
    name: ES_DATA_NAME
    required: true
    value: esdata
  - description: ES_JAVA_MEMSize_MB
    name: ES_JAVA_OPTS_MEM_SIZE
    required: true
    value: "10000"
  - description: persistentVolume_size_GB
    name: PV_SIZE
    required: true
    value: "30"
  - description: DATA_NODE_CPU_LIMIT_CORES
    name: DATA_CPU_LIMIT
    required: true
    value: "8"
  - description: DATA_NODE_MEM_LIMIT_GB
    name: DATA_MEM_LIMIT
    required: true
    value: "16"
  - description: Number of replicas.
    name: REPLICAS
    required: true
    value: "3"
- apiVersion: v1
  kind: Template
  labels:
    app: es-master
    template: es-master-persistent
  metadata:
    annotations:
      description: Create a es-master cluster, with persistent storage.
      iconClass: icon-database
      openshift.io/display-name: ES-Master (Persistent)
      tags: database,es-master
    creationTimestamp: 2017-10-10T04:03:37Z
    name: es-master-persistent
    namespace: openshift
    resourceVersion: "26410298"
    selfLink: /oapi/v1/namespaces/openshift/templates/es-master-persistent
    uid: fe5c2851-ad6f-11e7-b80c-ecf4bbe2f144
  objects:
  - apiVersion: v1
    data:
      elasticsearch.yml: |-
        cluster:
          name: ${CLUSTER_NAME}

        node:
          master: ${NODE_MASTER}
          data: ${NODE_DATA}
          name: ${NODE_NAME}
          ingest: ${NODE_INGEST}
          max_local_storage_nodes: ${MAX_LOCAL_STORAGE_NODES}

        network.host: ${NETWORK_HOST}
        action.auto_create_index: false

        path:
          data: /data/data
          logs: /data/log

        bootstrap:
          memory_lock: ${MEMORY_LOCK}

        http:
          enabled: ${HTTP_ENABLE}
          compression: true
          cors:
            enabled: ${HTTP_CORS_ENABLE}
            allow-origin: ${HTTP_CORS_ALLOW_ORIGIN}

        discovery:
          zen:
            ping.unicast.hosts: ${DISCOVERY_SERVICE}
            minimum_master_nodes: ${NUMBER_OF_MASTERS}
    kind: ConfigMap
    metadata:
      name: es-config
  - apiVersion: v1
    kind: Service
    metadata:
      labels:
        component: elasticsearch
        role: ${ES_MASTER_NAME}
      name: elasticsearch-discovery
    spec:
      ports:
      - name: transport
        port: 9300
        protocol: TCP
      selector:
        component: elasticsearch
        role: ${ES_MASTER_NAME}
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      labels:
        component: elasticsearch
        role: ${ES_MASTER_NAME}
      name: ${ES_MASTER_NAME}
    spec:
      replicas: ${REPLICAS}
      selector:
        component: elasticsearch
        role: ${ES_MASTER_NAME}
      strategy:
        type: Rolling
      template:
        metadata:
          labels:
            component: elasticsearch
            role: ${ES_MASTER_NAME}
        spec:
          containers:
          - env:
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
            - name: CLUSTER_NAME
              value: es.ycsb.cluster
            - name: NUMBER_OF_MASTERS
              value: ${REPLICAS}
            - name: NODE_MASTER
              value: "true"
            - name: NODE_INGEST
              value: "false"
            - name: NODE_DATA
              value: "false"
            - name: HTTP_ENABLE
              value: "false"
            - name: ES_JAVA_OPTS
              value: -Xmx${ES_JAVA_OPTS_MEM_SIZE}m -Xms${ES_JAVA_OPTS_MEM_SIZE}m
            image: ${ES_MASTER_IMAGE}
            imagePullPolicy: Always
            livenessProbe:
              tcpSocket:
                port: 9300
            name: ${ES_MASTER_NAME}
            ports:
            - containerPort: 9300
              name: transport
              protocol: TCP
            resources:
              limits:
                cpu: ${MASTER_CPU_LIMIT}
                memory: ${MASTER_MEM_LIMIT}G
              requests:
                cpu: 100m
                memory: 100M
            securityContext:
              privileged: true
              runAsUser: 0
            terminationMessagePath: /dev/termination-log
            volumeMounts:
            - mountPath: /data
              name: storage
            - mountPath: /elasticsearch/config/elasticsearch.yml
              name: conf-volume
              subPath: elasticsearch.yml
          dnsPolicy: ClusterFirst
          initContainers:
          - command:
            - sysctl
            - -w
            - vm.max_map_count=262144
            image: busybox
            imagePullPolicy: IfNotPresent
            name: init-sysctl
            securityContext:
              privileged: true
          restartPolicy: Always
          securityContext: {}
          terminationGracePeriodSeconds: 10
          volumes:
          - emptyDir: {}
            name: storage
          - configMap:
              items:
              - key: elasticsearch.yml
                path: elasticsearch.yml
              name: es-config
            name: conf-volume
      triggers:
      - type: ConfigChange
  parameters:
  - description: Name of the es-master image
    name: ES_MASTER_IMAGE
    required: true
    value: docker-elasticsearch-kubernetes:5.6.0
  - description: es master name used as a service name and a selector
    name: ES_MASTER_NAME
    required: true
    value: esmaster
  - description: ES_JAVA_MEMSize MB
    name: ES_JAVA_OPTS_MEM_SIZE
    required: true
    value: "1000"
  - description: MASTER_NODE_CPU_LIMIT_CORES
    name: MASTER_CPU_LIMIT
    required: true
    value: "1"
  - description: MASTER_NODE_MEM_LIMIT_GB
    name: MASTER_MEM_LIMIT
    required: true
    value: "2"
  - description: Number of replicas.
    name: REPLICAS
    required: true
    value: "3"
- apiVersion: v1
  kind: Template
  labels:
    app: etcd
    template: etcd-persistent
  metadata:
    annotations:
      description: Create a Etcd cluster, with persistent storage.
      iconClass: icon-database
      openshift.io/display-name: ETCD-Cluster (Persistent)
      tags: database,etcd
    creationTimestamp: 2017-11-15T03:13:19Z
    name: etcd-cluster-persistent
    namespace: openshift
    resourceVersion: "26208168"
    selfLink: /oapi/v1/namespaces/openshift/templates/etcd-cluster-persistent
    uid: ee548158-c9b2-11e7-bee4-364284a56e59
  objects:
  - apiVersion: v1
    kind: Service
    metadata:
      labels:
        app: ${NAME}
      name: etcd
    spec:
      clusterIP: None
      ports:
      - name: client
        port: 2379
        protocol: TCP
        targetPort: 2379
      - name: peer
        port: 2380
        protocol: TCP
        targetPort: 2380
      selector:
        app: ${NAME}
      sessionAffinity: None
      type: ClusterIP
  - apiVersion: apps/v1beta1
    kind: StatefulSet
    metadata:
      labels:
        app: ${NAME}
      name: etcd
    spec:
      replicas: ${REPLICAS}
      selector:
        matchLabels:
          app: ${NAME}
      serviceName: etcd
      template:
        metadata:
          labels:
            app: ${NAME}
        spec:
          containers:
          - command:
            - /bin/sh
            - -ecx
            - |-
              IP=$(hostname -i)
              for i in $(seq 0 $((${CLUSTER_SIZE} - 1))); do
                while true; do
                  echo "Waiting for ${SET_NAME}-${i}.${SET_NAME} to come up"
                  ping -W 1 -c 1 ${SET_NAME}-${i}.${SET_NAME} > /dev/null && break
                  sleep 1s
                done
              done

              PEERS=""

              for i in $(seq 0 $((${CLUSTER_SIZE} - 1))); do
                  PEERS="${PEERS}${PEERS:+,}${SET_NAME}-${i}=http://${SET_NAME}-${i}.${SET_NAME}:2380"
              done

              exec etcd --name ${HOSTNAME} \
                --listen-peer-urls http://${IP}:2380 \
                --listen-client-urls http://${IP}:2379,http://127.0.0.1:2379 \
                --advertise-client-urls http://${HOSTNAME}.${SET_NAME}:2379 \
                --initial-advertise-peer-urls http://${HOSTNAME}.${SET_NAME}:2380 \
                --initial-cluster-token etcd-cluster-1 \
                --initial-cluster ${PEERS} \
                --initial-cluster-state new \
                --data-dir /var/run/etcd/default.etcd
            env:
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
            - name: CLUSTER_SIZE
              value: ${REPLICAS}
            - name: SET_NAME
              value: etcd
            image: ${ETCD_IMAGE}
            imagePullPolicy: Always
            livenessProbe:
              failureThreshold: 3
              initialDelaySeconds: 30
              periodSeconds: 10
              successThreshold: 1
              tcpSocket:
                port: 2379
              timeoutSeconds: 8
            name: ${NAME}
            ports:
            - containerPort: 2379
              name: client
            - containerPort: 2380
              name: peer
            resources:
              limits:
                cpu: ${RESOURCE_CPU_LIMIT}
                memory: ${RESOURCE_MEMORY_LIMIT}G
              requests:
                cpu: 300m
                memory: 300M
            securityContext:
              capabilities: {}
              privileged: true
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            volumeMounts:
            - mountPath: /var/run/etcd
              name: data
            - mountPath: /etc/localtime
              name: timestamp
          dnsPolicy: ClusterFirst
          restartPolicy: Always
          securityContext: {}
          terminationGracePeriodSeconds: 10
          volumes:
          - hostPath:
              path: /etc/localtime
            name: timestamp
      volumeClaimTemplates:
      - metadata:
          annotations:
            volume.alpha.kubernetes.io/storage-class: slow
          creationTimestamp: null
          name: data
        spec:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: ${PV_SIZE}Gi
  parameters:
  - description: Name of the tipd image
    name: ETCD_IMAGE
    required: true
    value: etcd:v3.2.3
  - description: tipd name used as a service name and a selector
    name: NAME
    required: true
    value: etcd
  - description: persistentVolume_size_GB
    name: PV_SIZE
    required: true
    value: "2"
  - description: The limits for memory resource.
    name: RESOURCE_MEMORY_LIMIT
    value: "4"
  - description: The limits for CPU resource.
    name: RESOURCE_CPU_LIMIT
    value: "2"
  - description: Number of nodes
    name: REPLICAS
    required: true
    value: "3"
- apiVersion: v1
  kind: Template
  labels:
    app: flinkcluster
    template: flinkcluster-persistent
  metadata:
    annotations:
      description: Create a flink cluster, with persistent storage.
      iconClass: icon-database
      openshift.io/display-name: FlinkCluster (Persistent)
      tags: computing,flinkcluster
    creationTimestamp: 2017-09-26T10:14:13Z
    name: flinkcluster-persistent
    namespace: openshift
    resourceVersion: "27449577"
    selfLink: /oapi/v1/namespaces/openshift/templates/flinkcluster-persistent
    uid: 71d5855e-a2a3-11e7-b83c-ecf4bbe2f144
  objects:
  - apiVersion: v1
    data:
      core-site.xml: |
        <configuration>
          <property>
            <name>fs.alluxio.impl</name>
            <value>alluxio.hadoop.FileSystem</value>
          </property>
        </configuration>
      flink-conf.yaml: "# The external address of the host on which the JobManager
        runs and can be\n# reached by the TaskManagers and any clients which want
        to connect. This setting\n# is only used in Standalone mode and may be overwritten
        on the JobManager side\n# by specifying the --host <hostname> parameter of
        the bin/jobmanager.sh executable.\n# In high availability mode, if you use
        the bin/start-cluster.sh script and setup\n# the conf/masters file, this will
        be taken care of automatically. Yarn/Mesos\n# automatically configure the
        host name based on the hostname of the node where the\n# JobManager runs.\n\njobmanager.rpc.address:
        flinkmaster-0.flinkmaster\n\n# The RPC port where the JobManager is reachable.\n\njobmanager.rpc.port:
        6123\n\n\n# The heap size for the JobManager JVM\n\njobmanager.heap.mb: 2048\n\n\n#
        The heap size for the TaskManager JVM\n\ntaskmanager.heap.mb: 2048\n\n\n#
        The number of task slots that each TaskManager offers. Each slot runs one
        parallel pipeline.\n\n# 每台taskmanager可用的solt数目，一般设置成CPU的core数 taskmanager.numberOfTaskSlots:
        ${WORKER_CPU_LIMIT}\n# NumTaskManagers（slave的个数） * NumSlotsPerTaskManager
        parallelism.default: ${PARALLELISM}\n# 酌情修改临时目录。/tmp中的数据重启就没了。 taskmanager.tmp.dirs:
        /var/flinkdata/tmp\n\n# Specify whether TaskManager memory should be allocated
        when starting up (true) or when\n# memory is required in the memory manager
        (false)\n# Important Note: For pure streaming setups, we highly recommend
        to set this value to `false`\n# as the default state backends currently do
        not use the managed memory.\n\ntaskmanager.memory.preallocate: false\n\n\n\n#==============================================================================\n#
        Web Frontend\n#==============================================================================\n\n#
        The address under which the web-based runtime monitor listens.\n#\n#jobmanager.web.address:
        0.0.0.0\n\n# The port under which the web-based runtime monitor listens.\n#
        A value of -1 deactivates the web server.\n\njobmanager.web.port: 8081\n\n#
        Flag to specify whether job submission is enabled from the web-based\n# runtime
        monitor. Uncomment to disable.\n\n#jobmanager.web.submit.enable: false\n\n#==============================================================================\n#
        HistoryServer\n#==============================================================================\n\n#
        The HistoryServer is started and stopped via bin/historyserver.sh (start|stop)\n\n#
        Directory to upload completed jobs to. Add this directory to the list of\n#
        monitored directories of the HistoryServer as well (see below).\n#jobmanager.archive.fs.dir:
        hdfs:///completed-jobs/\n\n# The address under which the web-based HistoryServer
        listens.\n#historyserver.web.address: 0.0.0.0\n\n# The port under which the
        web-based HistoryServer listens.\n#historyserver.web.port: 8082\n\n# Comma
        separated list of directories to monitor for completed jobs.\n#historyserver.archive.fs.dir:
        hdfs:///completed-jobs/\n\n# Interval in milliseconds for refreshing the monitored
        directories.\n#historyserver.archive.fs.refresh-interval: 10000\n\n#==============================================================================\n#
        Streaming state checkpointing\n#==============================================================================\n\n#
        The backend that will be used to store operator state checkpoints if\n# checkpointing
        is enabled.\n#\n# Supported backends: jobmanager, filesystem, rocksdb, <class-name-of-factory>\n#\n#
        state.backend: filesystem\n\n\n# Directory for storing checkpoints in a Flink-supported
        filesystem\n# Note: State backend must be accessible from the JobManager and
        all TaskManagers.\n# Use \"hdfs://\" for HDFS setups, \"file://\" for UNIX/POSIX-compliant
        file systems,\n# (or any local file system under Windows), or \"S3://\" for
        S3 file system.\n#\n# state.backend.fs.checkpointdir: hdfs://namenode-host:port/flink-checkpoints\n\n\n#==============================================================================\n#
        Advanced\n#==============================================================================\n\n#
        The number of buffers for the network stack.\n#\n# taskmanager.network.numberOfBuffers:
        2048\n\n\n# Directories for temporary files.\n#\n# Add a delimited list for
        multiple directories, using the system directory\n# delimiter (colon ':' on
        unix) or a comma, e.g.:\n#     /data1/tmp:/data2/tmp:/data3/tmp\n#\n# Note:
        Each directory entry is read from and written to by a different I/O\n# thread.
        You can include the same directory multiple times in order to create\n# multiple
        I/O threads against that directory. This is for example relevant for\n# high-throughput
        RAIDs.\n#\n# If not specified, the system-specific Java temporary directory
        (java.io.tmpdir\n# property) is taken.\n#\n# taskmanager.tmp.dirs: /tmp\n\n\n#
        Path to the Hadoop configuration directory.\n#\n# This configuration is used
        when writing into HDFS. Unless specified otherwise,\n# HDFS file creation
        will use HDFS default settings with respect to block-size,\n# replication
        factor, etc.\n#\n# You can also directly specify the paths to hdfs-default.xml
        and hdfs-site.xml\n# via keys 'fs.hdfs.hdfsdefault' and 'fs.hdfs.hdfssite'.\n#\nfs.hdfs.hadoopconf:
        /opt/flink/conf/\n\n\n#==============================================================================\n#
        High Availability\n#==============================================================================\n\n#
        The high-availability mode. Possible options are 'NONE' or 'zookeeper'.\n#\n#
        high-availability: zookeeper\n\n# The path where metadata for master recovery
        is persisted. While ZooKeeper stored\n# the small ground truth for checkpoint
        and leader election, this location stores\n# the larger objects, like persisted
        dataflow graphs.\n# \n# Must be a durable file system that is accessible from
        all nodes\n# (like HDFS, S3, Ceph, nfs, ...) \n#\n# high-availability.storageDir:
        hdfs:///flink/ha/\n\n# The list of ZooKeeper quorum peers that coordinate
        the high-availability\n# setup. This must be a list of the form:\n# \"host1:clientPort,host2:clientPort,...\"
        (default clientPort: 2181)\n#\n# high-availability.zookeeper.quorum: localhost:2181\n\n\n#
        ACL options are based on https://zookeeper.apache.org/doc/r3.1.2/zookeeperProgrammers.html#sc_BuiltinACLSchemes\n#
        It can be either \"creator\" (ZOO_CREATE_ALL_ACL) or \"open\" (ZOO_OPEN_ACL_UNSAFE)\n#
        The default value is \"open\" and it can be changed to \"creator\" if ZK security
        is enabled\n#\n# high-availability.zookeeper.client.acl: open\n\n#==============================================================================\n#
        Flink Cluster Security Configuration (optional configuration)\n#==============================================================================\n\n#
        Kerberos authentication for various components - Hadoop, ZooKeeper, and connectors
        -\n# may be enabled in four steps:\n# 1. configure the local krb5.conf file\n#
        2. provide Kerberos credentials (either a keytab or a ticket cache w/ kinit)\n#
        3. make the credentials available to various JAAS login contexts\n# 4. configure
        the connector to use JAAS/SASL\n\n# The below configure how Kerberos credentials
        are provided. A keytab will be used instead of\n# a ticket cache if the keytab
        path and principal are set.\n\n# security.kerberos.login.use-ticket-cache:
        true\n# security.kerberos.login.keytab: /path/to/kerberos/keytab\n# security.kerberos.login.principal:
        flink-user\n\n# The configuration below defines which JAAS login contexts\n\n#
        security.kerberos.login.contexts: Client,KafkaClient\n\n#==============================================================================\n#
        ZK Security Configuration (optional configuration)\n#==============================================================================\n\n#
        Below configurations are applicable if ZK ensemble is configured for security\n\n#
        Override below configuration to provide custom ZK service name if configured\n#
        zookeeper.sasl.service-name: zookeeper\n\n# The configuration below must match
        one of the values set in \"security.kerberos.login.contexts\"\n# zookeeper.sasl.login-context-name:
        Client\nblob.server.port: 50101"
      log4j-cli.properties: |
        ################################################################################
        #  Licensed to the Apache Software Foundation (ASF) under one
        #  or more contributor license agreements.  See the NOTICE file
        #  distributed with this work for additional information
        #  regarding copyright ownership.  The ASF licenses this file
        #  to you under the Apache License, Version 2.0 (the
        #  "License"); you may not use this file except in compliance
        #  with the License.  You may obtain a copy of the License at
        #
        #      http://www.apache.org/licenses/LICENSE-2.0
        #
        #  Unless required by applicable law or agreed to in writing, software
        #  distributed under the License is distributed on an "AS IS" BASIS,
        #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
        #  See the License for the specific language governing permissions and
        # limitations under the License.
        ################################################################################

        log4j.rootLogger=INFO, file

        # Log all infos in the given file
        log4j.appender.file=org.apache.log4j.FileAppender
        log4j.appender.file.file=${log.file}
        log4j.appender.file.append=false
        log4j.appender.file.layout=org.apache.log4j.PatternLayout
        log4j.appender.file.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n


        # Log output from org.apache.flink.yarn to the console. This is used by the
        # CliFrontend class when using a per-job YARN cluster.
        log4j.logger.org.apache.flink.yarn=INFO, console
        log4j.logger.org.apache.flink.yarn.cli.FlinkYarnSessionCli=INFO, console
        log4j.logger.org.apache.hadoop=INFO, console

        log4j.appender.console=org.apache.log4j.ConsoleAppender
        log4j.appender.console.layout=org.apache.log4j.PatternLayout
        log4j.appender.console.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n

        # suppress the warning that hadoop native libraries are not loaded (irrelevant for the client)
        log4j.logger.org.apache.hadoop.util.NativeCodeLoader=OFF

        # suppress the irrelevant (wrong) warnings from the netty channel handler
        log4j.logger.org.jboss.netty.channel.DefaultChannelPipeline=ERROR, file
      log4j-console.properties: |
        ################################################################################
        #  Licensed to the Apache Software Foundation (ASF) under one
        #  or more contributor license agreements.  See the NOTICE file
        #  distributed with this work for additional information
        #  regarding copyright ownership.  The ASF licenses this file
        #  to you under the Apache License, Version 2.0 (the
        #  "License"); you may not use this file except in compliance
        #  with the License.  You may obtain a copy of the License at
        #
        #      http://www.apache.org/licenses/LICENSE-2.0
        #
        #  Unless required by applicable law or agreed to in writing, software
        #  distributed under the License is distributed on an "AS IS" BASIS,
        #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
        #  See the License for the specific language governing permissions and
        # limitations under the License.
        ################################################################################

        # This affects logging for both user code and Flink
        log4j.rootLogger=INFO, console

        # Uncomment this if you want to _only_ change Flink's logging
        #log4j.logger.org.apache.flink=INFO

        # The following lines keep the log level of common libraries/connectors on
        # log level INFO. The root logger does not override this. You have to manually
        # change the log levels here.
        log4j.logger.akka=INFO
        log4j.logger.org.apache.kafka=INFO
        log4j.logger.org.apache.hadoop=INFO
        log4j.logger.org.apache.zookeeper=INFO

        # Log all infos to the console
        log4j.appender.console=org.apache.log4j.ConsoleAppender
        log4j.appender.console.layout=org.apache.log4j.PatternLayout
        log4j.appender.console.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n

        # Suppress the irrelevant (wrong) warnings from the Netty channel handler
        log4j.logger.org.jboss.netty.channel.DefaultChannelPipeline=ERROR, console
      log4j.properties: |
        ################################################################################
        #  Licensed to the Apache Software Foundation (ASF) under one
        #  or more contributor license agreements.  See the NOTICE file
        #  distributed with this work for additional information
        #  regarding copyright ownership.  The ASF licenses this file
        #  to you under the Apache License, Version 2.0 (the
        #  "License"); you may not use this file except in compliance
        #  with the License.  You may obtain a copy of the License at
        #
        #      http://www.apache.org/licenses/LICENSE-2.0
        #
        #  Unless required by applicable law or agreed to in writing, software
        #  distributed under the License is distributed on an "AS IS" BASIS,
        #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
        #  See the License for the specific language governing permissions and
        # limitations under the License.
        ################################################################################

        log4j.rootLogger=INFO, file, console

        # Log all infos in the given file
        log4j.appender.file=org.apache.log4j.FileAppender
        log4j.appender.file.file=${log.file}
        log4j.appender.file.append=false
        log4j.appender.file.layout=org.apache.log4j.PatternLayout
        log4j.appender.file.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n

        # Console output
        log4j.appender.console=org.apache.log4j.ConsoleAppender
        log4j.appender.console.target=System.err
        log4j.appender.console.layout=org.apache.log4j.PatternLayout
        log4j.appender.console.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n

        # suppress the irrelevant (wrong) warnings from the netty channel handler
        log4j.logger.org.jboss.netty.channel.DefaultChannelPipeline=ERROR, file
      logback-console.xml: |
        <!--
          ~ Licensed to the Apache Software Foundation (ASF) under one
          ~ or more contributor license agreements.  See the NOTICE file
          ~ distributed with this work for additional information
          ~ regarding copyright ownership.  The ASF licenses this file
          ~ to you under the Apache License, Version 2.0 (the
          ~ "License"); you may not use this file except in compliance
          ~ with the License.  You may obtain a copy of the License at
          ~
          ~     http://www.apache.org/licenses/LICENSE-2.0
          ~
          ~ Unless required by applicable law or agreed to in writing, software
          ~ distributed under the License is distributed on an "AS IS" BASIS,
          ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
          ~ See the License for the specific language governing permissions and
          ~ limitations under the License.
          -->

        <configuration>
            <appender name="console" class="ch.qos.logback.core.ConsoleAppender">
                <encoder>
                    <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{60} %X{sourceThread} - %msg%n</pattern>
                </encoder>
            </appender>

            <!-- This affects logging for both user code and Flink -->
            <root level="INFO">
                <appender-ref ref="console"/>
            </root>

            <!-- Uncomment this if you want to only change Flink's logging -->
            <!--<logger name="org.apache.flink" level="INFO">-->
                <!--<appender-ref ref="console"/>-->
            <!--</logger>-->

            <!-- The following lines keep the log level of common libraries/connectors on
                 log level INFO. The root logger does not override this. You have to manually
                 change the log levels here. -->
            <logger name="akka" level="INFO">
                <appender-ref ref="console"/>
            </logger>
            <logger name="org.apache.kafka" level="INFO">
                <appender-ref ref="console"/>
            </logger>
            <logger name="org.apache.hadoop" level="INFO">
                <appender-ref ref="console"/>
            </logger>
            <logger name="org.apache.zookeeper" level="INFO">
                <appender-ref ref="console"/>
            </logger>

            <!-- Suppress the irrelevant (wrong) warnings from the Netty channel handler -->
            <logger name="org.jboss.netty.channel.DefaultChannelPipeline" level="ERROR">
                <appender-ref ref="console"/>
            </logger>
        </configuration>
      logback.xml: |
        <!--
          ~ Licensed to the Apache Software Foundation (ASF) under one
          ~ or more contributor license agreements.  See the NOTICE file
          ~ distributed with this work for additional information
          ~ regarding copyright ownership.  The ASF licenses this file
          ~ to you under the Apache License, Version 2.0 (the
          ~ "License"); you may not use this file except in compliance
          ~ with the License.  You may obtain a copy of the License at
          ~
          ~     http://www.apache.org/licenses/LICENSE-2.0
          ~
          ~ Unless required by applicable law or agreed to in writing, software
          ~ distributed under the License is distributed on an "AS IS" BASIS,
          ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
          ~ See the License for the specific language governing permissions and
          ~ limitations under the License.
          -->

        <configuration>
            <appender name="file" class="ch.qos.logback.core.FileAppender">
                <file>${log.file}</file>
                <append>false</append>
                <encoder>
                    <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{60} %X{sourceThread} - %msg%n</pattern>
                </encoder>
            </appender>

            <!-- This affects logging for both user code and Flink -->
            <root level="INFO">
                <appender-ref ref="file"/>
            </root>

            <!-- Uncomment this if you want to only change Flink's logging -->
            <!--<logger name="org.apache.flink" level="INFO">-->
                <!--<appender-ref ref="file"/>-->
            <!--</logger>-->

            <!-- The following lines keep the log level of common libraries/connectors on
                 log level INFO. The root logger does not override this. You have to manually
                 change the log levels here. -->
            <logger name="akka" level="INFO">
                <appender-ref ref="file"/>
            </logger>
            <logger name="org.apache.kafka" level="INFO">
                <appender-ref ref="file"/>
            </logger>
            <logger name="org.apache.hadoop" level="INFO">
                <appender-ref ref="file"/>
            </logger>
            <logger name="org.apache.zookeeper" level="INFO">
                <appender-ref ref="file"/>
            </logger>

            <!-- Suppress the irrelevant (wrong) warnings from the Netty channel handler -->
            <logger name="org.jboss.netty.channel.DefaultChannelPipeline" level="ERROR">
                <appender-ref ref="file"/>
            </logger>
        </configuration>
    kind: ConfigMap
    metadata:
      name: flink-master-config
  - apiVersion: v1
    data:
      core-site.xml: |
        <configuration>
          <property>
            <name>fs.alluxio.impl</name>
            <value>alluxio.hadoop.FileSystem</value>
          </property>
        </configuration>
      flink-conf.yaml: "# The external address of the host on which the JobManager
        runs and can be\n# reached by the TaskManagers and any clients which want
        to connect. This setting\n# is only used in Standalone mode and may be overwritten
        on the JobManager side\n# by specifying the --host <hostname> parameter of
        the bin/jobmanager.sh executable.\n# In high availability mode, if you use
        the bin/start-cluster.sh script and setup\n# the conf/masters file, this will
        be taken care of automatically. Yarn/Mesos\n# automatically configure the
        host name based on the hostname of the node where the\n# JobManager runs.\n\njobmanager.rpc.address:
        flinkmaster-0.flinkmaster\n\n# The RPC port where the JobManager is reachable.\n\njobmanager.rpc.port:
        6123\n\n\n# The heap size for the JobManager JVM\n\njobmanager.heap.mb: 2048\n\n\n#
        The heap size for the TaskManager JVM\n\ntaskmanager.heap.mb: 2048\n\n\n#
        The number of task slots that each TaskManager offers. Each slot runs one
        parallel pipeline.\n# 每台taskmanager可用的solt数目，一般设置成CPU的core数 taskmanager.numberOfTaskSlots:
        ${WORKER_CPU_LIMIT}\n# NumTaskManagers（slave的个数） * NumSlotsPerTaskManager
        parallelism.default: ${PARALLELISM}\n# 酌情修改临时目录。/tmp中的数据重启就没了。 taskmanager.tmp.dirs:
        /var/flinkdata/tmp\n\n# Specify whether TaskManager memory should be allocated
        when starting up (true) or when\n# memory is required in the memory manager
        (false)\n# Important Note: For pure streaming setups, we highly recommend
        to set this value to `false`\n# as the default state backends currently do
        not use the managed memory.\n\ntaskmanager.memory.preallocate: false\n\n\n\n#==============================================================================\n#
        Web Frontend\n#==============================================================================\n\n#
        The address under which the web-based runtime monitor listens.\n#\n#jobmanager.web.address:
        0.0.0.0\n\n# The port under which the web-based runtime monitor listens.\n#
        A value of -1 deactivates the web server.\n\njobmanager.web.port: 8081\n\n#
        Flag to specify whether job submission is enabled from the web-based\n# runtime
        monitor. Uncomment to disable.\n\n#jobmanager.web.submit.enable: false\n\n#==============================================================================\n#
        HistoryServer\n#==============================================================================\n\n#
        The HistoryServer is started and stopped via bin/historyserver.sh (start|stop)\n\n#
        Directory to upload completed jobs to. Add this directory to the list of\n#
        monitored directories of the HistoryServer as well (see below).\n#jobmanager.archive.fs.dir:
        hdfs:///completed-jobs/\n\n# The address under which the web-based HistoryServer
        listens.\n#historyserver.web.address: 0.0.0.0\n\n# The port under which the
        web-based HistoryServer listens.\n#historyserver.web.port: 8082\n\n# Comma
        separated list of directories to monitor for completed jobs.\n#historyserver.archive.fs.dir:
        hdfs:///completed-jobs/\n\n# Interval in milliseconds for refreshing the monitored
        directories.\n#historyserver.archive.fs.refresh-interval: 10000\n\n#==============================================================================\n#
        Streaming state checkpointing\n#==============================================================================\n\n#
        The backend that will be used to store operator state checkpoints if\n# checkpointing
        is enabled.\n#\n# Supported backends: jobmanager, filesystem, rocksdb, <class-name-of-factory>\n#\n#
        state.backend: filesystem\n\n\n# Directory for storing checkpoints in a Flink-supported
        filesystem\n# Note: State backend must be accessible from the JobManager and
        all TaskManagers.\n# Use \"hdfs://\" for HDFS setups, \"file://\" for UNIX/POSIX-compliant
        file systems,\n# (or any local file system under Windows), or \"S3://\" for
        S3 file system.\n#\n# state.backend.fs.checkpointdir: hdfs://namenode-host:port/flink-checkpoints\n\n\n#==============================================================================\n#
        Advanced\n#==============================================================================\n\n#
        The number of buffers for the network stack.\n#\n# taskmanager.network.numberOfBuffers:
        2048\n\n\n# Directories for temporary files.\n#\n# Add a delimited list for
        multiple directories, using the system directory\n# delimiter (colon ':' on
        unix) or a comma, e.g.:\n#     /data1/tmp:/data2/tmp:/data3/tmp\n#\n# Note:
        Each directory entry is read from and written to by a different I/O\n# thread.
        You can include the same directory multiple times in order to create\n# multiple
        I/O threads against that directory. This is for example relevant for\n# high-throughput
        RAIDs.\n#\n# If not specified, the system-specific Java temporary directory
        (java.io.tmpdir\n# property) is taken.\n#\n# taskmanager.tmp.dirs: /tmp\n\n\n#
        Path to the Hadoop configuration directory.\n#\n# This configuration is used
        when writing into HDFS. Unless specified otherwise,\n# HDFS file creation
        will use HDFS default settings with respect to block-size,\n# replication
        factor, etc.\n#\n# You can also directly specify the paths to hdfs-default.xml
        and hdfs-site.xml\n# via keys 'fs.hdfs.hdfsdefault' and 'fs.hdfs.hdfssite'.\n#\nfs.hdfs.hadoopconf:
        /opt/flink/conf/\n\n\n#==============================================================================\n#
        High Availability\n#==============================================================================\n\n#
        The high-availability mode. Possible options are 'NONE' or 'zookeeper'.\n#\n#
        high-availability: zookeeper\n\n# The path where metadata for master recovery
        is persisted. While ZooKeeper stored\n# the small ground truth for checkpoint
        and leader election, this location stores\n# the larger objects, like persisted
        dataflow graphs.\n# \n# Must be a durable file system that is accessible from
        all nodes\n# (like HDFS, S3, Ceph, nfs, ...) \n#\n# high-availability.storageDir:
        hdfs:///flink/ha/\n\n# The list of ZooKeeper quorum peers that coordinate
        the high-availability\n# setup. This must be a list of the form:\n# \"host1:clientPort,host2:clientPort,...\"
        (default clientPort: 2181)\n#\n# high-availability.zookeeper.quorum: localhost:2181\n\n\n#
        ACL options are based on https://zookeeper.apache.org/doc/r3.1.2/zookeeperProgrammers.html#sc_BuiltinACLSchemes\n#
        It can be either \"creator\" (ZOO_CREATE_ALL_ACL) or \"open\" (ZOO_OPEN_ACL_UNSAFE)\n#
        The default value is \"open\" and it can be changed to \"creator\" if ZK security
        is enabled\n#\n# high-availability.zookeeper.client.acl: open\n\n#==============================================================================\n#
        Flink Cluster Security Configuration (optional configuration)\n#==============================================================================\n\n#
        Kerberos authentication for various components - Hadoop, ZooKeeper, and connectors
        -\n# may be enabled in four steps:\n# 1. configure the local krb5.conf file\n#
        2. provide Kerberos credentials (either a keytab or a ticket cache w/ kinit)\n#
        3. make the credentials available to various JAAS login contexts\n# 4. configure
        the connector to use JAAS/SASL\n\n# The below configure how Kerberos credentials
        are provided. A keytab will be used instead of\n# a ticket cache if the keytab
        path and principal are set.\n\n# security.kerberos.login.use-ticket-cache:
        true\n# security.kerberos.login.keytab: /path/to/kerberos/keytab\n# security.kerberos.login.principal:
        flink-user\n\n# The configuration below defines which JAAS login contexts\n\n#
        security.kerberos.login.contexts: Client,KafkaClient\n\n#==============================================================================\n#
        ZK Security Configuration (optional configuration)\n#==============================================================================\n\n#
        Below configurations are applicable if ZK ensemble is configured for security\n\n#
        Override below configuration to provide custom ZK service name if configured\n#
        zookeeper.sasl.service-name: zookeeper\n\n# The configuration below must match
        one of the values set in \"security.kerberos.login.contexts\"\n# zookeeper.sasl.login-context-name:
        Client\nblob.server.port: 50101"
      log4j-cli.properties: |
        ################################################################################
        #  Licensed to the Apache Software Foundation (ASF) under one
        #  or more contributor license agreements.  See the NOTICE file
        #  distributed with this work for additional information
        #  regarding copyright ownership.  The ASF licenses this file
        #  to you under the Apache License, Version 2.0 (the
        #  "License"); you may not use this file except in compliance
        #  with the License.  You may obtain a copy of the License at
        #
        #      http://www.apache.org/licenses/LICENSE-2.0
        #
        #  Unless required by applicable law or agreed to in writing, software
        #  distributed under the License is distributed on an "AS IS" BASIS,
        #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
        #  See the License for the specific language governing permissions and
        # limitations under the License.
        ################################################################################

        log4j.rootLogger=INFO, file

        # Log all infos in the given file
        log4j.appender.file=org.apache.log4j.FileAppender
        log4j.appender.file.file=${log.file}
        log4j.appender.file.append=false
        log4j.appender.file.layout=org.apache.log4j.PatternLayout
        log4j.appender.file.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n


        # Log output from org.apache.flink.yarn to the console. This is used by the
        # CliFrontend class when using a per-job YARN cluster.
        log4j.logger.org.apache.flink.yarn=INFO, console
        log4j.logger.org.apache.flink.yarn.cli.FlinkYarnSessionCli=INFO, console
        log4j.logger.org.apache.hadoop=INFO, console

        log4j.appender.console=org.apache.log4j.ConsoleAppender
        log4j.appender.console.layout=org.apache.log4j.PatternLayout
        log4j.appender.console.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n

        # suppress the warning that hadoop native libraries are not loaded (irrelevant for the client)
        log4j.logger.org.apache.hadoop.util.NativeCodeLoader=OFF

        # suppress the irrelevant (wrong) warnings from the netty channel handler
        log4j.logger.org.jboss.netty.channel.DefaultChannelPipeline=ERROR, file
      log4j-console.properties: |
        ################################################################################
        #  Licensed to the Apache Software Foundation (ASF) under one
        #  or more contributor license agreements.  See the NOTICE file
        #  distributed with this work for additional information
        #  regarding copyright ownership.  The ASF licenses this file
        #  to you under the Apache License, Version 2.0 (the
        #  "License"); you may not use this file except in compliance
        #  with the License.  You may obtain a copy of the License at
        #
        #      http://www.apache.org/licenses/LICENSE-2.0
        #
        #  Unless required by applicable law or agreed to in writing, software
        #  distributed under the License is distributed on an "AS IS" BASIS,
        #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
        #  See the License for the specific language governing permissions and
        # limitations under the License.
        ################################################################################

        # This affects logging for both user code and Flink
        log4j.rootLogger=INFO, console

        # Uncomment this if you want to _only_ change Flink's logging
        #log4j.logger.org.apache.flink=INFO

        # The following lines keep the log level of common libraries/connectors on
        # log level INFO. The root logger does not override this. You have to manually
        # change the log levels here.
        log4j.logger.akka=INFO
        log4j.logger.org.apache.kafka=INFO
        log4j.logger.org.apache.hadoop=INFO
        log4j.logger.org.apache.zookeeper=INFO

        # Log all infos to the console
        log4j.appender.console=org.apache.log4j.ConsoleAppender
        log4j.appender.console.layout=org.apache.log4j.PatternLayout
        log4j.appender.console.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n

        # Suppress the irrelevant (wrong) warnings from the Netty channel handler
        log4j.logger.org.jboss.netty.channel.DefaultChannelPipeline=ERROR, console
      log4j.properties: |
        ################################################################################
        #  Licensed to the Apache Software Foundation (ASF) under one
        #  or more contributor license agreements.  See the NOTICE file
        #  distributed with this work for additional information
        #  regarding copyright ownership.  The ASF licenses this file
        #  to you under the Apache License, Version 2.0 (the
        #  "License"); you may not use this file except in compliance
        #  with the License.  You may obtain a copy of the License at
        #
        #      http://www.apache.org/licenses/LICENSE-2.0
        #
        #  Unless required by applicable law or agreed to in writing, software
        #  distributed under the License is distributed on an "AS IS" BASIS,
        #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
        #  See the License for the specific language governing permissions and
        # limitations under the License.
        ################################################################################

        log4j.rootLogger=INFO, file, console

        # Log all infos in the given file
        log4j.appender.file=org.apache.log4j.FileAppender
        log4j.appender.file.file=${log.file}
        log4j.appender.file.append=false
        log4j.appender.file.layout=org.apache.log4j.PatternLayout
        log4j.appender.file.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n

        # Console output
        log4j.appender.console=org.apache.log4j.ConsoleAppender
        log4j.appender.console.target=System.err
        log4j.appender.console.layout=org.apache.log4j.PatternLayout
        log4j.appender.console.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n

        # suppress the irrelevant (wrong) warnings from the netty channel handler
        log4j.logger.org.jboss.netty.channel.DefaultChannelPipeline=ERROR, file
      logback-console.xml: |
        <!--
          ~ Licensed to the Apache Software Foundation (ASF) under one
          ~ or more contributor license agreements.  See the NOTICE file
          ~ distributed with this work for additional information
          ~ regarding copyright ownership.  The ASF licenses this file
          ~ to you under the Apache License, Version 2.0 (the
          ~ "License"); you may not use this file except in compliance
          ~ with the License.  You may obtain a copy of the License at
          ~
          ~     http://www.apache.org/licenses/LICENSE-2.0
          ~
          ~ Unless required by applicable law or agreed to in writing, software
          ~ distributed under the License is distributed on an "AS IS" BASIS,
          ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
          ~ See the License for the specific language governing permissions and
          ~ limitations under the License.
          -->

        <configuration>
            <appender name="console" class="ch.qos.logback.core.ConsoleAppender">
                <encoder>
                    <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{60} %X{sourceThread} - %msg%n</pattern>
                </encoder>
            </appender>

            <!-- This affects logging for both user code and Flink -->
            <root level="INFO">
                <appender-ref ref="console"/>
            </root>

            <!-- Uncomment this if you want to only change Flink's logging -->
            <!--<logger name="org.apache.flink" level="INFO">-->
                <!--<appender-ref ref="console"/>-->
            <!--</logger>-->

            <!-- The following lines keep the log level of common libraries/connectors on
                 log level INFO. The root logger does not override this. You have to manually
                 change the log levels here. -->
            <logger name="akka" level="INFO">
                <appender-ref ref="console"/>
            </logger>
            <logger name="org.apache.kafka" level="INFO">
                <appender-ref ref="console"/>
            </logger>
            <logger name="org.apache.hadoop" level="INFO">
                <appender-ref ref="console"/>
            </logger>
            <logger name="org.apache.zookeeper" level="INFO">
                <appender-ref ref="console"/>
            </logger>

            <!-- Suppress the irrelevant (wrong) warnings from the Netty channel handler -->
            <logger name="org.jboss.netty.channel.DefaultChannelPipeline" level="ERROR">
                <appender-ref ref="console"/>
            </logger>
        </configuration>
      logback.xml: |
        <!--
          ~ Licensed to the Apache Software Foundation (ASF) under one
          ~ or more contributor license agreements.  See the NOTICE file
          ~ distributed with this work for additional information
          ~ regarding copyright ownership.  The ASF licenses this file
          ~ to you under the Apache License, Version 2.0 (the
          ~ "License"); you may not use this file except in compliance
          ~ with the License.  You may obtain a copy of the License at
          ~
          ~     http://www.apache.org/licenses/LICENSE-2.0
          ~
          ~ Unless required by applicable law or agreed to in writing, software
          ~ distributed under the License is distributed on an "AS IS" BASIS,
          ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
          ~ See the License for the specific language governing permissions and
          ~ limitations under the License.
          -->

        <configuration>
            <appender name="file" class="ch.qos.logback.core.FileAppender">
                <file>${log.file}</file>
                <append>false</append>
                <encoder>
                    <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{60} %X{sourceThread} - %msg%n</pattern>
                </encoder>
            </appender>

            <!-- This affects logging for both user code and Flink -->
            <root level="INFO">
                <appender-ref ref="file"/>
            </root>

            <!-- Uncomment this if you want to only change Flink's logging -->
            <!--<logger name="org.apache.flink" level="INFO">-->
                <!--<appender-ref ref="file"/>-->
            <!--</logger>-->

            <!-- The following lines keep the log level of common libraries/connectors on
                 log level INFO. The root logger does not override this. You have to manually
                 change the log levels here. -->
            <logger name="akka" level="INFO">
                <appender-ref ref="file"/>
            </logger>
            <logger name="org.apache.kafka" level="INFO">
                <appender-ref ref="file"/>
            </logger>
            <logger name="org.apache.hadoop" level="INFO">
                <appender-ref ref="file"/>
            </logger>
            <logger name="org.apache.zookeeper" level="INFO">
                <appender-ref ref="file"/>
            </logger>

            <!-- Suppress the irrelevant (wrong) warnings from the Netty channel handler -->
            <logger name="org.jboss.netty.channel.DefaultChannelPipeline" level="ERROR">
                <appender-ref ref="file"/>
            </logger>
        </configuration>
    kind: ConfigMap
    metadata:
      name: flink-worker-config
  - apiVersion: v1
    kind: Service
    metadata:
      labels:
        flink-name: ${MASTER_NAME}
      name: jobmanager
    spec:
      clusterIP: None
      ports:
      - name: flink
        port: 6123
        protocol: TCP
        targetPort: 6123
      - name: blobserver
        port: 50101
        protocol: TCP
        targetPort: 50101
      - name: http
        port: 8081
        protocol: TCP
        targetPort: 8081
      selector:
        flink-name: ${MASTER_NAME}
  - apiVersion: v1
    kind: Service
    metadata:
      labels:
        flink-name: ${WORKER_NAME}
      name: taskmanager
    spec:
      clusterIP: None
      ports:
      - name: flink
        port: 6121
        protocol: TCP
        targetPort: 6121
      - name: blobserver
        port: 50101
        protocol: TCP
        targetPort: 50101
      - name: http
        port: 6122
        protocol: TCP
        targetPort: 6122
      selector:
        flink-name: ${WORKER_NAME}
  - apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      annotations:
        volume.beta.kubernetes.io/storage-class: nfs
        volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/glusterfs
      name: ${MASTER_NAME}
    spec:
      accessModes:
      - ReadWriteMany
      resources:
        requests:
          storage: ${PV_SIZE}Gi
  - apiVersion: apps/v1beta1
    kind: StatefulSet
    metadata:
      labels:
        flink-name: ${MASTER_NAME}
      name: ${MASTER_NAME}
    spec:
      replicas: 1
      selector:
        matchLabels:
          flink-name: ${MASTER_NAME}
      serviceName: jobmanager
      template:
        metadata:
          labels:
            flink-name: ${MASTER_NAME}
        spec:
          affinity:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: flink-name
                      operator: In
                      values:
                      - ${MASTER_NAME}
                  topologyKey: kubernetes.io/hostname
                weight: 100
          containers:
          - args:
            - jobmanager
            - jobmanager.rpc.address=${MASTER_NAME}-0.jobmanager
            - jobmanager.heap.mb=${JOBMANAGER_HEAP_SIZE}
            - taskmanager.heap.mb=${TASKMANAGER_HEAP_SIZE}
            - fs.hdfs.hadoopconf=/opt/flink/conf/
            - blob.server.port=50101
            env:
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            image: ${FLINK_IMAGE}
            imagePullPolicy: Always
            livenessProbe:
              failureThreshold: 3
              initialDelaySeconds: 30
              periodSeconds: 10
              successThreshold: 1
              tcpSocket:
                port: 6123
              timeoutSeconds: 5
            name: ${MASTER_NAME}
            ports:
            - containerPort: 6123
              protocol: TCP
            - containerPort: 8081
              protocol: TCP
            - containerPort: 50101
              protocol: TCP
            resources:
              limits:
                cpu: ${MASTER_CPU_LIMIT}
                memory: ${MASTER_MEM_LIMIT}G
              requests:
                cpu: 200m
                memory: 200M
            securityContext:
              privileged: false
              runAsUser: 0
            terminationMessagePath: /dev/termination-log
            volumeMounts:
            - mountPath: /var/flinkdata
              name: flinkdata
            - mountPath: /opt/flink/conf/
              name: config-volume
          terminationGracePeriodSeconds: 10
          volumes:
          - name: flinkdata
            persistentVolumeClaim:
              claimName: ${MASTER_NAME}
          - configMap:
              name: flink-master-config
            name: config-volume
  - apiVersion: apps/v1beta1
    kind: StatefulSet
    metadata:
      labels:
        flink-name: ${WORKER_NAME}
      name: ${WORKER_NAME}
    spec:
      replicas: ${REPLICAS}
      selector:
        matchLabels:
          flink-name: ${WORKER_NAME}
      serviceName: taskmanager
      template:
        metadata:
          labels:
            flink-name: ${WORKER_NAME}
        spec:
          affinity:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: flink-name
                      operator: In
                      values:
                      - ${WORKER_NAME}
                  topologyKey: kubernetes.io/hostname
                weight: 100
          containers:
          - args:
            - taskmanager
            - jobmanager.rpc.address=${MASTER_NAME}-0.jobmanager
            - jobmanager.heap.mb=${JOBMANAGER_HEAP_SIZE}
            - taskmanager.heap.mb=${TASKMANAGER_HEAP_SIZE}
            - fs.hdfs.hadoopconf=/opt/flink/conf/
            - blob.server.port=50101
            env:
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            image: ${FLINK_IMAGE}
            imagePullPolicy: Always
            name: ${WORKER_NAME}
            ports:
            - containerPort: 6121
              protocol: TCP
            - containerPort: 6122
              protocol: TCP
            - containerPort: 50101
              protocol: TCP
            resources:
              limits:
                cpu: ${WORKER_CPU_LIMIT}
                memory: ${WORKER_MEM_LIMIT}G
              requests:
                cpu: 200m
                memory: 200M
            securityContext:
              privileged: false
              runAsUser: 0
            terminationMessagePath: /dev/termination-log
            volumeMounts:
            - mountPath: /var/flinkdata
              name: flinkdata
            - mountPath: /opt/flink/conf/
              name: config-volume
          terminationGracePeriodSeconds: 10
          volumes:
          - name: flinkdata
            persistentVolumeClaim:
              claimName: ${MASTER_NAME}
          - configMap:
              name: flink-worker-config
            name: config-volume
  parameters:
  - description: Name of the flinkcluster master/worker image
    name: FLINK_IMAGE
    required: true
    value: openshift/flink:v1.3.2
  - description: master name used as a service name and a selector
    name: MASTER_NAME
    required: true
    value: flinkmaster
  - description: worker name used as a selector
    name: WORKER_NAME
    required: true
    value: flinkworker
  - description: persistentVolume_size_GB
    name: PV_SIZE
    required: true
    value: "20"
  - description: MASTER_NODE_CPU_LIMIT_CORES
    name: MASTER_CPU_LIMIT
    required: true
    value: "2"
  - description: MASTER_NODE_MEM_LIMIT_GB
    name: MASTER_MEM_LIMIT
    required: true
    value: "3"
  - description: WORKER_CPU_LIMIT_CORES
    name: WORKER_CPU_LIMIT
    required: true
    value: "8"
  - description: WORKER_MEM_LIMIT_GB
    name: WORKER_MEM_LIMIT
    required: true
    value: "16"
  - description: Number of replicas.
    name: REPLICAS
    required: true
    value: "3"
  - description: taskmanager.heap.MB.
    name: TASKMANAGER_HEAP_SIZE
    required: true
    value: "12000"
  - description: jobmanager.heap.MB.
    name: JOBMANAGER_HEAP_SIZE
    required: true
    value: "2000"
  - description: parallelism.default=NumTaskManagers（slave的个数） * NumSlotsPerTaskManager
    name: PARALLELISM
    required: true
    value: "24"
- apiVersion: v1
  kind: Template
  labels:
    template: httpd-example
  message: |-
    The following service(s) have been created in your project: ${NAME}.

    For more information about using this template, including OpenShift considerations, see https://github.com/openshift/httpd-ex/blob/master/README.md.
  metadata:
    annotations:
      description: An example Httpd application that serves static content. For more
        information about using this template, including OpenShift considerations,
        see https://github.com/openshift/httpd-ex/blob/master/README.md.
      iconClass: icon-apache
      openshift.io/display-name: Httpd
      tags: quickstart,httpd
      template.openshift.io/documentation-url: https://github.com/openshift/httpd-ex
      template.openshift.io/long-description: This template defines resources needed
        to develop a static application served by httpd, including a build configuration
        and application deployment configuration.
      template.openshift.io/provider-display-name: Red Hat, Inc.
      template.openshift.io/support-url: https://access.redhat.com
    creationTimestamp: 2017-08-23T07:04:10Z
    name: httpd-example
    namespace: openshift
    resourceVersion: "866"
    selfLink: /oapi/v1/namespaces/openshift/templates/httpd-example
    uid: 435f07c3-87d1-11e7-9a67-ecf4bbe2f144
  objects:
  - apiVersion: v1
    kind: Service
    metadata:
      annotations:
        description: Exposes and load balances the application pods
      name: ${NAME}
    spec:
      ports:
      - name: web
        port: 8080
        targetPort: 8080
      selector:
        name: ${NAME}
  - apiVersion: v1
    kind: Route
    metadata:
      annotations:
        template.openshift.io/expose-uri: http://{.spec.host}{.spec.path}
      name: ${NAME}
    spec:
      host: ${APPLICATION_DOMAIN}
      to:
        kind: Service
        name: ${NAME}
  - apiVersion: v1
    kind: ImageStream
    metadata:
      annotations:
        description: Keeps track of changes in the application image
      name: ${NAME}
  - apiVersion: v1
    kind: BuildConfig
    metadata:
      annotations:
        description: Defines how to build the application
      name: ${NAME}
    spec:
      output:
        to:
          kind: ImageStreamTag
          name: ${NAME}:latest
      source:
        contextDir: ${CONTEXT_DIR}
        git:
          ref: ${SOURCE_REPOSITORY_REF}
          uri: ${SOURCE_REPOSITORY_URL}
        type: Git
      strategy:
        sourceStrategy:
          from:
            kind: ImageStreamTag
            name: httpd:2.4
            namespace: ${NAMESPACE}
        type: Source
      triggers:
      - type: ImageChange
      - type: ConfigChange
      - github:
          secret: ${GITHUB_WEBHOOK_SECRET}
        type: GitHub
      - generic:
          secret: ${GENERIC_WEBHOOK_SECRET}
        type: Generic
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      annotations:
        description: Defines how to deploy the application server
      name: ${NAME}
    spec:
      replicas: 1
      selector:
        name: ${NAME}
      strategy:
        type: Rolling
      template:
        metadata:
          labels:
            name: ${NAME}
          name: ${NAME}
        spec:
          containers:
          - env: []
            image: ' '
            livenessProbe:
              httpGet:
                path: /
                port: 8080
              initialDelaySeconds: 30
              timeoutSeconds: 3
            name: httpd-example
            ports:
            - containerPort: 8080
            readinessProbe:
              httpGet:
                path: /
                port: 8080
              initialDelaySeconds: 3
              timeoutSeconds: 3
            resources:
              limits:
                memory: ${MEMORY_LIMIT}
      triggers:
      - imageChangeParams:
          automatic: true
          containerNames:
          - httpd-example
          from:
            kind: ImageStreamTag
            name: ${NAME}:latest
        type: ImageChange
      - type: ConfigChange
  parameters:
  - description: The name assigned to all of the frontend objects defined in this
      template.
    displayName: Name
    name: NAME
    required: true
    value: httpd-example
  - description: The OpenShift Namespace where the ImageStream resides.
    displayName: Namespace
    name: NAMESPACE
    required: true
    value: openshift
  - description: Maximum amount of memory the container can use.
    displayName: Memory Limit
    name: MEMORY_LIMIT
    required: true
    value: 512Mi
  - description: The URL of the repository with your application source code.
    displayName: Git Repository URL
    name: SOURCE_REPOSITORY_URL
    required: true
    value: https://github.com/openshift/httpd-ex.git
  - description: Set this to a branch name, tag or other ref of your repository if
      you are not using the default branch.
    displayName: Git Reference
    name: SOURCE_REPOSITORY_REF
  - description: Set this to the relative path to your project if it is not in the
      root of your repository.
    displayName: Context Directory
    name: CONTEXT_DIR
  - description: The exposed hostname that will route to the httpd service, if left
      blank a value will be defaulted.
    displayName: Application Hostname
    name: APPLICATION_DOMAIN
  - description: Github trigger secret.  A difficult to guess string encoded as part
      of the webhook URL.  Not encrypted.
    displayName: GitHub Webhook Secret
    from: '[a-zA-Z0-9]{40}'
    generate: expression
    name: GITHUB_WEBHOOK_SECRET
  - description: A secret string used to configure the Generic webhook.
    displayName: Generic Webhook Secret
    from: '[a-zA-Z0-9]{40}'
    generate: expression
    name: GENERIC_WEBHOOK_SECRET
- apiVersion: v1
  kind: Template
  labels:
    component: ignite
    template: ignite-persistent
  metadata:
    annotations:
      description: Create a ignite cluster, with persistent storage.
      iconClass: icon-database
      openshift.io/display-name: ignite (Persistent)
      tags: database,ignite
    creationTimestamp: 2017-09-29T03:10:50Z
    name: ignite-persistent
    namespace: openshift
    resourceVersion: "27449535"
    selfLink: /oapi/v1/namespaces/openshift/templates/ignite-persistent
    uid: cc19d1de-a4c3-11e7-9c7f-ecf4bbe2f144
  objects:
  - apiVersion: v1
    data:
      example-kube.xml: "<?xml version=\"1.0\" encoding=\"UTF-8\"?>                                                                                                                             \n<beans
        xmlns=\"http://www.springframework.org/schema/beans\"                                                                                                         \n
        \      xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"                                                                                                       \n
        \      xmlns:util=\"http://www.springframework.org/schema/util\"                                                                                                     \n
        \      xsi:schemaLocation=\"                                                                                                                                        \n
        \       http://www.springframework.org/schema/beans                                                                                                                \n
        \       http://www.springframework.org/schema/beans/spring-beans.xsd                                                                                               \n
        \       http://www.springframework.org/schema/util                                                                                                                 \n
        \       http://www.springframework.org/schema/util/spring-util.xsd\">                                                                                               \n<bean
        id=\"ignite.cfg\"                                                                                                                                              \n
        \   class=\"org.apache.ignite.configuration.IgniteConfiguration\">                                                                                                   \n
        \   <property name=\"discoverySpi\">                                                                                                                                 \n
        \       <bean class=\"org.apache.ignite.spi.discovery.tcp.TcpDiscoverySpi\">
        \                                                                                        \n
        \               <property name=\"ipFinder\">                                                                                                                         \n
        \                   <bean                                                                                                                                          \nclass=\"org.apache.ignite.spi.discovery.tcp.ipfinder.kubernetes.TcpDiscoveryKubernetesIpFinder\">
        \                                                                   \n<property
        name=\"serviceName\" value=\"ignite\"/>                                                                                                                      \n<property
        name=\"namespace\" value=\"${NAMESPACE}\"/>                                                                                                                     \n
        \            </bean>                                                                                                                                               \n
        \           </property>                                                                                                                                            \n
        \       </bean>                                                                                                                                                    \n
        \   </property>                                                                                                                                                    \n</bean>
        \                                                                                                                                                           \n</beans>
        \  "
    kind: ConfigMap
    metadata:
      name: ignite-ipfind-conf
  - apiVersion: v1
    kind: Service
    metadata:
      labels:
        ignite-name: ${NAME}
      name: ignite
    spec:
      clusterIP: None
      ports:
      - name: rest
        port: 11211
        protocol: TCP
        targetPort: 11211
      - name: communication
        port: 47100
        protocol: TCP
        targetPort: 47100
      - name: discovery
        port: 47500
        protocol: TCP
        targetPort: 47500
      - name: jmx
        port: 49112
        protocol: TCP
        targetPort: 49112
      selector:
        ignite-name: ${NAME}
  - apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      annotations:
        volume.beta.kubernetes.io/storage-class: nfs
        volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/glusterfs
      name: ${NAME}
    spec:
      accessModes:
      - ReadWriteMany
      resources:
        requests:
          storage: ${PV_SIZE}Gi
  - apiVersion: apps/v1beta1
    kind: StatefulSet
    metadata:
      labels:
        ignite-name: ${NAME}
      name: ${NAME}
    spec:
      podManagementPolicy: Parallel
      replicas: ${REPLICAS}
      selector:
        matchLabels:
          ignite-name: ${NAME}
      serviceName: ignite
      template:
        metadata:
          labels:
            component: ignite
            ignite-name: ${NAME}
            template: ignite-persistent
        spec:
          affinity:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: ignite-name
                      operator: In
                      values:
                      - ${NAME}
                  topologyKey: kubernetes.io/hostname
                weight: 100
          containers:
          - env:
            - name: JVM_OPTS
              value: -Xmx${JAVA_OPTS_MEM_SIZE}m -Xms${JAVA_OPTS_MEM_SIZE}m
            - name: OPTION_LIBS
              value: ignite-kubernetes
            - name: CONFIG_URI
              value: file:////opt/ignite/apache-ignite-fabric-${IGNITE_VERSION}-bin/config/example-kube.xml
            image: ${SOURCE_IMAGE}
            imagePullPolicy: Always
            name: ignite-node
            ports:
            - containerPort: 11211
              protocol: TCP
            - containerPort: 47100
              protocol: TCP
            - containerPort: 47500
              protocol: TCP
            - containerPort: 49112
              protocol: TCP
            resources:
              limits:
                cpu: ${RESOURCE_CPU_LIMIT}
                memory: ${RESOURCE_MEMORY_LIMIT}G
              requests:
                cpu: 300m
                memory: 300M
            securityContext:
              privileged: false
              runAsUser: 0
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            volumeMounts:
            - mountPath: /data/ignite
              name: ignite-storage
            - mountPath: /opt/ignite/apache-ignite-fabric-${IGNITE_VERSION}-bin/config/example-kube.xml
              name: ignite-conf-volume
              subPath: example-kube.xml
          terminationGracePeriodSeconds: 10
          volumes:
          - name: ignite-storage
            persistentVolumeClaim:
              claimName: ${NAME}
          - configMap:
              defaultMode: 420
              items:
              - key: example-kube.xml
                path: example-kube.xml
              name: ignite-ipfind-conf
            name: ignite-conf-volume
  parameters:
  - description: Name.
    name: NAME
    required: true
    value: ignite
  - description: namespace
    name: NAMESPACE
    required: true
  - description: Ignite Version.
    name: IGNITE_VERSION
    required: true
    value: 2.2.0
  - description: Container image source.
    name: SOURCE_IMAGE
    required: true
    value: openshift/ignite:2.2.0
  - description: Number of replicas.
    name: REPLICAS
    required: true
    value: "3"
  - description: The limits for memory resource.
    name: RESOURCE_MEMORY_LIMIT
    value: "16"
  - description: The limits for CPU resource.
    name: RESOURCE_CPU_LIMIT
    value: "2"
  - description: persistentVolume_size_GB
    name: PV_SIZE
    required: true
    value: "20"
  - description: JAVA_OPTS_MEM_SIZE
    name: JAVA_OPTS_MEM_SIZE
    required: true
    value: "8000"
- apiVersion: v1
  kind: Template
  labels:
    template: jenkins-persistent-template
  message: A Jenkins service has been created in your project.  Log into Jenkins with
    your OpenShift account.  The tutorial at https://github.com/openshift/origin/blob/master/examples/jenkins/README.md
    contains more information about using this template.
  metadata:
    annotations:
      description: |-
        Jenkins service, with persistent storage.

        NOTE: You must have persistent volumes available in your cluster to use this template.
      iconClass: icon-jenkins
      openshift.io/display-name: Jenkins (Persistent)
      tags: instant-app,jenkins
      template.openshift.io/documentation-url: https://docs.openshift.org/latest/using_images/other_images/jenkins.html
      template.openshift.io/long-description: This template deploys a Jenkins server
        capable of managing OpenShift Pipeline builds and supporting OpenShift-based
        oauth login.
      template.openshift.io/provider-display-name: Red Hat, Inc.
      template.openshift.io/support-url: https://access.redhat.com
    creationTimestamp: 2017-08-23T07:04:10Z
    name: jenkins-persistent
    namespace: openshift
    resourceVersion: "25059190"
    selfLink: /oapi/v1/namespaces/openshift/templates/jenkins-persistent
    uid: 4361a7a4-87d1-11e7-9a67-ecf4bbe2f144
  objects:
  - apiVersion: v1
    kind: Route
    metadata:
      annotations:
        template.openshift.io/expose-uri: http://{.spec.host}{.spec.path}
      creationTimestamp: null
      name: ${JENKINS_SERVICE_NAME}
    spec:
      tls:
        insecureEdgeTerminationPolicy: Redirect
        termination: edge
      to:
        kind: Service
        name: ${JENKINS_SERVICE_NAME}
  - apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: ${JENKINS_SERVICE_NAME}
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: ${VOLUME_CAPACITY}
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      creationTimestamp: null
      name: ${JENKINS_SERVICE_NAME}
    spec:
      replicas: 1
      selector:
        name: ${JENKINS_SERVICE_NAME}
      strategy:
        type: Recreate
      template:
        metadata:
          creationTimestamp: null
          labels:
            name: ${JENKINS_SERVICE_NAME}
        spec:
          containers:
          - capabilities: {}
            env:
            - name: OPENSHIFT_ENABLE_OAUTH
              value: ${ENABLE_OAUTH}
            - name: OPENSHIFT_ENABLE_REDIRECT_PROMPT
              value: "true"
            - name: OPENSHIFT_JENKINS_JVM_ARCH
              value: ${JVM_ARCH}
            - name: KUBERNETES_MASTER
              value: https://kubernetes.default:443
            - name: KUBERNETES_TRUST_CERTIFICATES
              value: "true"
            - name: JNLP_SERVICE_NAME
              value: ${JNLP_SERVICE_NAME}
            image: openshift/jenkins-2-centos7:3.6.0
            imagePullPolicy: Always
            livenessProbe:
              failureThreshold: 30
              httpGet:
                path: /login
                port: 8080
              initialDelaySeconds: 420
              timeoutSeconds: 3
            name: jenkins
            readinessProbe:
              httpGet:
                path: /login
                port: 8080
              initialDelaySeconds: 3
              timeoutSeconds: 3
            resources:
              limits:
                memory: ${MEMORY_LIMIT}
            securityContext:
              privileged: true
              runAsUser: 0
            terminationMessagePath: /dev/termination-log
            volumeMounts:
            - mountPath: /var/lib/jenkins
              name: ${JENKINS_SERVICE_NAME}-data
          dnsPolicy: ClusterFirst
          restartPolicy: Always
          serviceAccountName: ${JENKINS_SERVICE_NAME}
          volumes:
          - name: ${JENKINS_SERVICE_NAME}-data
            persistentVolumeClaim:
              claimName: ${JENKINS_SERVICE_NAME}
      triggers:
      - type: ConfigChange
  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      annotations:
        serviceaccounts.openshift.io/oauth-redirectreference.jenkins: '{"kind":"OAuthRedirectReference","apiVersion":"v1","reference":{"kind":"Route","name":"${JENKINS_SERVICE_NAME}"}}'
      name: ${JENKINS_SERVICE_NAME}
  - apiVersion: v1
    groupNames: null
    kind: RoleBinding
    metadata:
      name: ${JENKINS_SERVICE_NAME}_edit
    roleRef:
      name: edit
    subjects:
    - kind: ServiceAccount
      name: ${JENKINS_SERVICE_NAME}
  - apiVersion: v1
    kind: Service
    metadata:
      name: ${JNLP_SERVICE_NAME}
    spec:
      ports:
      - name: agent
        nodePort: 0
        port: 50000
        protocol: TCP
        targetPort: 50000
      selector:
        name: ${JENKINS_SERVICE_NAME}
      sessionAffinity: None
      type: ClusterIP
  - apiVersion: v1
    kind: Service
    metadata:
      annotations:
        service.alpha.openshift.io/dependencies: '[{"name": "${JNLP_SERVICE_NAME}",
          "namespace": "", "kind": "Service"}]'
        service.openshift.io/infrastructure: "true"
      creationTimestamp: null
      name: ${JENKINS_SERVICE_NAME}
    spec:
      ports:
      - name: web
        nodePort: 0
        port: 80
        protocol: TCP
        targetPort: 8080
      selector:
        name: ${JENKINS_SERVICE_NAME}
      sessionAffinity: None
      type: ClusterIP
  parameters:
  - description: The name of the OpenShift Service exposed for the Jenkins container.
    displayName: Jenkins Service Name
    name: JENKINS_SERVICE_NAME
    value: jenkins
  - description: The name of the service used for master/slave communication.
    displayName: Jenkins JNLP Service Name
    name: JNLP_SERVICE_NAME
    value: jenkins-jnlp
  - description: Whether to enable OAuth OpenShift integration. If false, the static
      account 'admin' will be initialized with the password 'password'.
    displayName: Enable OAuth in Jenkins
    name: ENABLE_OAUTH
    value: "true"
  - description: Whether Jenkins runs with a 32 bit (i386) or 64 bit (x86_64) JVM.
    displayName: Jenkins JVM Architecture
    name: JVM_ARCH
    value: x86_64
  - description: Maximum amount of memory the container can use.
    displayName: Memory Limit
    name: MEMORY_LIMIT
    value: 4512Mi
  - description: Volume space available for data, e.g. 512Mi, 2Gi.
    displayName: Volume Capacity
    name: VOLUME_CAPACITY
    required: true
    value: 10Gi
  - description: The OpenShift Namespace where the Jenkins ImageStream resides.
    displayName: Jenkins ImageStream Namespace
    name: NAMESPACE
    value: openshift
  - description: Name of the ImageStreamTag to be used for the Jenkins image.
    displayName: Jenkins ImageStreamTag
    name: JENKINS_IMAGE_STREAM_TAG
    value: jenkins:latest
- apiVersion: v1
  kind: Template
  labels:
    template: application-template-sample-pipeline
  message: |-
    A Jenkins server will be automatically instantiated in this project to manage
    the Pipeline BuildConfig created by this template.  You will be able to log in to
    it using your OpenShift user credentials.
  metadata:
    annotations:
      description: |-
        This example showcases the new Jenkins Pipeline integration in OpenShift,
        which performs continuous integration and deployment right on the platform.
        The template contains a Jenkinsfile - a definition of a multi-stage CI/CD process - that
        leverages the underlying OpenShift platform for dynamic and scalable
        builds. OpenShift integrates the status of your pipeline builds into the web
        console allowing you to see your entire application lifecycle in a single view.
      iconClass: icon-jenkins
      openshift.io/display-name: Pipeline Build Example
      tags: instant-app,jenkins
    creationTimestamp: 2017-11-01T10:25:17Z
    name: jenkins-pipeline-example
    namespace: openshift
    resourceVersion: "25081255"
    selfLink: /oapi/v1/namespaces/openshift/templates/jenkins-pipeline-example
    uid: f4db1835-beee-11e7-bc85-ecf4bbe2f144
  objects:
  - apiVersion: v1
    kind: BuildConfig
    metadata:
      annotations:
        pipeline.alpha.openshift.io/uses: '[{"name": "${NAME}", "namespace": "", "kind":
          "DeploymentConfig"}]'
      labels:
        name: sample-pipeline
      name: sample-pipeline
    spec:
      strategy:
        jenkinsPipelineStrategy:
          jenkinsfile: "try {\n   timeout(time: 20, unit: 'MINUTES') {\n      node('nodejs')
            {\n          stage('build') {\n            openshiftBuild(buildConfig:
            '${NAME}', showBuildLogs: 'true')\n          }\n          stage('deploy')
            {\n            openshiftDeploy(deploymentConfig: '${NAME}')\n          }\n
            \       }\n   }\n} catch (err) {\n   echo \"in catch block\"\n   echo
            \"Caught: ${err}\"\n   currentBuild.result = 'FAILURE'\n   throw err\n}
            \         "
        type: JenkinsPipeline
      triggers:
      - github:
          secret: secret101
        type: GitHub
      - generic:
          secret: secret101
        type: Generic
  - apiVersion: v1
    kind: Service
    metadata:
      annotations:
        service.alpha.openshift.io/dependencies: '[{"name": "${DATABASE_SERVICE_NAME}",
          "namespace": "", "kind": "Service"}]'
      name: ${NAME}
    spec:
      ports:
      - name: web
        port: 8080
        targetPort: 8080
      selector:
        name: ${NAME}
  - apiVersion: v1
    kind: Route
    metadata:
      name: ${NAME}
    spec:
      host: ${APPLICATION_DOMAIN}
      to:
        kind: Service
        name: ${NAME}
  - apiVersion: v1
    kind: ImageStream
    metadata:
      annotations:
        description: Keeps track of changes in the application image
      name: ${NAME}
  - apiVersion: v1
    kind: BuildConfig
    metadata:
      annotations:
        description: Defines how to build the application
      name: ${NAME}
    spec:
      output:
        to:
          kind: ImageStreamTag
          name: ${NAME}:latest
      postCommit:
        script: npm test
      source:
        contextDir: ${CONTEXT_DIR}
        git:
          ref: ${SOURCE_REPOSITORY_REF}
          uri: ${SOURCE_REPOSITORY_URL}
        type: Git
      strategy:
        sourceStrategy:
          env:
          - name: NPM_MIRROR
            value: ${NPM_MIRROR}
          from:
            kind: ImageStreamTag
            name: nodejs:4
            namespace: ${NAMESPACE}
        type: Source
      triggers:
      - github:
          secret: ${GITHUB_WEBHOOK_SECRET}
        type: GitHub
      - generic:
          secret: ${GENERIC_WEBHOOK_SECRET}
        type: Generic
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      annotations:
        description: Defines how to deploy the application server
      name: ${NAME}
    spec:
      replicas: 1
      selector:
        name: ${NAME}
      strategy:
        type: Rolling
      template:
        metadata:
          labels:
            name: ${NAME}
          name: ${NAME}
        spec:
          containers:
          - env:
            - name: DATABASE_SERVICE_NAME
              value: ${DATABASE_SERVICE_NAME}
            - name: MONGODB_USER
              value: ${DATABASE_USER}
            - name: MONGODB_PASSWORD
              value: ${DATABASE_PASSWORD}
            - name: MONGODB_DATABASE
              value: ${DATABASE_NAME}
            - name: MONGODB_ADMIN_PASSWORD
              value: ${DATABASE_ADMIN_PASSWORD}
            image: ' '
            livenessProbe:
              httpGet:
                path: /pagecount
                port: 8080
              initialDelaySeconds: 30
              timeoutSeconds: 3
            name: nodejs-mongodb-example
            ports:
            - containerPort: 8080
            readinessProbe:
              httpGet:
                path: /pagecount
                port: 8080
              initialDelaySeconds: 3
              timeoutSeconds: 3
            resources:
              limits:
                memory: ${MEMORY_LIMIT}
      triggers:
      - imageChangeParams:
          automatic: false
          containerNames:
          - nodejs-mongodb-example
          from:
            kind: ImageStreamTag
            name: ${NAME}:latest
        type: ImageChange
      - type: ConfigChange
  - apiVersion: v1
    kind: Service
    metadata:
      annotations:
        description: Exposes the database server
      name: ${DATABASE_SERVICE_NAME}
    spec:
      ports:
      - name: mongodb
        port: 27017
        targetPort: 27017
      selector:
        name: ${DATABASE_SERVICE_NAME}
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      annotations:
        description: Defines how to deploy the database
      name: ${DATABASE_SERVICE_NAME}
    spec:
      replicas: 1
      selector:
        name: ${DATABASE_SERVICE_NAME}
      strategy:
        type: Recreate
      template:
        metadata:
          labels:
            name: ${DATABASE_SERVICE_NAME}
          name: ${DATABASE_SERVICE_NAME}
        spec:
          containers:
          - env:
            - name: MONGODB_USER
              value: ${DATABASE_USER}
            - name: MONGODB_PASSWORD
              value: ${DATABASE_PASSWORD}
            - name: MONGODB_DATABASE
              value: ${DATABASE_NAME}
            - name: MONGODB_ADMIN_PASSWORD
              value: ${DATABASE_ADMIN_PASSWORD}
            image: ' '
            livenessProbe:
              initialDelaySeconds: 30
              tcpSocket:
                port: 27017
              timeoutSeconds: 1
            name: mongodb
            ports:
            - containerPort: 27017
            readinessProbe:
              exec:
                command:
                - /bin/sh
                - -i
                - -c
                - mongo 127.0.0.1:27017/$MONGODB_DATABASE -u $MONGODB_USER -p $MONGODB_PASSWORD
                  --eval="quit()"
              initialDelaySeconds: 3
              timeoutSeconds: 1
            resources:
              limits:
                memory: ${MEMORY_MONGODB_LIMIT}
            volumeMounts:
            - mountPath: /var/lib/mongodb/data
              name: ${DATABASE_SERVICE_NAME}-data
          volumes:
          - emptyDir:
              medium: ""
            name: ${DATABASE_SERVICE_NAME}-data
      triggers:
      - imageChangeParams:
          automatic: true
          containerNames:
          - mongodb
          from:
            kind: ImageStreamTag
            name: mongodb:3.2
            namespace: ${NAMESPACE}
        type: ImageChange
      - type: ConfigChange
  parameters:
  - description: The name assigned to all of the frontend objects defined in this
      template.
    displayName: Name
    name: NAME
    required: true
    value: nodejs-mongodb-example
  - description: The exposed hostname that will route to the Node.js service, if left
      blank a value will be defaulted.
    displayName: Application Hostname
    name: APPLICATION_DOMAIN
  - description: The URL of the repository with your application source code.
    displayName: Git Repository URL
    name: SOURCE_REPOSITORY_URL
    required: true
    value: https://github.com/openshift/nodejs-ex.git
  - displayName: Database Name
    name: DATABASE_NAME
    required: true
    value: sampledb
  - description: Username for MongoDB user that will be used for accessing the database.
    displayName: MongoDB Username
    from: user[A-Z0-9]{3}
    generate: expression
    name: DATABASE_USER
  - description: Password for the MongoDB user.
    displayName: MongoDB Password
    from: '[a-zA-Z0-9]{16}'
    generate: expression
    name: DATABASE_PASSWORD
  - description: Maximum amount of memory the Node.js container can use.
    displayName: Memory Limit
    name: MEMORY_LIMIT
    required: true
    value: 512Mi
  - description: Maximum amount of memory the MongoDB container can use.
    displayName: Memory Limit (MongoDB)
    name: MEMORY_MONGODB_LIMIT
    required: true
    value: 512Mi
  - displayName: Database Service Name
    name: DATABASE_SERVICE_NAME
    required: true
    value: mongodb
  - description: Password for the database admin user.
    displayName: Database Administrator Password
    from: '[a-zA-Z0-9]{16}'
    generate: expression
    name: DATABASE_ADMIN_PASSWORD
  - description: Set this to a branch name, tag or other ref of your repository if
      you are not using the default branch.
    displayName: Git Reference
    name: SOURCE_REPOSITORY_REF
  - description: Set this to the relative path to your project if it is not in the
      root of your repository.
    displayName: Context Directory
    name: CONTEXT_DIR
  - description: Github trigger secret.  A difficult to guess string encoded as part
      of the webhook URL.  Not encrypted.
    displayName: GitHub Webhook Secret
    from: '[a-zA-Z0-9]{40}'
    generate: expression
    name: GITHUB_WEBHOOK_SECRET
  - description: A secret string used to configure the Generic webhook.
    displayName: Generic Webhook Secret
    from: '[a-zA-Z0-9]{40}'
    generate: expression
    name: GENERIC_WEBHOOK_SECRET
  - description: The custom NPM mirror URL
    displayName: Custom NPM Mirror URL
    name: NPM_MIRROR
  - description: The OpenShift Namespace where the NodeJS and MongoDB ImageStreams
      reside.
    displayName: Namespace
    name: NAMESPACE
    required: true
    value: openshift
- apiVersion: v1
  kind: Template
  labels:
    component: kafka
    template: kafka-persistent
  metadata:
    annotations:
      description: Create a Kafka cluster, with persistent storage.
      iconClass: icon-database
      openshift.io/display-name: Kafka (Persistent)
      tags: messaging,kafka
    creationTimestamp: 2017-09-13T08:38:40Z
    name: kafka-persistent
    namespace: openshift
    resourceVersion: "27778366"
    selfLink: /oapi/v1/namespaces/openshift/templates/kafka-persistent
    uid: f1791c1e-985e-11e7-a1ba-ecf4bbe2f144
  objects:
  - apiVersion: v1
    kind: Service
    metadata:
      labels:
        kafka-name: ${NAME}
      name: ${NAME}
    spec:
      clusterIP: None
      ports:
      - name: server
        port: 9092
      - name: zkclient
        port: 2181
      - name: zkserver
        port: 2888
      - name: zkleader
        port: 3888
      selector:
        kafka-name: ${NAME}
  - apiVersion: apps/v1beta1
    kind: StatefulSet
    metadata:
      labels:
        kafka-name: ${NAME}
      name: ${NAME}
    spec:
      replicas: ${REPLICAS}
      serviceName: ${NAME}
      template:
        metadata:
          labels:
            component: kafka
            kafka-name: ${NAME}
            template: kafka-persistent
        spec:
          affinity:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: kafka-name
                      operator: In
                      values:
                      - ${NAME}
                  topologyKey: kubernetes.io/hostname
                weight: 100
          containers:
          - env:
            - name: KAFKA_VERSION
              value: ${KAFKA_VERSION}
            - name: SCALA_VERSION
              value: ${SCALA_VERSION}
            - name: KAFKA_REPLICAS
              value: ${REPLICAS}
            - name: KAFKA_ZK_LOCAL
              value: "false"
            - name: KAFKA_HEAP_OPTS
              value: -Xmx${JAVA_OPTS_MEM_SIZE}m -Xms${JAVA_OPTS_MEM_SIZE}m
            - name: SERVER_num_partitions
              value: ${SERVER_NUM_PARTITIONS}
            - name: SERVER_delete_topic_enable
              value: ${SERVER_DELETE_TOPIC_ENABLE}
            - name: SERVER_log_retention_hours
              value: ${SERVER_LOG_RETENTION_HOURS}
            - name: SERVER_zookeeper_connect
              value: ${SERVER_ZOOKEEPER_CONNECT}
            - name: SERVER_default_replication_factor
              value: "2"
            - name: SERVER_log_dirs
              value: /opt/kafka/data/logs
            - name: SERVER_num_replica_fetchers
              value: "3"
            - name: SERVER_zookeeper_connection_timeout_ms
              value: ${SERVER_ZOOKEEPER_CONNECT_TIMEOUT}
            image: ${SOURCE_IMAGE}
            imagePullPolicy: Always
            livenessProbe:
              exec:
                command:
                - kafka_server_status.sh
              initialDelaySeconds: ${LP_INITIAL_DELAY}
              timeoutSeconds: 5
            name: ${NAME}
            ports:
            - containerPort: 9092
              name: server
            - containerPort: 2181
              name: zkclient
            - containerPort: 2888
              name: zkserver
            - containerPort: 3888
              name: zkleader
            resources:
              limits:
                cpu: ${RESOURCE_CPU_LIMIT}
                memory: ${RESOURCE_MEMORY_LIMIT}G
              requests:
                cpu: 300m
                memory: 300M
            securityContext:
              runAsUser: 0
            volumeMounts:
            - mountPath: /opt/kafka/data
              name: kafka-data
          dnsPolicy: ClusterFirst
          restartPolicy: Always
          securityContext: {}
          terminationGracePeriodSeconds: 10
      volumeClaimTemplates:
      - metadata:
          annotations:
            volume.alpha.kubernetes.io/storage-class: slow
          name: kafka-data
        spec:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: ${VOLUME_KAFKA_CAPACITY}Gi
  parameters:
  - description: Name.
    name: NAME
    required: true
    value: kafka
  - description: Kafka Version.
    name: KAFKA_VERSION
    required: true
    value: 1.0.0
  - description: Scala Version.
    name: SCALA_VERSION
    required: true
    value: "2.12"
  - description: Container image source.
    name: SOURCE_IMAGE
    required: true
    value: openshift/kafka:1.0.0
  - description: Number of replicas.
    name: REPLICAS
    required: true
    value: "3"
  - description: JAVA_MEMSize MB
    name: JAVA_OPTS_MEM_SIZE
    required: true
    value: "2000"
  - description: |
      The default number of log partitions per topic. More partitions allow greater parallelism for consumption, but this will also result in more files across the brokers.
    name: SERVER_NUM_PARTITIONS
    required: true
    value: "1"
  - description: |
      Topic deletion enabled. Switch to enable topic deletion or not, default value is 'true'
    name: SERVER_DELETE_TOPIC_ENABLE
    value: "true"
  - description: |
      Log retention hours. The minimum age of a log file to be eligible for deletion.
    name: SERVER_LOG_RETENTION_HOURS
    value: "24"
  - description: |
      Zookeeper conection string, a list as URL with nodes separated by ','.
    name: SERVER_ZOOKEEPER_CONNECT
    required: true
    value: zk-0.zk:2181,zk-1.zk:2181,zk-2.zk:2181
  - description: |
      The max time that the client waits to establish a connection to zookeeper (ms).
    name: SERVER_ZOOKEEPER_CONNECT_TIMEOUT
    required: true
    value: "6000"
  - description: Kafka logs capacity.
    name: VOLUME_KAFKA_CAPACITY
    required: true
    value: "20"
  - description: The limits for memory resource.
    name: RESOURCE_MEMORY_LIMIT
    value: "4"
  - description: The limits for CPU resource.
    name: RESOURCE_CPU_LIMIT
    value: "2"
  - description: |
      LivenessProbe initial delay in seconds.
    name: LP_INITIAL_DELAY
    value: "40"
- apiVersion: v1
  kind: Template
  labels:
    template: mariadb-persistent-template
  message: |-
    The following service(s) have been created in your project: ${DATABASE_SERVICE_NAME}.

           Username: ${MYSQL_USER}
           Password: ${MYSQL_PASSWORD}
      Database Name: ${MYSQL_DATABASE}
     Connection URL: mysql://${DATABASE_SERVICE_NAME}:3306/

    For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/mariadb-container/blob/master/10.1/README.md.
  metadata:
    annotations:
      description: |-
        MariaDB database service, without persistent storage. For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/mariadb-container/blob/master/10.1/README.md.

        WARNING: Any data stored will be lost upon pod destruction. Only use this template for testing
      iconClass: icon-mariadb
      openshift.io/display-name: MariaDB (Ephemeral)
      tags: database,mariadb
      template.openshift.io/documentation-url: https://github.com/sclorg/mariadb-container/blob/master/10.1/README.md
      template.openshift.io/long-description: This template provides a standalone
        MariaDB server with a database created.  The database is not stored on persistent
        storage, so any restart of the service will result in all data being lost.  The
        database name, username, and password are chosen via parameters when provisioning
        this service.
      template.openshift.io/provider-display-name: Red Hat, Inc.
      template.openshift.io/support-url: https://access.redhat.com
    creationTimestamp: 2017-08-23T07:04:05Z
    name: mariadb-ephemeral
    namespace: openshift
    resourceVersion: "844"
    selfLink: /oapi/v1/namespaces/openshift/templates/mariadb-ephemeral
    uid: 4098147e-87d1-11e7-9a67-ecf4bbe2f144
  objects:
  - apiVersion: v1
    kind: Secret
    metadata:
      annotations:
        template.openshift.io/expose-password: '{.data[''database-password'']}'
        template.openshift.io/expose-root_password: '{.data[''database-root-password'']}'
        template.openshift.io/expose-username: '{.data[''database-user'']}'
      name: ${DATABASE_SERVICE_NAME}
    stringData:
      database-password: ${MYSQL_PASSWORD}
      database-root-password: ${MYSQL_ROOT_PASSWORD}
      database-user: ${MYSQL_USER}
  - apiVersion: v1
    kind: Service
    metadata:
      annotations:
        template.openshift.io/expose-uri: mysql://{.spec.clusterIP}:{.spec.ports[?(.name=="mariadb")].port}
      name: ${DATABASE_SERVICE_NAME}
    spec:
      ports:
      - name: mariadb
        port: 3306
      selector:
        name: ${DATABASE_SERVICE_NAME}
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      name: ${DATABASE_SERVICE_NAME}
    spec:
      replicas: 1
      selector:
        name: ${DATABASE_SERVICE_NAME}
      strategy:
        type: Recreate
      template:
        metadata:
          labels:
            name: ${DATABASE_SERVICE_NAME}
        spec:
          containers:
          - env:
            - name: MYSQL_USER
              valueFrom:
                secretKeyRef:
                  key: database-user
                  name: ${DATABASE_SERVICE_NAME}
            - name: MYSQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: database-password
                  name: ${DATABASE_SERVICE_NAME}
            - name: MYSQL_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: database-root-password
                  name: ${DATABASE_SERVICE_NAME}
            - name: MYSQL_DATABASE
              value: ${MYSQL_DATABASE}
            image: ' '
            imagePullPolicy: IfNotPresent
            livenessProbe:
              initialDelaySeconds: 30
              tcpSocket:
                port: 3306
              timeoutSeconds: 1
            name: mariadb
            ports:
            - containerPort: 3306
            readinessProbe:
              exec:
                command:
                - /bin/sh
                - -i
                - -c
                - MYSQL_PWD="$MYSQL_PASSWORD" mysql -h 127.0.0.1 -u $MYSQL_USER -D
                  $MYSQL_DATABASE -e 'SELECT 1'
              initialDelaySeconds: 5
              timeoutSeconds: 1
            resources:
              limits:
                memory: ${MEMORY_LIMIT}
            volumeMounts:
            - mountPath: /var/lib/mysql/data
              name: ${DATABASE_SERVICE_NAME}-data
          volumes:
          - emptyDir:
              medium: ""
            name: ${DATABASE_SERVICE_NAME}-data
      triggers:
      - imageChangeParams:
          automatic: true
          containerNames:
          - mariadb
          from:
            kind: ImageStreamTag
            name: mariadb:10.1
            namespace: ${NAMESPACE}
        type: ImageChange
      - type: ConfigChange
  parameters:
  - description: Maximum amount of memory the container can use.
    displayName: Memory Limit
    name: MEMORY_LIMIT
    required: true
    value: 512Mi
  - description: The OpenShift Namespace where the ImageStream resides.
    displayName: Namespace
    name: NAMESPACE
    value: openshift
  - description: The name of the OpenShift Service exposed for the database.
    displayName: Database Service Name
    name: DATABASE_SERVICE_NAME
    required: true
    value: mariadb
  - description: Username for MariaDB user that will be used for accessing the database.
    displayName: MariaDB Connection Username
    from: user[A-Z0-9]{3}
    generate: expression
    name: MYSQL_USER
    required: true
  - description: Password for the MariaDB connection user.
    displayName: MariaDB Connection Password
    from: '[a-zA-Z0-9]{16}'
    generate: expression
    name: MYSQL_PASSWORD
    required: true
  - description: Password for the MariaDB root user.
    displayName: MariaDB root Password
    from: '[a-zA-Z0-9]{16}'
    generate: expression
    name: MYSQL_ROOT_PASSWORD
    required: true
  - description: Name of the MariaDB database accessed.
    displayName: MariaDB Database Name
    name: MYSQL_DATABASE
    required: true
    value: sampledb
- apiVersion: v1
  kind: Template
  labels:
    template: mariadb-persistent-template
  message: |-
    The following service(s) have been created in your project: ${DATABASE_SERVICE_NAME}.

           Username: ${MYSQL_USER}
           Password: ${MYSQL_PASSWORD}
      Database Name: ${MYSQL_DATABASE}
     Connection URL: mysql://${DATABASE_SERVICE_NAME}:3306/

    For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/mariadb-container/blob/master/10.1/README.md.
  metadata:
    annotations:
      description: |-
        MariaDB database service, with persistent storage. For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/mariadb-container/blob/master/10.1/README.md.

        NOTE: Scaling to more than one replica is not supported. You must have persistent volumes available in your cluster to use this template.
      iconClass: icon-mariadb
      openshift.io/display-name: MariaDB (Persistent)
      tags: database,mariadb
      template.openshift.io/documentation-url: https://github.com/sclorg/mariadb-container/blob/master/10.1/README.md
      template.openshift.io/long-description: This template provides a standalone
        MariaDB server with a database created.  The database is stored on persistent
        storage.  The database name, username, and password are chosen via parameters
        when provisioning this service.
      template.openshift.io/provider-display-name: Red Hat, Inc.
      template.openshift.io/support-url: https://access.redhat.com
    creationTimestamp: 2017-08-23T07:04:05Z
    name: mariadb-persistent
    namespace: openshift
    resourceVersion: "923830"
    selfLink: /oapi/v1/namespaces/openshift/templates/mariadb-persistent
    uid: 4099c17d-87d1-11e7-9a67-ecf4bbe2f144
  objects:
  - apiVersion: v1
    kind: Secret
    metadata:
      annotations:
        template.openshift.io/expose-password: '{.data[''database-password'']}'
        template.openshift.io/expose-root_password: '{.data[''database-root-password'']}'
        template.openshift.io/expose-username: '{.data[''database-user'']}'
      name: ${DATABASE_SERVICE_NAME}
    stringData:
      database-password: ${MYSQL_PASSWORD}
      database-root-password: ${MYSQL_ROOT_PASSWORD}
      database-user: ${MYSQL_USER}
  - apiVersion: v1
    kind: Service
    metadata:
      annotations:
        template.openshift.io/expose-uri: mysql://{.spec.clusterIP}:{.spec.ports[?(.name=="mariadb")].port}
      name: ${DATABASE_SERVICE_NAME}
    spec:
      ports:
      - name: mariadb
        port: 3306
      selector:
        name: ${DATABASE_SERVICE_NAME}
  - apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: ${DATABASE_SERVICE_NAME}
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: ${VOLUME_CAPACITY}
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      name: ${DATABASE_SERVICE_NAME}
    spec:
      replicas: 1
      selector:
        name: ${DATABASE_SERVICE_NAME}
      strategy:
        type: Recreate
      template:
        metadata:
          labels:
            name: ${DATABASE_SERVICE_NAME}
        spec:
          containers:
          - env:
            - name: MYSQL_USER
              valueFrom:
                secretKeyRef:
                  key: database-user
                  name: ${DATABASE_SERVICE_NAME}
            - name: MYSQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: database-password
                  name: ${DATABASE_SERVICE_NAME}
            - name: MYSQL_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: database-root-password
                  name: ${DATABASE_SERVICE_NAME}
            - name: MYSQL_DATABASE
              value: ${MYSQL_DATABASE}
            image: mariadb:10.3.1
            imagePullPolicy: IfNotPresent
            livenessProbe:
              initialDelaySeconds: 30
              tcpSocket:
                port: 3306
              timeoutSeconds: 1
            name: mariadb
            ports:
            - containerPort: 3306
            readinessProbe:
              exec:
                command:
                - /bin/sh
                - -i
                - -c
                - MYSQL_PWD="$MYSQL_PASSWORD" mysql -h 127.0.0.1 -u $MYSQL_USER -D
                  $MYSQL_DATABASE -e 'SELECT 1'
              initialDelaySeconds: 5
              timeoutSeconds: 1
            resources:
              limits:
                memory: ${MEMORY_LIMIT}
            securityContext:
              privileged: true
            volumeMounts:
            - mountPath: /var/lib/mysql/data
              name: ${DATABASE_SERVICE_NAME}-data
          volumes:
          - name: ${DATABASE_SERVICE_NAME}-data
            persistentVolumeClaim:
              claimName: ${DATABASE_SERVICE_NAME}
      triggers:
      - type: ConfigChange
  - description: Maximum amount of memory the container can use.
    displayName: Memory Limit
    name: MEMORY_LIMIT
    required: true
    value: 2048Mi
  - description: The OpenShift Namespace where the ImageStream resides.
    displayName: Namespace
    name: NAMESPACE
    value: null
  - description: The name of the OpenShift Service exposed for the database.
    displayName: Database Service Name
    name: DATABASE_SERVICE_NAME
    required: true
    value: mariadb
  - description: Username for MariaDB user that will be used for accessing the database.
    displayName: MariaDB Connection Username
    from: user[A-Z0-9]{3}
    generate: expression
    name: MYSQL_USER
    required: true
  - description: Password for the MariaDB connection user.
    displayName: MariaDB Connection Password
    from: '[a-zA-Z0-9]{16}'
    generate: expression
    name: MYSQL_PASSWORD
    required: true
  - description: Password for the MariaDB root user.
    displayName: MariaDB root Password
    from: '[a-zA-Z0-9]{16}'
    generate: expression
    name: MYSQL_ROOT_PASSWORD
    required: true
  - description: Name of the MariaDB database accessed.
    displayName: MariaDB Database Name
    name: MYSQL_DATABASE
    required: true
    value: sampledb
  - description: Volume space available for data, e.g. 512Mi, 2Gi.
    displayName: Volume Capacity
    name: VOLUME_CAPACITY
    required: true
    value: 10Gi
- apiVersion: v1
  kind: Template
  labels:
    app: minio
    template: minio-persistent
  metadata:
    annotations:
      description: Create a minio node, with persistent storage.
      iconClass: icon-database
      openshift.io/display-name: Minio (Persistent)
      tags: database,minio
    creationTimestamp: 2017-11-28T10:42:31Z
    name: minio-persistent
    namespace: openshift
    resourceVersion: "27939928"
    selfLink: /oapi/v1/namespaces/openshift/templates/minio-persistent
    uid: d5eb114b-d428-11e7-9c36-364284a56e59
  objects:
  - apiVersion: v1
    kind: Service
    metadata:
      labels:
        name: ${NAME}
      name: minio-service
    spec:
      ports:
      - name: httpport
        port: 9000
        protocol: TCP
        targetPort: 9000
      selector:
        name: ${NAME}
      type: LoadBalancer
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      labels:
        name: ${NAME}
      name: ${NAME}
    spec:
      replicas: 1
      selector:
        name: ${NAME}
      strategy:
        type: Rolling
      template:
        metadata:
          labels:
            name: ${NAME}
        spec:
          containers:
          - args:
            - server
            - /data
            env:
            - name: MINIO_ACCESS_KEY
              value: ${ACCESSKEY}
            - name: MINIO_SECRET_KEY
              value: ${SECRETKEY}
            image: ${SOURCE_IMAGE}
            imagePullPolicy: Always
            livenessProbe:
              initialDelaySeconds: 60
              tcpSocket:
                port: 9000
              timeoutSeconds: 5
            name: ${NAME}
            ports:
            - containerPort: 9000
              protocol: TCP
            resources:
              limits:
                cpu: ${RESOURCE_CPU_LIMIT}
                memory: ${RESOURCE_MEMORY_LIMIT}G
              requests:
                cpu: 300m
                memory: 300Mi
            securityContext:
              privileged: false
              runAsUser: 0
            terminationMessagePath: /dev/termination-log
            volumeMounts:
            - mountPath: /data
              name: datadir
          terminationGracePeriodSeconds: 10
          volumes:
          - name: datadir
            persistentVolumeClaim:
              claimName: ${CLAIMNAME}
      triggers:
      - type: ConfigChange
  parameters:
  - description: Name.
    name: NAME
    required: true
    value: minio
  - description: Container image source.
    name: SOURCE_IMAGE
    required: true
    value: minio/minio:latest
  - description: CLAIMNAME To Share.
    name: CLAIMNAME
    required: true
  - description: The limits for memory resource.GB
    name: RESOURCE_MEMORY_LIMIT
    value: "1"
  - description: The limits for CPU resource.
    name: RESOURCE_CPU_LIMIT
    value: "1"
  - description: ACCESS_KEY To Share.
    name: ACCESSKEY
    required: true
    value: minio
  - description: SECRET_KEY To Share.
    name: SECRETKEY
    required: true
    value: minio123
- apiVersion: v1
  kind: Template
  labels:
    template: mongodb-ephemeral-template
  message: |-
    The following service(s) have been created in your project: ${DATABASE_SERVICE_NAME}.

           Username: ${MONGODB_USER}
           Password: ${MONGODB_PASSWORD}
      Database Name: ${MONGODB_DATABASE}
     Connection URL: mongodb://${MONGODB_USER}:${MONGODB_PASSWORD}@${DATABASE_SERVICE_NAME}/${MONGODB_DATABASE}

    For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/mongodb-container/blob/master/3.2/README.md.
  metadata:
    annotations:
      description: |-
        MongoDB database service, without persistent storage. For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/mongodb-container/blob/master/3.2/README.md.

        WARNING: Any data stored will be lost upon pod destruction. Only use this template for testing
      iconClass: icon-mongodb
      openshift.io/display-name: MongoDB (Ephemeral)
      tags: database,mongodb
      template.openshift.io/documentation-url: https://docs.openshift.org/latest/using_images/db_images/mongodb.html
      template.openshift.io/long-description: This template provides a standalone
        MongoDB server with a database created.  The database is not stored on persistent
        storage, so any restart of the service will result in all data being lost.  The
        database name, username, and password are chosen via parameters when provisioning
        this service.
      template.openshift.io/provider-display-name: Red Hat, Inc.
      template.openshift.io/support-url: https://access.redhat.com
    creationTimestamp: 2017-08-23T07:04:05Z
    name: mongodb-ephemeral
    namespace: openshift
    resourceVersion: "846"
    selfLink: /oapi/v1/namespaces/openshift/templates/mongodb-ephemeral
    uid: 409b5299-87d1-11e7-9a67-ecf4bbe2f144
  objects:
  - apiVersion: v1
    kind: Secret
    metadata:
      annotations:
        template.openshift.io/expose-admin_password: '{.data[''database-admin-password'']}'
        template.openshift.io/expose-password: '{.data[''database-password'']}'
        template.openshift.io/expose-username: '{.data[''database-user'']}'
      name: ${DATABASE_SERVICE_NAME}
    stringData:
      database-admin-password: ${MONGODB_ADMIN_PASSWORD}
      database-password: ${MONGODB_PASSWORD}
      database-user: ${MONGODB_USER}
  - apiVersion: v1
    kind: Service
    metadata:
      annotations:
        template.openshift.io/expose-uri: mongodb://{.spec.clusterIP}:{.spec.ports[?(.name=="mongo")].port}
      creationTimestamp: null
      name: ${DATABASE_SERVICE_NAME}
    spec:
      ports:
      - name: mongo
        nodePort: 0
        port: 27017
        protocol: TCP
        targetPort: 27017
      selector:
        name: ${DATABASE_SERVICE_NAME}
      sessionAffinity: None
      type: ClusterIP
    status:
      loadBalancer: {}
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      creationTimestamp: null
      name: ${DATABASE_SERVICE_NAME}
    spec:
      replicas: 1
      selector:
        name: ${DATABASE_SERVICE_NAME}
      strategy:
        type: Recreate
      template:
        metadata:
          creationTimestamp: null
          labels:
            name: ${DATABASE_SERVICE_NAME}
        spec:
          containers:
          - capabilities: {}
            env:
            - name: MONGODB_USER
              valueFrom:
                secretKeyRef:
                  key: database-user
                  name: ${DATABASE_SERVICE_NAME}
            - name: MONGODB_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: database-password
                  name: ${DATABASE_SERVICE_NAME}
            - name: MONGODB_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: database-admin-password
                  name: ${DATABASE_SERVICE_NAME}
            - name: MONGODB_DATABASE
              value: ${MONGODB_DATABASE}
            image: ' '
            imagePullPolicy: IfNotPresent
            livenessProbe:
              initialDelaySeconds: 30
              tcpSocket:
                port: 27017
              timeoutSeconds: 1
            name: mongodb
            ports:
            - containerPort: 27017
              protocol: TCP
            readinessProbe:
              exec:
                command:
                - /bin/sh
                - -i
                - -c
                - mongo 127.0.0.1:27017/$MONGODB_DATABASE -u $MONGODB_USER -p $MONGODB_PASSWORD
                  --eval="quit()"
              initialDelaySeconds: 3
              timeoutSeconds: 1
            resources:
              limits:
                memory: ${MEMORY_LIMIT}
            securityContext:
              capabilities: {}
              privileged: false
            terminationMessagePath: /dev/termination-log
            volumeMounts:
            - mountPath: /var/lib/mongodb/data
              name: ${DATABASE_SERVICE_NAME}-data
          dnsPolicy: ClusterFirst
          restartPolicy: Always
          volumes:
          - emptyDir:
              medium: ""
            name: ${DATABASE_SERVICE_NAME}-data
      triggers:
      - imageChangeParams:
          automatic: true
          containerNames:
          - mongodb
          from:
            kind: ImageStreamTag
            name: mongodb:${MONGODB_VERSION}
            namespace: ${NAMESPACE}
          lastTriggeredImage: ""
        type: ImageChange
      - type: ConfigChange
    status: {}
  parameters:
  - description: Maximum amount of memory the container can use.
    displayName: Memory Limit
    name: MEMORY_LIMIT
    required: true
    value: 512Mi
  - description: The OpenShift Namespace where the ImageStream resides.
    displayName: Namespace
    name: NAMESPACE
    value: openshift
  - description: The name of the OpenShift Service exposed for the database.
    displayName: Database Service Name
    name: DATABASE_SERVICE_NAME
    required: true
    value: mongodb
  - description: Username for MongoDB user that will be used for accessing the database.
    displayName: MongoDB Connection Username
    from: user[A-Z0-9]{3}
    generate: expression
    name: MONGODB_USER
    required: true
  - description: Password for the MongoDB connection user.
    displayName: MongoDB Connection Password
    from: '[a-zA-Z0-9]{16}'
    generate: expression
    name: MONGODB_PASSWORD
    required: true
  - description: Name of the MongoDB database accessed.
    displayName: MongoDB Database Name
    name: MONGODB_DATABASE
    required: true
    value: sampledb
  - description: Password for the database admin user.
    displayName: MongoDB Admin Password
    from: '[a-zA-Z0-9]{16}'
    generate: expression
    name: MONGODB_ADMIN_PASSWORD
    required: true
  - description: Version of MongoDB image to be used (2.4, 2.6, 3.2 or latest).
    displayName: Version of MongoDB Image
    name: MONGODB_VERSION
    required: true
    value: "3.2"
- apiVersion: v1
  kind: Template
  labels:
    template: mongodb-persistent-template
  message: |-
    The following service(s) have been created in your project: ${DATABASE_SERVICE_NAME}.

           Username: ${MONGODB_USER}
           Password: ${MONGODB_PASSWORD}
      Database Name: ${MONGODB_DATABASE}
     Connection URL: mongodb://${MONGODB_USER}:${MONGODB_PASSWORD}@${DATABASE_SERVICE_NAME}/${MONGODB_DATABASE}

    For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/mongodb-container/blob/master/3.2/README.md.
  metadata:
    annotations:
      description: |-
        MongoDB database service, with persistent storage. For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/mongodb-container/blob/master/3.2/README.md.

        NOTE: Scaling to more than one replica is not supported. You must have persistent volumes available in your cluster to use this template.
      iconClass: icon-mongodb
      openshift.io/display-name: MongoDB (Persistent)
      tags: database,mongodb
      template.openshift.io/documentation-url: https://docs.openshift.org/latest/using_images/db_images/mongodb.html
      template.openshift.io/long-description: This template provides a standalone
        MongoDB server with a database created.  The database is stored on persistent
        storage.  The database name, username, and password are chosen via parameters
        when provisioning this service.
      template.openshift.io/provider-display-name: Red Hat, Inc.
      template.openshift.io/support-url: https://access.redhat.com
    creationTimestamp: 2017-08-23T07:04:06Z
    name: mongodb-persistent
    namespace: openshift
    resourceVersion: "847"
    selfLink: /oapi/v1/namespaces/openshift/templates/mongodb-persistent
    uid: 409ccb52-87d1-11e7-9a67-ecf4bbe2f144
  objects:
  - apiVersion: v1
    kind: Secret
    metadata:
      annotations:
        template.openshift.io/expose-admin_password: '{.data[''database-admin-password'']}'
        template.openshift.io/expose-password: '{.data[''database-password'']}'
        template.openshift.io/expose-username: '{.data[''database-user'']}'
      name: ${DATABASE_SERVICE_NAME}
    stringData:
      database-admin-password: ${MONGODB_ADMIN_PASSWORD}
      database-password: ${MONGODB_PASSWORD}
      database-user: ${MONGODB_USER}
  - apiVersion: v1
    kind: Service
    metadata:
      annotations:
        template.openshift.io/expose-uri: mongodb://{.spec.clusterIP}:{.spec.ports[?(.name=="mongo")].port}
      creationTimestamp: null
      name: ${DATABASE_SERVICE_NAME}
    spec:
      ports:
      - name: mongo
        nodePort: 0
        port: 27017
        protocol: TCP
        targetPort: 27017
      selector:
        name: ${DATABASE_SERVICE_NAME}
      sessionAffinity: None
      type: ClusterIP
    status:
      loadBalancer: {}
  - apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: ${DATABASE_SERVICE_NAME}
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: ${VOLUME_CAPACITY}
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      creationTimestamp: null
      name: ${DATABASE_SERVICE_NAME}
    spec:
      replicas: 1
      selector:
        name: ${DATABASE_SERVICE_NAME}
      strategy:
        type: Recreate
      template:
        metadata:
          creationTimestamp: null
          labels:
            name: ${DATABASE_SERVICE_NAME}
        spec:
          containers:
          - capabilities: {}
            env:
            - name: MONGODB_USER
              valueFrom:
                secretKeyRef:
                  key: database-user
                  name: ${DATABASE_SERVICE_NAME}
            - name: MONGODB_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: database-password
                  name: ${DATABASE_SERVICE_NAME}
            - name: MONGODB_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: database-admin-password
                  name: ${DATABASE_SERVICE_NAME}
            - name: MONGODB_DATABASE
              value: ${MONGODB_DATABASE}
            image: ' '
            imagePullPolicy: IfNotPresent
            livenessProbe:
              initialDelaySeconds: 30
              tcpSocket:
                port: 27017
              timeoutSeconds: 1
            name: mongodb
            ports:
            - containerPort: 27017
              protocol: TCP
            readinessProbe:
              exec:
                command:
                - /bin/sh
                - -i
                - -c
                - mongo 127.0.0.1:27017/$MONGODB_DATABASE -u $MONGODB_USER -p $MONGODB_PASSWORD
                  --eval="quit()"
              initialDelaySeconds: 3
              timeoutSeconds: 1
            resources:
              limits:
                memory: ${MEMORY_LIMIT}
            securityContext:
              capabilities: {}
              privileged: false
            terminationMessagePath: /dev/termination-log
            volumeMounts:
            - mountPath: /var/lib/mongodb/data
              name: ${DATABASE_SERVICE_NAME}-data
          dnsPolicy: ClusterFirst
          restartPolicy: Always
          volumes:
          - name: ${DATABASE_SERVICE_NAME}-data
            persistentVolumeClaim:
              claimName: ${DATABASE_SERVICE_NAME}
      triggers:
      - imageChangeParams:
          automatic: true
          containerNames:
          - mongodb
          from:
            kind: ImageStreamTag
            name: mongodb:${MONGODB_VERSION}
            namespace: ${NAMESPACE}
          lastTriggeredImage: ""
        type: ImageChange
      - type: ConfigChange
    status: {}
  parameters:
  - description: Maximum amount of memory the container can use.
    displayName: Memory Limit
    name: MEMORY_LIMIT
    required: true
    value: 512Mi
  - description: The OpenShift Namespace where the ImageStream resides.
    displayName: Namespace
    name: NAMESPACE
    value: openshift
  - description: The name of the OpenShift Service exposed for the database.
    displayName: Database Service Name
    name: DATABASE_SERVICE_NAME
    required: true
    value: mongodb
  - description: Username for MongoDB user that will be used for accessing the database.
    displayName: MongoDB Connection Username
    from: user[A-Z0-9]{3}
    generate: expression
    name: MONGODB_USER
    required: true
  - description: Password for the MongoDB connection user.
    displayName: MongoDB Connection Password
    from: '[a-zA-Z0-9]{16}'
    generate: expression
    name: MONGODB_PASSWORD
    required: true
  - description: Name of the MongoDB database accessed.
    displayName: MongoDB Database Name
    name: MONGODB_DATABASE
    required: true
    value: sampledb
  - description: Password for the database admin user.
    displayName: MongoDB Admin Password
    from: '[a-zA-Z0-9]{16}'
    generate: expression
    name: MONGODB_ADMIN_PASSWORD
    required: true
  - description: Volume space available for data, e.g. 512Mi, 2Gi.
    displayName: Volume Capacity
    name: VOLUME_CAPACITY
    required: true
    value: 1Gi
  - description: Version of MongoDB image to be used (2.4, 2.6, 3.2 or latest).
    displayName: Version of MongoDB Image
    name: MONGODB_VERSION
    required: true
    value: "3.2"
- apiVersion: v1
  kind: Template
  labels:
    component: mysql_single
    template: mysql-single-persistent
  message: "The following service(s) have been created in your project:\n\n\n       Username:
    ${MYSQL_USER}\n       Password: ${MYSQL_PASSWORD}\n  Database Name: ${MYSQL_DATABASE}\n
    \n\nFor more information about using this template, including OpenShift considerations,
    see https://github.com/sclorg/mariadb-container/blob/master/10.1/README.md."
  metadata:
    annotations:
      description: |-
        MariaDB database service, with persistent storage. For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/mariadb-container/blob/master/10.1/README.md.

        NOTE: Scaling to more than one replica is not supported. You must have persistent volumes available in your cluster to use this template.
      iconClass: icon-mariadb
      openshift.io/display-name: MariaDB-Single (Persistent)
      tags: database,mariadb
      template.openshift.io/documentation-url: https://github.com/sclorg/mariadb-container/blob/master/10.1/README.md
      template.openshift.io/long-description: This template provides a standalone
        MariaDB server with a database created.  The database is stored on persistent
        storage.  The database name, username, and password are chosen via parameters
        when provisioning this service.
    creationTimestamp: 2017-10-17T02:14:28Z
    name: mysql-single-persistent
    namespace: openshift
    resourceVersion: "27449352"
    selfLink: /oapi/v1/namespaces/openshift/templates/mysql-single-persistent
    uid: e7c135b7-b2e0-11e7-9b95-ecf4bbe2f144
  objects:
  - apiVersion: v1
    data:
      mariadb.cnf: "# MariaDB-specific config file.\n# Read by /etc/mysql/my.cnf\n\n[client]\n#
        Default is Latin1, if you need UTF-8 set this (also in server section)\ndefault-character-set
        = utf8 \n\n[mysqld]\n#\n# * Character sets\n# \n# Default is Latin1, if you
        need UTF-8 set all this (also in client section)\n#\ncharacter-set-server
        \ = utf8 \ncollation-server      = utf8_general_ci \n#character_set_server
        \  = utf8 \n#collation_server       = utf8_general_ci \n# Import all .cnf
        files from configuration directory\n!includedir /etc/mysql/mariadb.conf.d/"
      my.cnf: "# MariaDB database server configuration file.\n#\n# You can copy this
        file to one of:\n# - \"/etc/mysql/my.cnf\" to set global options,\n# - \"~/.my.cnf\"
        to set user-specific options.\n# \n# One can use all long options that the
        program supports.\n# Run program with --help to get a list of available options
        and with\n# --print-defaults to see which it would actually understand and
        use.\n#\n# For explanations see\n# http://dev.mysql.com/doc/mysql/en/server-system-variables.html\n\n#
        This will be passed to all mysql clients\n# It has been reported that passwords
        should be enclosed with ticks/quotes\n# escpecially if they contain \"#\"
        chars...\n# Remember to edit /etc/mysql/debian.cnf when changing the socket
        location.\n[client]\nport            = 3306\nsocket          = /var/run/mysqld/mysqld.sock\n\n#
        Here is entries for some specific programs\n# The following values assume
        you have at least 32M ram\n\n# This was formally known as [safe_mysqld]. Both
        versions are currently parsed.\n[mysqld_safe]\nsocket          = /var/run/mysqld/mysqld.sock\nnice
        \           = 0\n\n[mysqld]\n#\n# * Basic Settings\n#\n#user           = mysql\npid-file
        \       = /var/run/mysqld/mysqld.pid\nsocket          = /var/run/mysqld/mysqld.sock\nport
        \           = 3306\nbasedir         = /usr\ndatadir         = /var/lib/mysql\ntmpdir
        \         = /tmp\nlc_messages_dir = /usr/share/mysql\nlc_messages     = en_US\nskip-external-locking\n#\n#
        Instead of skip-networking the default is now to listen only on\n# localhost
        which is more compatible and is not less secure.\n#bind-address           =
        127.0.0.1\n#\n# * Fine Tuning\n#\nmax_connections         = 3000\nconnect_timeout
        \        = 5\nwait_timeout            = 600\nmax_allowed_packet      = 32M\nthread_cache_size
        \      = 128\nsort_buffer_size        = 4M\nbulk_insert_buffer_size = 16M\ntmp_table_size
        \         = 32M\nmax_heap_table_size     = 32M\n#\n# * MyISAM\n#\n# This replaces
        the startup script and checks MyISAM tables if needed\n# the first time they
        are touched. On error, make copy and try a repair.\nmyisam_recover_options
        = BACKUP\nkey_buffer_size         = 128M\n#open-files-limit       = 2000\ntable_open_cache
        \       = 400\nmyisam_sort_buffer_size = 512M\nconcurrent_insert       = 2\nread_buffer_size
        \       = 2M\nread_rnd_buffer_size    = 1M\n#\n# * Query Cache Configuration\n#\n#
        Cache only tiny result sets, so we can fit more in the query cache.\nquery_cache_limit
        \              = 1M\nquery_cache_size                = 64M\n# for more write
        intensive setups, set to DEMAND or OFF\n#query_cache_type               =
        DEMAND\n#\n# * Logging and Replication\n#\n# Both location gets rotated by
        the cronjob.\n# Be aware that this log type is a performance killer.\n# As
        of 5.1 you can enable the log at runtime!\n#general_log_file        = /var/log/mysql/mysql.log\n#general_log
        \            = 1\n#\n# Error logging goes to syslog due to /etc/mysql/conf.d/mysqld_safe_syslog.cnf.\n#\n#
        we do want to know about network errors and such\n#log_warnings           =
        2\n#\n# Enable the slow query log to see queries with especially long duration\n#slow_query_log[={0|1}]\nslow_query_log_file
        \    = /var/log/mysql/mariadb-slow.log\nlong_query_time = 10\n#log_slow_rate_limit
        \   = 1000\n#log_slow_verbosity     = query_plan\n\n#log-queries-not-using-indexes\n#log_slow_admin_statements\n#\n#
        The following can be used as easy to replay backup logs or for replication.\n#
        note: if you are setting up a replication slave, see README.Debian about\n#
        \      other settings you may need to change.\n#server-id              = 1\n#report_host
        \           = master1\n#auto_increment_increment = 2\n#auto_increment_offset
        \ = 1\n#log_bin                        = /var/log/mysql/mariadb-bin\n#log_bin_index
        \         = /var/log/mysql/mariadb-bin.index\n# not fab for performance, but
        safer\n#sync_binlog            = 1\nexpire_logs_days        = 10\nmax_binlog_size
        \        = 100M\n# slaves\n#relay_log              = /var/log/mysql/relay-bin\n#relay_log_index
        \       = /var/log/mysql/relay-bin.index\n#relay_log_info_file    = /var/log/mysql/relay-bin.info\n#log_slave_updates\n#read_only\n#\n#
        If applications support it, this stricter sql_mode prevents some\n# mistakes
        like inserting invalid dates etc.\n#sql_mode               = NO_ENGINE_SUBSTITUTION,TRADITIONAL\n#\n#
        * InnoDB\n#\n# InnoDB is enabled by default with a 10MB datafile in /var/lib/mysql/.\n#
        Read the manual for more InnoDB related options. There are many!\ndefault_storage_engine
        \ = InnoDB\n# you can't just change log file size, requires special procedure\ninnodb_log_file_size
        \  = 256M\ninnodb_buffer_pool_size = 256M\ninnodb_log_buffer_size  = 8M\ninnodb_file_per_table
        \  = 1\ninnodb_open_files       = 400\ninnodb_io_capacity      = 400\ninnodb_flush_method
        \    = O_DIRECT\n#\n# * Security Features\n#\n# Read the manual, too, if you
        want chroot!\n# chroot = /var/lib/mysql/\n#\n# For generating SSL certificates
        I recommend the OpenSSL GUI \"tinyca\".\n#\n# ssl-ca=/etc/mysql/cacert.pem\n#
        ssl-cert=/etc/mysql/server-cert.pem\n# ssl-key=/etc/mysql/server-key.pem\n\n#\n#
        * Galera-related settings\n#\n[galera]\n# Mandatory settings\n#wsrep_on=ON\n#wsrep_provider=\n#wsrep_cluster_address=\n#binlog_format=row\n#default_storage_engine=InnoDB\n#innodb_autoinc_lock_mode=2\n#\n#
        Allow server to accept connections on all interfaces.\n#\n#bind-address=0.0.0.0\n#\n#
        Optional setting\n#wsrep_slave_threads=1\n#innodb_flush_log_at_trx_commit=0\n\n[mysqldump]\nquick\nquote-names\nmax_allowed_packet
        \     = 32M\n\n[mysql]\n#no-auto-rehash # faster start of mysql but no tab
        completion\n\n[isamchk]\nkey_buffer              = 16M\n\n#\n# * IMPORTANT:
        Additional settings that can override those from this file!\n#   The files
        must end with '.cnf', otherwise they'll be ignored.\n#\n!includedir /etc/mysql/conf.d/"
    kind: ConfigMap
    metadata:
      name: mysql-config
  - apiVersion: v1
    kind: Secret
    metadata:
      annotations:
        template.openshift.io/expose-password: '{.data[''database-password'']}'
        template.openshift.io/expose-root_password: '{.data[''database-root-password'']}'
        template.openshift.io/expose-username: '{.data[''database-user'']}'
      labels:
        app: ${NAME}
      name: ${NAME}
    stringData:
      database-password: ${MYSQL_PASSWORD}
      database-root-password: ${MYSQL_ROOT_PASSWORD}
      database-user: ${MYSQL_USER}
  - apiVersion: v1
    kind: Service
    metadata:
      labels:
        app: ${NAME}
      name: mysql
    spec:
      clusterIP: None
      ports:
      - name: mariadb
        port: 3306
      selector:
        app: ${NAME}
  - apiVersion: v1
    kind: Service
    metadata:
      annotations:
        l4Proxy: "Yes"
      labels:
        app: ${NAME}
      name: mysql-out
    spec:
      ports:
      - name: mariadb
        port: 3306
      selector:
        app: ${NAME}
      type: NodePort
  - apiVersion: apps/v1beta1
    kind: StatefulSet
    metadata:
      labels:
        app: ${NAME}
      name: ${NAME}
    spec:
      replicas: "1"
      serviceName: mysql
      template:
        metadata:
          labels:
            app: ${NAME}
            component: mysql_single
            template: mysql_single-persistent
        spec:
          containers:
          - env:
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: MYSQL_USER
              valueFrom:
                secretKeyRef:
                  key: database-user
                  name: ${NAME}
            - name: MYSQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: database-password
                  name: ${NAME}
            - name: MYSQL_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: database-root-password
                  name: ${NAME}
            - name: MYSQL_DATABASE
              value: ${MYSQL_DATABASE}
            image: ${SOURCE_IMAGE}
            imagePullPolicy: Always
            livenessProbe:
              initialDelaySeconds: 30
              tcpSocket:
                port: 3306
              timeoutSeconds: 1
            name: ${NAME}
            ports:
            - containerPort: 3306
            readinessProbe:
              exec:
                command:
                - /bin/sh
                - -i
                - -c
                - MYSQL_PWD="$MYSQL_PASSWORD" mysql -h 127.0.0.1 -u $MYSQL_USER -D
                  $MYSQL_DATABASE -e 'SELECT 1'
              initialDelaySeconds: 10
              timeoutSeconds: 1
            resources:
              limits:
                cpu: ${RESOURCE_CPU_LIMIT}
                memory: ${RESOURCE_MEMORY_LIMIT}G
              requests:
                cpu: 300m
                memory: 300M
            securityContext:
              privileged: true
            volumeMounts:
            - mountPath: /var/lib/mysql/data
              name: storage
            - mountPath: /etc/mysql/my.cnf
              name: my-cnf-volume
              subPath: my.cnf
            - mountPath: /etc/mysql/conf.d/mariadb.cnf
              name: mariadb-cnf-volume
              subPath: mariadb.cnf
          terminationGracePeriodSeconds: 10
          volumes:
          - configMap:
              defaultMode: 420
              items:
              - key: my.cnf
                path: my.cnf
              name: mysql-config
            name: my-cnf-volume
          - configMap:
              defaultMode: 420
              items:
              - key: mariadb.cnf
                path: mariadb.cnf
              name: mysql-config
            name: mariadb-cnf-volume
      volumeClaimTemplates:
      - metadata:
          annotations:
            volume.alpha.kubernetes.io/storage-class: slow
          name: storage
        spec:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: ${VOLUME_CAPACITY}Gi
  parameters:
  - description: Name.
    name: NAME
    required: true
    value: mysql-single
  - description: Container image source.
    name: SOURCE_IMAGE
    required: true
    value: mariadb:10.3.1
  - description: mysql Data capacity.
    name: VOLUME_CAPACITY
    required: true
    value: "20"
  - description: The limits for memory resource.
    name: RESOURCE_MEMORY_LIMIT
    value: "4"
  - description: The limits for CPU resource.
    name: RESOURCE_CPU_LIMIT
    value: "2"
  - description: Name of the MariaDB database accessed.
    displayName: MariaDB Database Name
    name: MYSQL_DATABASE
    required: true
    value: sampledb
  - description: Username for MariaDB user that will be used for accessing the database.
    displayName: MariaDB Connection Username
    from: user[A-Z0-9]{3}
    generate: expression
    name: MYSQL_USER
    required: true
  - description: Password for the MariaDB connection user.
    displayName: MariaDB Connection Password
    from: '[a-zA-Z0-9]{16}'
    generate: expression
    name: MYSQL_PASSWORD
    required: true
  - description: Password for the MariaDB root user.
    displayName: MariaDB root Password
    from: '[a-zA-Z0-9]{16}'
    generate: expression
    name: MYSQL_ROOT_PASSWORD
    required: true
- apiVersion: v1
  kind: Template
  labels:
    template: nodejs-mongo-persistent
  message: |-
    The following service(s) have been created in your project: ${NAME}, ${DATABASE_SERVICE_NAME}.

    For more information about using this template, including OpenShift considerations, see https://github.com/openshift/nodejs-ex/blob/master/README.md.
  metadata:
    annotations:
      description: An example Node.js application with a MongoDB database. For more
        information about using this template, including OpenShift considerations,
        see https://github.com/openshift/nodejs-ex/blob/master/README.md.
      iconClass: icon-nodejs
      openshift.io/display-name: Node.js + MongoDB (Persistent)
      tags: quickstart,nodejs
      template.openshift.io/documentation-url: https://github.com/openshift/nodejs-ex
      template.openshift.io/long-description: This template defines resources needed
        to develop a NodeJS application, including a build configuration, application
        deployment configuration, and database deployment configuration.
      template.openshift.io/provider-display-name: Red Hat, Inc.
      template.openshift.io/support-url: https://access.redhat.com
    creationTimestamp: 2017-08-23T07:04:10Z
    name: nodejs-mongo-persistent
    namespace: openshift
    resourceVersion: "869"
    selfLink: /oapi/v1/namespaces/openshift/templates/nodejs-mongo-persistent
    uid: 4362fd85-87d1-11e7-9a67-ecf4bbe2f144
  objects:
  - apiVersion: v1
    kind: Secret
    metadata:
      name: ${NAME}
    stringData:
      database-admin-password: ${DATABASE_ADMIN_PASSWORD}
      database-password: ${DATABASE_PASSWORD}
      database-user: ${DATABASE_USER}
  - apiVersion: v1
    kind: Service
    metadata:
      annotations:
        description: Exposes and load balances the application pods
        service.alpha.openshift.io/dependencies: '[{"name": "${DATABASE_SERVICE_NAME}",
          "kind": "Service"}]'
      name: ${NAME}
    spec:
      ports:
      - name: web
        port: 8080
        targetPort: 8080
      selector:
        name: ${NAME}
  - apiVersion: v1
    kind: Route
    metadata:
      annotations:
        template.openshift.io/expose-uri: http://{.spec.host}{.spec.path}
      name: ${NAME}
    spec:
      host: ${APPLICATION_DOMAIN}
      to:
        kind: Service
        name: ${NAME}
  - apiVersion: v1
    kind: ImageStream
    metadata:
      annotations:
        description: Keeps track of changes in the application image
      name: ${NAME}
  - apiVersion: v1
    kind: BuildConfig
    metadata:
      annotations:
        description: Defines how to build the application
      name: ${NAME}
    spec:
      output:
        to:
          kind: ImageStreamTag
          name: ${NAME}:latest
      postCommit:
        script: npm test
      source:
        contextDir: ${CONTEXT_DIR}
        git:
          ref: ${SOURCE_REPOSITORY_REF}
          uri: ${SOURCE_REPOSITORY_URL}
        type: Git
      strategy:
        sourceStrategy:
          env:
          - name: NPM_MIRROR
            value: ${NPM_MIRROR}
          from:
            kind: ImageStreamTag
            name: nodejs:6
            namespace: ${NAMESPACE}
        type: Source
      triggers:
      - type: ImageChange
      - type: ConfigChange
      - github:
          secret: ${GITHUB_WEBHOOK_SECRET}
        type: GitHub
      - generic:
          secret: ${GENERIC_WEBHOOK_SECRET}
        type: Generic
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      annotations:
        description: Defines how to deploy the application server
      name: ${NAME}
    spec:
      replicas: 1
      selector:
        name: ${NAME}
      strategy:
        type: Recreate
      template:
        metadata:
          labels:
            name: ${NAME}
          name: ${NAME}
        spec:
          containers:
          - env:
            - name: DATABASE_SERVICE_NAME
              value: ${DATABASE_SERVICE_NAME}
            - name: MONGODB_USER
              valueFrom:
                secretKeyRef:
                  key: database-user
                  name: ${NAME}
            - name: MONGODB_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: database-password
                  name: ${NAME}
            - name: MONGODB_DATABASE
              value: ${DATABASE_NAME}
            - name: MONGODB_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: database-admin-password
                  name: ${NAME}
            image: ' '
            livenessProbe:
              httpGet:
                path: /pagecount
                port: 8080
              initialDelaySeconds: 30
              timeoutSeconds: 3
            name: nodejs-mongo-persistent
            ports:
            - containerPort: 8080
            readinessProbe:
              httpGet:
                path: /pagecount
                port: 8080
              initialDelaySeconds: 3
              timeoutSeconds: 3
            resources:
              limits:
                memory: ${MEMORY_LIMIT}
      triggers:
      - imageChangeParams:
          automatic: true
          containerNames:
          - nodejs-mongo-persistent
          from:
            kind: ImageStreamTag
            name: ${NAME}:latest
        type: ImageChange
      - type: ConfigChange
  - apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: ${DATABASE_SERVICE_NAME}
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: ${VOLUME_CAPACITY}
  - apiVersion: v1
    kind: Service
    metadata:
      annotations:
        description: Exposes the database server
      name: ${DATABASE_SERVICE_NAME}
    spec:
      ports:
      - name: mongodb
        port: 27017
        targetPort: 27017
      selector:
        name: ${DATABASE_SERVICE_NAME}
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      annotations:
        description: Defines how to deploy the database
      name: ${DATABASE_SERVICE_NAME}
    spec:
      replicas: 1
      selector:
        name: ${DATABASE_SERVICE_NAME}
      strategy:
        type: Recreate
      template:
        metadata:
          labels:
            name: ${DATABASE_SERVICE_NAME}
          name: ${DATABASE_SERVICE_NAME}
        spec:
          containers:
          - env:
            - name: MONGODB_USER
              valueFrom:
                secretKeyRef:
                  key: database-user
                  name: ${NAME}
            - name: MONGODB_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: database-password
                  name: ${NAME}
            - name: MONGODB_DATABASE
              value: ${DATABASE_NAME}
            - name: MONGODB_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: database-admin-password
                  name: ${NAME}
            image: ' '
            livenessProbe:
              initialDelaySeconds: 30
              tcpSocket:
                port: 27017
              timeoutSeconds: 1
            name: mongodb
            ports:
            - containerPort: 27017
            readinessProbe:
              exec:
                command:
                - /bin/sh
                - -i
                - -c
                - mongo 127.0.0.1:27017/$MONGODB_DATABASE -u $MONGODB_USER -p $MONGODB_PASSWORD
                  --eval="quit()"
              initialDelaySeconds: 3
              timeoutSeconds: 1
            resources:
              limits:
                memory: ${MEMORY_MONGODB_LIMIT}
            volumeMounts:
            - mountPath: /var/lib/mongodb/data
              name: ${DATABASE_SERVICE_NAME}-data
          volumes:
          - name: ${DATABASE_SERVICE_NAME}-data
            persistentVolumeClaim:
              claimName: ${DATABASE_SERVICE_NAME}
      triggers:
      - imageChangeParams:
          automatic: true
          containerNames:
          - mongodb
          from:
            kind: ImageStreamTag
            name: mongodb:3.2
            namespace: ${NAMESPACE}
        type: ImageChange
      - type: ConfigChange
  parameters:
  - description: The name assigned to all of the frontend objects defined in this
      template.
    displayName: Name
    name: NAME
    required: true
    value: nodejs-mongo-persistent
  - description: The OpenShift Namespace where the ImageStream resides.
    displayName: Namespace
    name: NAMESPACE
    required: true
    value: openshift
  - description: Maximum amount of memory the Node.js container can use.
    displayName: Memory Limit
    name: MEMORY_LIMIT
    required: true
    value: 512Mi
  - description: Maximum amount of memory the MongoDB container can use.
    displayName: Memory Limit (MongoDB)
    name: MEMORY_MONGODB_LIMIT
    required: true
    value: 512Mi
  - description: Volume space available for data, e.g. 512Mi, 2Gi
    displayName: Volume Capacity
    name: VOLUME_CAPACITY
    required: true
    value: 1Gi
  - description: The URL of the repository with your application source code.
    displayName: Git Repository URL
    name: SOURCE_REPOSITORY_URL
    required: true
    value: https://github.com/openshift/nodejs-ex.git
  - description: Set this to a branch name, tag or other ref of your repository if
      you are not using the default branch.
    displayName: Git Reference
    name: SOURCE_REPOSITORY_REF
  - description: Set this to the relative path to your project if it is not in the
      root of your repository.
    displayName: Context Directory
    name: CONTEXT_DIR
  - description: The exposed hostname that will route to the Node.js service, if left
      blank a value will be defaulted.
    displayName: Application Hostname
    name: APPLICATION_DOMAIN
  - description: Github trigger secret.  A difficult to guess string encoded as part
      of the webhook URL.  Not encrypted.
    displayName: GitHub Webhook Secret
    from: '[a-zA-Z0-9]{40}'
    generate: expression
    name: GITHUB_WEBHOOK_SECRET
  - description: A secret string used to configure the Generic webhook.
    displayName: Generic Webhook Secret
    from: '[a-zA-Z0-9]{40}'
    generate: expression
    name: GENERIC_WEBHOOK_SECRET
  - displayName: Database Service Name
    name: DATABASE_SERVICE_NAME
    required: true
    value: mongodb
  - description: Username for MongoDB user that will be used for accessing the database.
    displayName: MongoDB Username
    from: user[A-Z0-9]{3}
    generate: expression
    name: DATABASE_USER
  - description: Password for the MongoDB user.
    displayName: MongoDB Password
    from: '[a-zA-Z0-9]{16}'
    generate: expression
    name: DATABASE_PASSWORD
  - displayName: Database Name
    name: DATABASE_NAME
    required: true
    value: sampledb
  - description: Password for the database admin user.
    displayName: Database Administrator Password
    from: '[a-zA-Z0-9]{16}'
    generate: expression
    name: DATABASE_ADMIN_PASSWORD
  - description: The custom NPM mirror URL
    displayName: Custom NPM Mirror URL
    name: NPM_MIRROR
- apiVersion: v1
  kind: Template
  labels:
    template: nodejs-mongodb-example
  message: |-
    The following service(s) have been created in your project: ${NAME}, ${DATABASE_SERVICE_NAME}.

    For more information about using this template, including OpenShift considerations, see https://github.com/openshift/nodejs-ex/blob/master/README.md.
  metadata:
    annotations:
      description: |-
        An example Node.js application with a MongoDB database. For more information about using this template, including OpenShift considerations, see https://github.com/openshift/nodejs-ex/blob/master/README.md.

        WARNING: Any data stored will be lost upon pod destruction. Only use this template for testing.
      iconClass: icon-nodejs
      openshift.io/display-name: Node.js + MongoDB (Ephemeral)
      tags: quickstart,nodejs
      template.openshift.io/documentation-url: https://github.com/openshift/nodejs-ex
      template.openshift.io/long-description: This template defines resources needed
        to develop a NodeJS application, including a build configuration, application
        deployment configuration, and database deployment configuration.  The database
        is stored in non-persistent storage, so this configuration should be used
        for experimental purposes only.
      template.openshift.io/provider-display-name: Red Hat, Inc.
      template.openshift.io/support-url: https://access.redhat.com
    creationTimestamp: 2017-08-23T07:04:10Z
    name: nodejs-mongodb-example
    namespace: openshift
    resourceVersion: "870"
    selfLink: /oapi/v1/namespaces/openshift/templates/nodejs-mongodb-example
    uid: 43643d50-87d1-11e7-9a67-ecf4bbe2f144
  objects:
  - apiVersion: v1
    kind: Secret
    metadata:
      name: ${NAME}
    stringData:
      database-admin-password: ${DATABASE_ADMIN_PASSWORD}
      database-password: ${DATABASE_PASSWORD}
      database-user: ${DATABASE_USER}
  - apiVersion: v1
    kind: Service
    metadata:
      annotations:
        description: Exposes and load balances the application pods
        service.alpha.openshift.io/dependencies: '[{"name": "${DATABASE_SERVICE_NAME}",
          "kind": "Service"}]'
      name: ${NAME}
    spec:
      ports:
      - name: web
        port: 8080
        targetPort: 8080
      selector:
        name: ${NAME}
  - apiVersion: v1
    kind: Route
    metadata:
      annotations:
        template.openshift.io/expose-uri: http://{.spec.host}{.spec.path}
      name: ${NAME}
    spec:
      host: ${APPLICATION_DOMAIN}
      to:
        kind: Service
        name: ${NAME}
  - apiVersion: v1
    kind: ImageStream
    metadata:
      annotations:
        description: Keeps track of changes in the application image
      name: ${NAME}
  - apiVersion: v1
    kind: BuildConfig
    metadata:
      annotations:
        description: Defines how to build the application
      name: ${NAME}
    spec:
      output:
        to:
          kind: ImageStreamTag
          name: ${NAME}:latest
      postCommit:
        script: npm test
      source:
        contextDir: ${CONTEXT_DIR}
        git:
          ref: ${SOURCE_REPOSITORY_REF}
          uri: ${SOURCE_REPOSITORY_URL}
        type: Git
      strategy:
        sourceStrategy:
          env:
          - name: NPM_MIRROR
            value: ${NPM_MIRROR}
          from:
            kind: ImageStreamTag
            name: nodejs:6
            namespace: ${NAMESPACE}
        type: Source
      triggers:
      - type: ImageChange
      - type: ConfigChange
      - github:
          secret: ${GITHUB_WEBHOOK_SECRET}
        type: GitHub
      - generic:
          secret: ${GENERIC_WEBHOOK_SECRET}
        type: Generic
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      annotations:
        description: Defines how to deploy the application server
      name: ${NAME}
    spec:
      replicas: 1
      selector:
        name: ${NAME}
      strategy:
        type: Recreate
      template:
        metadata:
          labels:
            name: ${NAME}
          name: ${NAME}
        spec:
          containers:
          - env:
            - name: DATABASE_SERVICE_NAME
              value: ${DATABASE_SERVICE_NAME}
            - name: MONGODB_USER
              valueFrom:
                secretKeyRef:
                  key: database-user
                  name: ${NAME}
            - name: MONGODB_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: database-password
                  name: ${NAME}
            - name: MONGODB_DATABASE
              value: ${DATABASE_NAME}
            - name: MONGODB_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: database-admin-password
                  name: ${NAME}
            image: ' '
            livenessProbe:
              httpGet:
                path: /pagecount
                port: 8080
              initialDelaySeconds: 30
              timeoutSeconds: 3
            name: nodejs-mongodb-example
            ports:
            - containerPort: 8080
            readinessProbe:
              httpGet:
                path: /pagecount
                port: 8080
              initialDelaySeconds: 3
              timeoutSeconds: 3
            resources:
              limits:
                memory: ${MEMORY_LIMIT}
      triggers:
      - imageChangeParams:
          automatic: true
          containerNames:
          - nodejs-mongodb-example
          from:
            kind: ImageStreamTag
            name: ${NAME}:latest
        type: ImageChange
      - type: ConfigChange
  - apiVersion: v1
    kind: Service
    metadata:
      annotations:
        description: Exposes the database server
      name: ${DATABASE_SERVICE_NAME}
    spec:
      ports:
      - name: mongodb
        port: 27017
        targetPort: 27017
      selector:
        name: ${DATABASE_SERVICE_NAME}
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      annotations:
        description: Defines how to deploy the database
      name: ${DATABASE_SERVICE_NAME}
    spec:
      replicas: 1
      selector:
        name: ${DATABASE_SERVICE_NAME}
      strategy:
        type: Recreate
      template:
        metadata:
          labels:
            name: ${DATABASE_SERVICE_NAME}
          name: ${DATABASE_SERVICE_NAME}
        spec:
          containers:
          - env:
            - name: MONGODB_USER
              valueFrom:
                secretKeyRef:
                  key: database-user
                  name: ${NAME}
            - name: MONGODB_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: database-password
                  name: ${NAME}
            - name: MONGODB_DATABASE
              value: ${DATABASE_NAME}
            - name: MONGODB_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: database-admin-password
                  name: ${NAME}
            image: ' '
            livenessProbe:
              initialDelaySeconds: 30
              tcpSocket:
                port: 27017
              timeoutSeconds: 1
            name: mongodb
            ports:
            - containerPort: 27017
            readinessProbe:
              exec:
                command:
                - /bin/sh
                - -i
                - -c
                - mongo 127.0.0.1:27017/$MONGODB_DATABASE -u $MONGODB_USER -p $MONGODB_PASSWORD
                  --eval="quit()"
              initialDelaySeconds: 3
              timeoutSeconds: 1
            resources:
              limits:
                memory: ${MEMORY_MONGODB_LIMIT}
            volumeMounts:
            - mountPath: /var/lib/mongodb/data
              name: ${DATABASE_SERVICE_NAME}-data
          volumes:
          - emptyDir:
              medium: ""
            name: ${DATABASE_SERVICE_NAME}-data
      triggers:
      - imageChangeParams:
          automatic: true
          containerNames:
          - mongodb
          from:
            kind: ImageStreamTag
            name: mongodb:3.2
            namespace: ${NAMESPACE}
        type: ImageChange
      - type: ConfigChange
  parameters:
  - description: The name assigned to all of the frontend objects defined in this
      template.
    displayName: Name
    name: NAME
    required: true
    value: nodejs-mongodb-example
  - description: The OpenShift Namespace where the ImageStream resides.
    displayName: Namespace
    name: NAMESPACE
    required: true
    value: openshift
  - description: Maximum amount of memory the Node.js container can use.
    displayName: Memory Limit
    name: MEMORY_LIMIT
    required: true
    value: 512Mi
  - description: Maximum amount of memory the MongoDB container can use.
    displayName: Memory Limit (MongoDB)
    name: MEMORY_MONGODB_LIMIT
    required: true
    value: 512Mi
  - description: The URL of the repository with your application source code.
    displayName: Git Repository URL
    name: SOURCE_REPOSITORY_URL
    required: true
    value: https://github.com/openshift/nodejs-ex.git
  - description: Set this to a branch name, tag or other ref of your repository if
      you are not using the default branch.
    displayName: Git Reference
    name: SOURCE_REPOSITORY_REF
  - description: Set this to the relative path to your project if it is not in the
      root of your repository.
    displayName: Context Directory
    name: CONTEXT_DIR
  - description: The exposed hostname that will route to the Node.js service, if left
      blank a value will be defaulted.
    displayName: Application Hostname
    name: APPLICATION_DOMAIN
  - description: Github trigger secret.  A difficult to guess string encoded as part
      of the webhook URL.  Not encrypted.
    displayName: GitHub Webhook Secret
    from: '[a-zA-Z0-9]{40}'
    generate: expression
    name: GITHUB_WEBHOOK_SECRET
  - description: A secret string used to configure the Generic webhook.
    displayName: Generic Webhook Secret
    from: '[a-zA-Z0-9]{40}'
    generate: expression
    name: GENERIC_WEBHOOK_SECRET
  - displayName: Database Service Name
    name: DATABASE_SERVICE_NAME
    required: true
    value: mongodb
  - description: Username for MongoDB user that will be used for accessing the database.
    displayName: MongoDB Username
    from: user[A-Z0-9]{3}
    generate: expression
    name: DATABASE_USER
  - description: Password for the MongoDB user.
    displayName: MongoDB Password
    from: '[a-zA-Z0-9]{16}'
    generate: expression
    name: DATABASE_PASSWORD
  - displayName: Database Name
    name: DATABASE_NAME
    required: true
    value: sampledb
  - description: Password for the database admin user.
    displayName: Database Administrator Password
    from: '[a-zA-Z0-9]{16}'
    generate: expression
    name: DATABASE_ADMIN_PASSWORD
  - description: The custom NPM mirror URL
    displayName: Custom NPM Mirror URL
    name: NPM_MIRROR
- apiVersion: v1
  kind: Template
  metadata:
    annotations:
      description: PostgreSQL Replication Example
      iconClass: icon-database
      tags: database,postgresql,replication
    creationTimestamp: 2017-11-03T08:43:56Z
    name: pg-replica-example
    namespace: openshift
    resourceVersion: "27449126"
    selfLink: /oapi/v1/namespaces/openshift/templates/pg-replica-example
    uid: 21207e2b-c073-11e7-bc85-ecf4bbe2f144
  objects:
  - apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: postgresql-data-claim
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: ${VOLUME_CAPACITY}
  - apiVersion: v1
    kind: Service
    metadata:
      labels:
        name: ${POSTGRESQL_MASTER_SERVICE_NAME}
      name: ${POSTGRESQL_MASTER_SERVICE_NAME}
    spec:
      clusterIP: None
      ports:
      - port: 5432
        targetPort: 5432
      selector:
        name: ${POSTGRESQL_MASTER_SERVICE_NAME}
  - apiVersion: v1
    kind: Service
    metadata:
      labels:
        name: ${POSTGRESQL_SLAVE_SERVICE_NAME}
      name: ${POSTGRESQL_SLAVE_SERVICE_NAME}
    spec:
      clusterIP: None
      ports:
      - port: 5432
        targetPort: 5432
      selector:
        name: ${POSTGRESQL_SLAVE_SERVICE_NAME}
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      name: ${POSTGRESQL_MASTER_SERVICE_NAME}
    spec:
      replicas: 1
      selector:
        name: ${POSTGRESQL_MASTER_SERVICE_NAME}
      strategy:
        type: Recreate
      template:
        metadata:
          labels:
            name: ${POSTGRESQL_MASTER_SERVICE_NAME}
        spec:
          containers:
          - args:
            - run-postgresql-master
            env:
            - name: POSTGRESQL_MASTER_USER
              value: ${POSTGRESQL_MASTER_USER}
            - name: POSTGRESQL_MASTER_PASSWORD
              value: ${POSTGRESQL_MASTER_PASSWORD}
            - name: POSTGRESQL_USER
              value: ${POSTGRESQL_USER}
            - name: POSTGRESQL_PASSWORD
              value: ${POSTGRESQL_PASSWORD}
            - name: POSTGRESQL_DATABASE
              value: ${POSTGRESQL_DATABASE}
            - name: POSTGRESQL_ADMIN_PASSWORD
              value: ${POSTGRESQL_ADMIN_PASSWORD}
            image: ' '
            livenessProbe:
              initialDelaySeconds: 30
              tcpSocket:
                port: 5432
              timeoutSeconds: 1
            name: postgresql-master
            ports:
            - containerPort: 5432
            readinessProbe:
              exec:
                command:
                - /bin/sh
                - -i
                - -c
                - psql -h 127.0.0.1 -U $POSTGRESQL_USER -q -d $POSTGRESQL_DATABASE
                  -c 'SELECT 1'
              initialDelaySeconds: 5
              timeoutSeconds: 1
            volumeMounts:
            - mountPath: /var/lib/pgsql/data
              name: postgresql-data
          terminationGracePeriodSeconds: 10
          volumes:
          - name: postgresql-data
            persistentVolumeClaim:
              claimName: postgresql-data-claim
      triggers:
      - imageChangeParams:
          automatic: true
          containerNames:
          - postgresql-master
          from:
            kind: ImageStreamTag
            name: ${IMAGESTREAMTAG}
            namespace: ${NAMESPACE}
          lastTriggeredImage: ""
        type: ImageChange
      - type: ConfigChange
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      name: ${POSTGRESQL_SLAVE_SERVICE_NAME}
    spec:
      replicas: 1
      selector:
        name: ${POSTGRESQL_SLAVE_SERVICE_NAME}
      strategy:
        type: Recreate
      template:
        metadata:
          labels:
            name: ${POSTGRESQL_SLAVE_SERVICE_NAME}
        spec:
          containers:
          - args:
            - run-postgresql-slave
            env:
            - name: POSTGRESQL_MASTER_SERVICE_NAME
              value: ${POSTGRESQL_MASTER_SERVICE_NAME}
            - name: POSTGRESQL_MASTER_USER
              value: ${POSTGRESQL_MASTER_USER}
            - name: POSTGRESQL_MASTER_PASSWORD
              value: ${POSTGRESQL_MASTER_PASSWORD}
            - name: POSTGRESQL_USER
              value: ${POSTGRESQL_USER}
            - name: POSTGRESQL_PASSWORD
              value: ${POSTGRESQL_PASSWORD}
            - name: POSTGRESQL_DATABASE
              value: ${POSTGRESQL_DATABASE}
            image: ' '
            livenessProbe:
              initialDelaySeconds: 30
              tcpSocket:
                port: 5432
              timeoutSeconds: 1
            name: postgresql-slave
            ports:
            - containerPort: 5432
            readinessProbe:
              exec:
                command:
                - /bin/sh
                - -i
                - -c
                - psql -h 127.0.0.1 -U $POSTGRESQL_USER -q -d $POSTGRESQL_DATABASE
                  -c 'SELECT 1'
              initialDelaySeconds: 5
              timeoutSeconds: 1
            volumeMounts:
            - mountPath: /var/lib/pgsql/data
              name: postgresql-data
          terminationGracePeriodSeconds: 10
          volumes:
          - emptyDir: {}
            name: postgresql-data
      triggers:
      - imageChangeParams:
          automatic: true
          containerNames:
          - postgresql-slave
          from:
            kind: ImageStreamTag
            name: ${IMAGESTREAMTAG}
            namespace: ${NAMESPACE}
          lastTriggeredImage: ""
        type: ImageChange
      - type: ConfigChange
  parameters:
  - description: The username used for master-slave replication
    name: POSTGRESQL_MASTER_USER
    required: true
    value: master
  - description: The password for the PostgreSQL replication user
    from: '[a-zA-Z0-9]{12}'
    generate: expression
    name: POSTGRESQL_MASTER_PASSWORD
    required: true
  - description: The username that clients will use to connect to PostgreSQL server
    name: POSTGRESQL_USER
    required: true
    value: user
  - description: The password for the PostgreSQL master user
    from: '[a-zA-Z0-9]{12}'
    generate: expression
    name: POSTGRESQL_PASSWORD
    required: true
  - description: The name of the database that will be created
    name: POSTGRESQL_DATABASE
    required: true
    value: userdb
  - description: The password for the PostgreSQL administrator
    from: '[a-zA-Z0-9]{12}'
    generate: expression
    name: POSTGRESQL_ADMIN_PASSWORD
  - description: 'The name of the PostgreSQL Service (used to DNS lookup, default:
      ''postgresql-master'')'
    name: POSTGRESQL_MASTER_SERVICE_NAME
    required: true
    value: postgresql-master
  - description: 'The name of the PostgreSQL Service (used to DNS lookup, default:
      ''postgresql-slave'')'
    name: POSTGRESQL_SLAVE_SERVICE_NAME
    required: true
    value: postgresql-slave
  - description: Volume space available for data, e.g. 512Mi, 2Gi
    name: VOLUME_CAPACITY
    required: true
    value: 512Mi
  - description: The OpenShift ImageStreamTag to use for PostgreSQL.
    displayName: ImageStreamTag
    name: IMAGESTREAMTAG
    value: postgresql:9.5
  - description: The OpenShift Namespace where the ImageStream resides.
    displayName: Namespace
    name: NAMESPACE
    value: openshift
- apiVersion: v1
  kind: Template
  labels:
    component: postgresql_single
    template: postgresql-single-persistent-template
  message: |-
    The following service(s) have been created in your project: ${DATABASE_SERVICE_NAME}.

           Username: ${POSTGRESQL_USER}
           Password: ${POSTGRESQL_PASSWORD}
      Database Name: ${POSTGRESQL_DATABASE}
     Connection URL: postgresql://${DATABASE_SERVICE_NAME}:5432/

    For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/postgresql-container/blob/master/9.4.
  metadata:
    annotations:
      description: ""
      iconClass: icon-postgresql
      openshift.io/display-name: Postgresql-Single (Persistent)
      tags: database,postgresql
    creationTimestamp: 2017-11-03T02:29:52Z
    name: postgresql-single-persistent
    namespace: openshift
    resourceVersion: "26032680"
    selfLink: /oapi/v1/namespaces/openshift/templates/postgresql-single-persistent
    uid: df333118-c03e-11e7-bc85-ecf4bbe2f144
  objects:
  - apiVersion: v1
    kind: Secret
    metadata:
      annotations:
        template.openshift.io/expose-password: '{.data[''database-password'']}'
        template.openshift.io/expose-username: '{.data[''database-user'']}'
      labels:
        app: ${NAME}
      name: ${NAME}
    stringData:
      database-password: ${POSTGRESQL_PASSWORD}
      database-user: ${POSTGRESQL_USER}
  - apiVersion: v1
    kind: Service
    metadata:
      annotations:
        template.openshift.io/expose-uri: postgres://{.spec.clusterIP}:{.spec.ports[?(.name=="postgresql")].port}
      labels:
        app: ${NAME}
      name: postgresql
    spec:
      clusterIP: None
      ports:
      - name: postgresql
        port: 5432
        protocol: TCP
        targetPort: 5432
      selector:
        app: ${NAME}
  - apiVersion: v1
    kind: Service
    metadata:
      labels:
        app: ${NAME}
      name: postgresql-out
    spec:
      ports:
      - name: postgresql
        port: 5432
        protocol: TCP
        targetPort: 5432
      selector:
        app: ${NAME}
      type: NodePort
  - apiVersion: apps/v1beta1
    kind: StatefulSet
    metadata:
      labels:
        app: ${NAME}
      name: ${NAME}
    spec:
      replicas: "1"
      serviceName: postgresql
      template:
        metadata:
          labels:
            app: ${NAME}
            component: postgresql_single
            template: postgresql_single-persistent
        spec:
          containers:
          - capabilities: {}
            env:
            - name: POSTGRESQL_USERNAME
              valueFrom:
                secretKeyRef:
                  key: database-user
                  name: ${NAME}
            - name: POSTGRESQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: database-password
                  name: ${NAME}
            - name: POSTGRESQL_DATABASE
              value: ${POSTGRESQL_DATABASE}
            image: ${SOURCE_IMAGE}
            imagePullPolicy: Always
            livenessProbe:
              initialDelaySeconds: 30
              tcpSocket:
                port: 5432
              timeoutSeconds: 8
            name: ${NAME}
            ports:
            - containerPort: 5432
              protocol: TCP
            readinessProbe:
              initialDelaySeconds: 15
              tcpSocket:
                port: 5432
              timeoutSeconds: 1
            resources:
              limits:
                cpu: ${RESOURCE_CPU_LIMIT}
                memory: ${RESOURCE_MEMORY_LIMIT}G
              requests:
                cpu: 300m
                memory: 300M
            securityContext:
              capabilities: {}
              privileged: true
            terminationMessagePath: /dev/termination-log
            volumeMounts:
            - mountPath: /var/lib/pgsql/data
              name: storage
          dnsPolicy: ClusterFirst
          restartPolicy: Always
      volumeClaimTemplates:
      - metadata:
          annotations:
            volume.alpha.kubernetes.io/storage-class: slow
          name: storage
        spec:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: ${VOLUME_CAPACITY}Gi
  parameters:
  - description: Name.
    name: NAME
    required: true
    value: postgresql-single
  - description: Container image source.
    name: SOURCE_IMAGE
    required: true
    value: bitnami/postgresql:9.6
  - description: mysql Data capacity.
    name: VOLUME_CAPACITY
    required: true
    value: "5"
  - description: The limits for memory resource.
    name: RESOURCE_MEMORY_LIMIT
    value: "4"
  - description: The limits for CPU resource.
    name: RESOURCE_CPU_LIMIT
    value: "2"
  - description: Name of the PostgreSQL database accessed.
    displayName: PostgreSQL Database Name
    name: POSTGRESQL_DATABASE
    required: true
    value: sampledb
  - description: Username for PostgreSQL user that will be used for accessing the
      database.
    displayName: PostgreSQL Connection Username
    from: user[A-Z0-9]{3}
    generate: expression
    name: POSTGRESQL_USER
    required: true
  - description: Password for the PostgreSQL connection user.
    displayName: PostgreSQL Connection Password
    from: '[a-zA-Z0-9]{16}'
    generate: expression
    name: POSTGRESQL_PASSWORD
    required: true
- apiVersion: v1
  kind: Template
  labels:
    component: rabbitmq_single
    template: rabbitmq-single-persistent
  metadata:
    annotations:
      description: Create a single RabbitMQ, with persistent storage.
      iconClass: icon-database
      openshift.io/display-name: RabbitMQ_single (Persistent)
      tags: database,rabbitmq-single
    creationTimestamp: 2017-10-16T04:20:00Z
    name: rabbitmq-single-persistent
    namespace: openshift
    resourceVersion: "27449430"
    selfLink: /oapi/v1/namespaces/openshift/templates/rabbitmq-single-persistent
    uid: 46426f12-b229-11e7-9b95-ecf4bbe2f144
  objects:
  - apiVersion: v1
    kind: Service
    metadata:
      labels:
        app: ${NAME}
      name: rabbitmq
    spec:
      clusterIP: None
      ports:
      - name: queue-port
        port: 5672
      - name: cluster-port
        port: 4369
      - name: dist-port
        port: 25672
      selector:
        app: ${NAME}
  - apiVersion: v1
    kind: Service
    metadata:
      name: rabbitmq-management
    spec:
      ports:
      - name: management-console
        port: 15672
      selector:
        app: ${NAME}
      sessionAffinity: ClientIP
      type: NodePort
  - apiVersion: apps/v1beta1
    kind: StatefulSet
    metadata:
      labels:
        app: ${NAME}
      name: ${NAME}
    spec:
      replicas: "1"
      serviceName: rabbitmq
      template:
        metadata:
          labels:
            app: ${NAME}
            component: rabbitmq_single
            template: rabbitmq_single-persistent
        spec:
          affinity:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: app
                      operator: In
                      values:
                      - ${NAME}
                  topologyKey: kubernetes.io/hostname
                weight: 100
          containers:
          - env:
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: RABBITMQ_DEFAULT_USER
              value: admin
            - name: RABBITMQ_DEFAULT_PASS
              value: adminpass
            - name: RABBITMQ_ERLANG_COOKIE
              value: x8KQ67SZ6GtxxwRxw7VOcwy8KXg3jX5z
            - name: RABBITMQ_USE_LONGNAME
              value: "true"
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: RABBITMQ_NODENAME
              value: rabbit@$(NODE_NAME).rabbitmq.$(NAMESPACE).svc.cluster.local
            image: ${SOURCE_IMAGE}
            imagePullPolicy: Always
            livenessProbe:
              exec:
                command:
                - rabbitmqctl
                - status
              initialDelaySeconds: 30
              timeoutSeconds: 5
            name: ${NAME}
            ports:
            - containerPort: 5672
              name: queue-port
            - containerPort: 15672
              name: management-port
            - containerPort: 4369
              name: cluster-port
            - containerPort: 25672
              name: dist-port
            readinessProbe:
              exec:
                command:
                - rabbitmqctl
                - status
              initialDelaySeconds: 10
              timeoutSeconds: 5
            resources:
              limits:
                cpu: ${RESOURCE_CPU_LIMIT}
                memory: ${RESOURCE_MEMORY_LIMIT}G
              requests:
                cpu: 300m
                memory: 300M
            securityContext:
              privileged: true
              runAsUser: 0
            volumeMounts:
            - mountPath: /var/lib/rabbitmq
              name: storage
          terminationGracePeriodSeconds: 10
      volumeClaimTemplates:
      - metadata:
          annotations:
            volume.alpha.kubernetes.io/storage-class: slow
          name: storage
        spec:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: ${VOLUME_MQ_CAPACITY}Gi
  parameters:
  - description: Name.
    name: NAME
    required: true
    value: rabbitmq
  - description: Container image source.
    name: SOURCE_IMAGE
    required: true
    value: rabbitmq:3.6.12-management
  - description: rabbitmq Data capacity.
    name: VOLUME_MQ_CAPACITY
    required: true
    value: "20"
  - description: The limits for memory resource.
    name: RESOURCE_MEMORY_LIMIT
    value: "4"
  - description: The limits for CPU resource.
    name: RESOURCE_CPU_LIMIT
    value: "2"
- apiVersion: v1
  kind: Template
  labels:
    template: rails-pgsql-persistent
  message: |-
    The following service(s) have been created in your project: ${NAME}, ${DATABASE_SERVICE_NAME}.

    For more information about using this template, including OpenShift considerations, see https://github.com/openshift/rails-ex/blob/master/README.md.
  metadata:
    annotations:
      description: An example Rails application with a PostgreSQL database. For more
        information about using this template, including OpenShift considerations,
        see https://github.com/openshift/rails-ex/blob/master/README.md.
      iconClass: icon-ruby
      openshift.io/display-name: Rails + PostgreSQL (Persistent)
      tags: quickstart,ruby,rails
      template.openshift.io/documentation-url: https://github.com/openshift/rails-ex
      template.openshift.io/long-description: This template defines resources needed
        to develop a Rails application, including a build configuration, application
        deployment configuration, and database deployment configuration.
      template.openshift.io/provider-display-name: Red Hat, Inc.
      template.openshift.io/support-url: https://access.redhat.com
    creationTimestamp: 2017-08-23T07:04:10Z
    name: rails-pgsql-persistent
    namespace: openshift
    resourceVersion: "872"
    selfLink: /oapi/v1/namespaces/openshift/templates/rails-pgsql-persistent
    uid: 43667834-87d1-11e7-9a67-ecf4bbe2f144
  objects:
  - apiVersion: v1
    kind: Secret
    metadata:
      annotations:
        template.openshift.io/expose-password: '{.data[''application-password'']}'
        template.openshift.io/expose-username: '{.data[''application-user'']}'
      name: ${NAME}
    stringData:
      application-password: ${APPLICATION_PASSWORD}
      application-user: ${APPLICATION_USER}
      database-password: ${DATABASE_PASSWORD}
      database-user: ${DATABASE_USER}
      keybase: ${SECRET_KEY_BASE}
  - apiVersion: v1
    kind: Service
    metadata:
      annotations:
        description: Exposes and load balances the application pods
        service.alpha.openshift.io/dependencies: '[{"name": "${DATABASE_SERVICE_NAME}",
          "kind": "Service"}]'
      name: ${NAME}
    spec:
      ports:
      - name: web
        port: 8080
        targetPort: 8080
      selector:
        name: ${NAME}
  - apiVersion: v1
    kind: Route
    metadata:
      annotations:
        template.openshift.io/expose-uri: http://{.spec.host}{.spec.path}
      name: ${NAME}
    spec:
      host: ${APPLICATION_DOMAIN}
      to:
        kind: Service
        name: ${NAME}
  - apiVersion: v1
    kind: ImageStream
    metadata:
      annotations:
        description: Keeps track of changes in the application image
      name: ${NAME}
  - apiVersion: v1
    kind: BuildConfig
    metadata:
      annotations:
        description: Defines how to build the application
      name: ${NAME}
    spec:
      output:
        to:
          kind: ImageStreamTag
          name: ${NAME}:latest
      postCommit:
        script: bundle exec rake test
      source:
        contextDir: ${CONTEXT_DIR}
        git:
          ref: ${SOURCE_REPOSITORY_REF}
          uri: ${SOURCE_REPOSITORY_URL}
        type: Git
      strategy:
        sourceStrategy:
          env:
          - name: RUBYGEM_MIRROR
            value: ${RUBYGEM_MIRROR}
          from:
            kind: ImageStreamTag
            name: ruby:2.3
            namespace: ${NAMESPACE}
        type: Source
      triggers:
      - type: ImageChange
      - type: ConfigChange
      - github:
          secret: ${GITHUB_WEBHOOK_SECRET}
        type: GitHub
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      annotations:
        description: Defines how to deploy the application server
      name: ${NAME}
    spec:
      replicas: 1
      selector:
        name: ${NAME}
      strategy:
        recreateParams:
          pre:
            execNewPod:
              command:
              - ./migrate-database.sh
              containerName: ${NAME}
            failurePolicy: Abort
        type: Recreate
      template:
        metadata:
          labels:
            name: ${NAME}
          name: ${NAME}
        spec:
          containers:
          - env:
            - name: DATABASE_SERVICE_NAME
              value: ${DATABASE_SERVICE_NAME}
            - name: POSTGRESQL_USER
              valueFrom:
                secretKeyRef:
                  key: database-user
                  name: ${NAME}
            - name: POSTGRESQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: database-password
                  name: ${NAME}
            - name: SECRET_KEY_BASE
              valueFrom:
                secretKeyRef:
                  key: keybase
                  name: ${NAME}
            - name: POSTGRESQL_DATABASE
              value: ${DATABASE_NAME}
            - name: POSTGRESQL_MAX_CONNECTIONS
              value: ${POSTGRESQL_MAX_CONNECTIONS}
            - name: POSTGRESQL_SHARED_BUFFERS
              value: ${POSTGRESQL_SHARED_BUFFERS}
            - name: APPLICATION_DOMAIN
              value: ${APPLICATION_DOMAIN}
            - name: APPLICATION_USER
              valueFrom:
                secretKeyRef:
                  key: application-user
                  name: ${NAME}
            - name: APPLICATION_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: application-password
                  name: ${NAME}
            - name: RAILS_ENV
              value: ${RAILS_ENV}
            image: ' '
            livenessProbe:
              httpGet:
                path: /articles
                port: 8080
              initialDelaySeconds: 10
              timeoutSeconds: 3
            name: ${NAME}
            ports:
            - containerPort: 8080
            readinessProbe:
              httpGet:
                path: /articles
                port: 8080
              initialDelaySeconds: 5
              timeoutSeconds: 3
            resources:
              limits:
                memory: ${MEMORY_LIMIT}
      triggers:
      - imageChangeParams:
          automatic: true
          containerNames:
          - ${NAME}
          from:
            kind: ImageStreamTag
            name: ${NAME}:latest
        type: ImageChange
      - type: ConfigChange
  - apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: ${DATABASE_SERVICE_NAME}
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: ${VOLUME_CAPACITY}
  - apiVersion: v1
    kind: Service
    metadata:
      annotations:
        description: Exposes the database server
      name: ${DATABASE_SERVICE_NAME}
    spec:
      ports:
      - name: postgresql
        port: 5432
        targetPort: 5432
      selector:
        name: ${DATABASE_SERVICE_NAME}
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      annotations:
        description: Defines how to deploy the database
      name: ${DATABASE_SERVICE_NAME}
    spec:
      replicas: 1
      selector:
        name: ${DATABASE_SERVICE_NAME}
      strategy:
        type: Recreate
      template:
        metadata:
          labels:
            name: ${DATABASE_SERVICE_NAME}
          name: ${DATABASE_SERVICE_NAME}
        spec:
          containers:
          - env:
            - name: POSTGRESQL_USER
              valueFrom:
                secretKeyRef:
                  key: database-user
                  name: ${NAME}
            - name: POSTGRESQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: database-password
                  name: ${NAME}
            - name: POSTGRESQL_DATABASE
              value: ${DATABASE_NAME}
            - name: POSTGRESQL_MAX_CONNECTIONS
              value: ${POSTGRESQL_MAX_CONNECTIONS}
            - name: POSTGRESQL_SHARED_BUFFERS
              value: ${POSTGRESQL_SHARED_BUFFERS}
            image: ' '
            livenessProbe:
              initialDelaySeconds: 30
              tcpSocket:
                port: 5432
              timeoutSeconds: 1
            name: postgresql
            ports:
            - containerPort: 5432
            readinessProbe:
              exec:
                command:
                - /bin/sh
                - -i
                - -c
                - psql -h 127.0.0.1 -U ${POSTGRESQL_USER} -q -d ${POSTGRESQL_DATABASE}
                  -c 'SELECT 1'
              initialDelaySeconds: 5
              timeoutSeconds: 1
            resources:
              limits:
                memory: ${MEMORY_POSTGRESQL_LIMIT}
            volumeMounts:
            - mountPath: /var/lib/pgsql/data
              name: ${DATABASE_SERVICE_NAME}-data
          volumes:
          - name: ${DATABASE_SERVICE_NAME}-data
            persistentVolumeClaim:
              claimName: ${DATABASE_SERVICE_NAME}
      triggers:
      - imageChangeParams:
          automatic: true
          containerNames:
          - postgresql
          from:
            kind: ImageStreamTag
            name: postgresql:9.5
            namespace: ${NAMESPACE}
        type: ImageChange
      - type: ConfigChange
  parameters:
  - description: The name assigned to all of the frontend objects defined in this
      template.
    displayName: Name
    name: NAME
    required: true
    value: rails-pgsql-persistent
  - description: The OpenShift Namespace where the ImageStream resides.
    displayName: Namespace
    name: NAMESPACE
    required: true
    value: openshift
  - description: Maximum amount of memory the Rails container can use.
    displayName: Memory Limit
    name: MEMORY_LIMIT
    required: true
    value: 512Mi
  - description: Maximum amount of memory the PostgreSQL container can use.
    displayName: Memory Limit (PostgreSQL)
    name: MEMORY_POSTGRESQL_LIMIT
    required: true
    value: 512Mi
  - description: Volume space available for data, e.g. 512Mi, 2Gi
    displayName: Volume Capacity
    name: VOLUME_CAPACITY
    required: true
    value: 1Gi
  - description: The URL of the repository with your application source code.
    displayName: Git Repository URL
    name: SOURCE_REPOSITORY_URL
    required: true
    value: https://github.com/openshift/rails-ex.git
  - description: Set this to a branch name, tag or other ref of your repository if
      you are not using the default branch.
    displayName: Git Reference
    name: SOURCE_REPOSITORY_REF
  - description: Set this to the relative path to your project if it is not in the
      root of your repository.
    displayName: Context Directory
    name: CONTEXT_DIR
  - description: The exposed hostname that will route to the Rails service, if left
      blank a value will be defaulted.
    displayName: Application Hostname
    name: APPLICATION_DOMAIN
  - description: Github trigger secret.  A difficult to guess string encoded as part
      of the webhook URL.  Not encrypted.
    displayName: GitHub Webhook Secret
    from: '[a-zA-Z0-9]{40}'
    generate: expression
    name: GITHUB_WEBHOOK_SECRET
  - description: Your secret key for verifying the integrity of signed cookies.
    displayName: Secret Key
    from: '[a-z0-9]{127}'
    generate: expression
    name: SECRET_KEY_BASE
  - description: The application user that is used within the sample application to
      authorize access on pages.
    displayName: Application Username
    name: APPLICATION_USER
    required: true
    value: openshift
  - description: The application password that is used within the sample application
      to authorize access on pages.
    displayName: Application Password
    name: APPLICATION_PASSWORD
    required: true
    value: secret
  - description: Environment under which the sample application will run. Could be
      set to production, development or test.
    displayName: Rails Environment
    name: RAILS_ENV
    required: true
    value: production
  - displayName: Database Service Name
    name: DATABASE_SERVICE_NAME
    required: true
    value: postgresql
  - displayName: Database Username
    from: user[A-Z0-9]{3}
    generate: expression
    name: DATABASE_USER
  - displayName: Database Password
    from: '[a-zA-Z0-9]{8}'
    generate: expression
    name: DATABASE_PASSWORD
  - displayName: Database Name
    name: DATABASE_NAME
    required: true
    value: root
  - displayName: Maximum Database Connections
    name: POSTGRESQL_MAX_CONNECTIONS
    value: "100"
  - displayName: Shared Buffer Amount
    name: POSTGRESQL_SHARED_BUFFERS
    value: 12MB
  - description: The custom RubyGems mirror URL
    displayName: Custom RubyGems Mirror URL
    name: RUBYGEM_MIRROR
- apiVersion: v1
  kind: Template
  labels:
    template: rails-postgresql-example
  message: |-
    The following service(s) have been created in your project: ${NAME}, ${DATABASE_SERVICE_NAME}.

    For more information about using this template, including OpenShift considerations, see https://github.com/openshift/rails-ex/blob/master/README.md.
  metadata:
    annotations:
      description: |-
        An example Rails application with a PostgreSQL database. For more information about using this template, including OpenShift considerations, see https://github.com/openshift/rails-ex/blob/master/README.md.

        WARNING: Any data stored will be lost upon pod destruction. Only use this template for testing.
      iconClass: icon-ruby
      openshift.io/display-name: Rails + PostgreSQL (Ephemeral)
      tags: quickstart,ruby,rails
      template.openshift.io/documentation-url: https://github.com/openshift/rails-ex
      template.openshift.io/long-description: This template defines resources needed
        to develop a Rails application, including a build configuration, application
        deployment configuration, and database deployment configuration.  The database
        is stored in non-persistent storage, so this configuration should be used
        for experimental purposes only.
      template.openshift.io/provider-display-name: Red Hat, Inc.
      template.openshift.io/support-url: https://access.redhat.com
    creationTimestamp: 2017-08-23T07:04:10Z
    name: rails-postgresql-example
    namespace: openshift
    resourceVersion: "873"
    selfLink: /oapi/v1/namespaces/openshift/templates/rails-postgresql-example
    uid: 4367b994-87d1-11e7-9a67-ecf4bbe2f144
  objects:
  - apiVersion: v1
    kind: Secret
    metadata:
      annotations:
        template.openshift.io/expose-password: '{.data[''application-password'']}'
        template.openshift.io/expose-username: '{.data[''application-user'']}'
      name: ${NAME}
    stringData:
      application-password: ${APPLICATION_PASSWORD}
      application-user: ${APPLICATION_USER}
      database-password: ${DATABASE_PASSWORD}
      database-user: ${DATABASE_USER}
      keybase: ${SECRET_KEY_BASE}
  - apiVersion: v1
    kind: Service
    metadata:
      annotations:
        description: Exposes and load balances the application pods
        service.alpha.openshift.io/dependencies: '[{"name": "${DATABASE_SERVICE_NAME}",
          "kind": "Service"}]'
      name: ${NAME}
    spec:
      ports:
      - name: web
        port: 8080
        targetPort: 8080
      selector:
        name: ${NAME}
  - apiVersion: v1
    kind: Route
    metadata:
      annotations:
        template.openshift.io/expose-uri: http://{.spec.host}{.spec.path}
      name: ${NAME}
    spec:
      host: ${APPLICATION_DOMAIN}
      to:
        kind: Service
        name: ${NAME}
  - apiVersion: v1
    kind: ImageStream
    metadata:
      annotations:
        description: Keeps track of changes in the application image
      name: ${NAME}
  - apiVersion: v1
    kind: BuildConfig
    metadata:
      annotations:
        description: Defines how to build the application
      name: ${NAME}
    spec:
      output:
        to:
          kind: ImageStreamTag
          name: ${NAME}:latest
      postCommit:
        script: bundle exec rake test
      source:
        contextDir: ${CONTEXT_DIR}
        git:
          ref: ${SOURCE_REPOSITORY_REF}
          uri: ${SOURCE_REPOSITORY_URL}
        type: Git
      strategy:
        sourceStrategy:
          env:
          - name: RUBYGEM_MIRROR
            value: ${RUBYGEM_MIRROR}
          from:
            kind: ImageStreamTag
            name: ruby:2.3
            namespace: ${NAMESPACE}
        type: Source
      triggers:
      - type: ImageChange
      - type: ConfigChange
      - github:
          secret: ${GITHUB_WEBHOOK_SECRET}
        type: GitHub
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      annotations:
        description: Defines how to deploy the application server
      name: ${NAME}
    spec:
      replicas: 1
      selector:
        name: ${NAME}
      strategy:
        recreateParams:
          pre:
            execNewPod:
              command:
              - ./migrate-database.sh
              containerName: ${NAME}
            failurePolicy: Abort
        type: Recreate
      template:
        metadata:
          labels:
            name: ${NAME}
          name: ${NAME}
        spec:
          containers:
          - env:
            - name: DATABASE_SERVICE_NAME
              value: ${DATABASE_SERVICE_NAME}
            - name: POSTGRESQL_USER
              valueFrom:
                secretKeyRef:
                  key: database-user
                  name: ${NAME}
            - name: POSTGRESQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: database-password
                  name: ${NAME}
            - name: POSTGRESQL_DATABASE
              value: ${DATABASE_NAME}
            - name: SECRET_KEY_BASE
              valueFrom:
                secretKeyRef:
                  key: keybase
                  name: ${NAME}
            - name: POSTGRESQL_MAX_CONNECTIONS
              value: ${POSTGRESQL_MAX_CONNECTIONS}
            - name: POSTGRESQL_SHARED_BUFFERS
              value: ${POSTGRESQL_SHARED_BUFFERS}
            - name: APPLICATION_DOMAIN
              value: ${APPLICATION_DOMAIN}
            - name: APPLICATION_USER
              valueFrom:
                secretKeyRef:
                  key: application-user
                  name: ${NAME}
            - name: APPLICATION_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: application-password
                  name: ${NAME}
            - name: RAILS_ENV
              value: ${RAILS_ENV}
            image: ' '
            livenessProbe:
              httpGet:
                path: /articles
                port: 8080
              initialDelaySeconds: 10
              timeoutSeconds: 3
            name: ${NAME}
            ports:
            - containerPort: 8080
            readinessProbe:
              httpGet:
                path: /articles
                port: 8080
              initialDelaySeconds: 5
              timeoutSeconds: 3
            resources:
              limits:
                memory: ${MEMORY_LIMIT}
      triggers:
      - imageChangeParams:
          automatic: true
          containerNames:
          - ${NAME}
          from:
            kind: ImageStreamTag
            name: ${NAME}:latest
        type: ImageChange
      - type: ConfigChange
  - apiVersion: v1
    kind: Service
    metadata:
      annotations:
        description: Exposes the database server
      name: ${DATABASE_SERVICE_NAME}
    spec:
      ports:
      - name: postgresql
        port: 5432
        targetPort: 5432
      selector:
        name: ${DATABASE_SERVICE_NAME}
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      annotations:
        description: Defines how to deploy the database
      name: ${DATABASE_SERVICE_NAME}
    spec:
      replicas: 1
      selector:
        name: ${DATABASE_SERVICE_NAME}
      strategy:
        type: Recreate
      template:
        metadata:
          labels:
            name: ${DATABASE_SERVICE_NAME}
          name: ${DATABASE_SERVICE_NAME}
        spec:
          containers:
          - env:
            - name: POSTGRESQL_USER
              valueFrom:
                secretKeyRef:
                  key: database-user
                  name: ${NAME}
            - name: POSTGRESQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: database-password
                  name: ${NAME}
            - name: POSTGRESQL_DATABASE
              value: ${DATABASE_NAME}
            - name: POSTGRESQL_MAX_CONNECTIONS
              value: ${POSTGRESQL_MAX_CONNECTIONS}
            - name: POSTGRESQL_SHARED_BUFFERS
              value: ${POSTGRESQL_SHARED_BUFFERS}
            image: ' '
            livenessProbe:
              initialDelaySeconds: 30
              tcpSocket:
                port: 5432
              timeoutSeconds: 1
            name: postgresql
            ports:
            - containerPort: 5432
            readinessProbe:
              exec:
                command:
                - /bin/sh
                - -i
                - -c
                - psql -h 127.0.0.1 -U ${POSTGRESQL_USER} -q -d ${POSTGRESQL_DATABASE}
                  -c 'SELECT 1'
              initialDelaySeconds: 5
              timeoutSeconds: 1
            resources:
              limits:
                memory: ${MEMORY_POSTGRESQL_LIMIT}
            volumeMounts:
            - mountPath: /var/lib/pgsql/data
              name: data
          volumes:
          - emptyDir: {}
            name: data
      triggers:
      - imageChangeParams:
          automatic: true
          containerNames:
          - postgresql
          from:
            kind: ImageStreamTag
            name: postgresql:9.5
            namespace: ${NAMESPACE}
        type: ImageChange
      - type: ConfigChange
  parameters:
  - description: The name assigned to all of the frontend objects defined in this
      template.
    displayName: Name
    name: NAME
    required: true
    value: rails-postgresql-example
  - description: The OpenShift Namespace where the ImageStream resides.
    displayName: Namespace
    name: NAMESPACE
    required: true
    value: openshift
  - description: Maximum amount of memory the Rails container can use.
    displayName: Memory Limit
    name: MEMORY_LIMIT
    required: true
    value: 512Mi
  - description: Maximum amount of memory the PostgreSQL container can use.
    displayName: Memory Limit (PostgreSQL)
    name: MEMORY_POSTGRESQL_LIMIT
    required: true
    value: 512Mi
  - description: The URL of the repository with your application source code.
    displayName: Git Repository URL
    name: SOURCE_REPOSITORY_URL
    required: true
    value: https://github.com/openshift/rails-ex.git
  - description: Set this to a branch name, tag or other ref of your repository if
      you are not using the default branch.
    displayName: Git Reference
    name: SOURCE_REPOSITORY_REF
  - description: Set this to the relative path to your project if it is not in the
      root of your repository.
    displayName: Context Directory
    name: CONTEXT_DIR
  - description: The exposed hostname that will route to the Rails service, if left
      blank a value will be defaulted.
    displayName: Application Hostname
    name: APPLICATION_DOMAIN
  - description: Github trigger secret.  A difficult to guess string encoded as part
      of the webhook URL.  Not encrypted.
    displayName: GitHub Webhook Secret
    from: '[a-zA-Z0-9]{40}'
    generate: expression
    name: GITHUB_WEBHOOK_SECRET
  - description: Your secret key for verifying the integrity of signed cookies.
    displayName: Secret Key
    from: '[a-z0-9]{127}'
    generate: expression
    name: SECRET_KEY_BASE
  - description: The application user that is used within the sample application to
      authorize access on pages.
    displayName: Application Username
    name: APPLICATION_USER
    required: true
    value: openshift
  - description: The application password that is used within the sample application
      to authorize access on pages.
    displayName: Application Password
    name: APPLICATION_PASSWORD
    required: true
    value: secret
  - description: Environment under which the sample application will run. Could be
      set to production, development or test.
    displayName: Rails Environment
    name: RAILS_ENV
    required: true
    value: production
  - displayName: Database Service Name
    name: DATABASE_SERVICE_NAME
    required: true
    value: postgresql
  - displayName: Database Username
    from: user[A-Z0-9]{3}
    generate: expression
    name: DATABASE_USER
  - displayName: Database Password
    from: '[a-zA-Z0-9]{8}'
    generate: expression
    name: DATABASE_PASSWORD
  - displayName: Database Name
    name: DATABASE_NAME
    required: true
    value: root
  - displayName: Maximum Database Connections
    name: POSTGRESQL_MAX_CONNECTIONS
    value: "100"
  - displayName: Shared Buffer Amount
    name: POSTGRESQL_SHARED_BUFFERS
    value: 12MB
  - description: The custom RubyGems mirror URL
    displayName: Custom RubyGems Mirror URL
    name: RUBYGEM_MIRROR
- apiVersion: v1
  kind: Template
  labels:
    template: redis-persistent-template
  message: |-
    The following service(s) have been created in your project: ${DATABASE_SERVICE_NAME}.

           Password: ${REDIS_PASSWORD}
     Connection URL: redis://${DATABASE_SERVICE_NAME}:6379/

    For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/redis-container/blob/master/3.2.
  metadata:
    annotations:
      description: |-
        Redis in-memory data structure store, with persistent storage. For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/redis-container/blob/master/3.2.

        NOTE: You must have persistent volumes available in your cluster to use this template.
      iconClass: icon-redis
      openshift.io/display-name: Redis (Persistent)
      tags: database,redis
      template.openshift.io/documentation-url: https://github.com/sclorg/redis-container/tree/master/3.2
      template.openshift.io/long-description: This template provides a standalone
        Redis server.  The data is stored on persistent storage.
      template.openshift.io/provider-display-name: Red Hat, Inc.
      template.openshift.io/support-url: https://access.redhat.com
    creationTimestamp: 2017-08-23T07:04:06Z
    name: redis-persistent
    namespace: openshift
    resourceVersion: "27940147"
    selfLink: /oapi/v1/namespaces/openshift/templates/redis-persistent
    uid: 40a5049f-87d1-11e7-9a67-ecf4bbe2f144
  objects:
  - apiVersion: v1
    kind: Secret
    metadata:
      annotations:
        template.openshift.io/expose-password: '{.data[''database-password'']}'
      name: ${DATABASE_SERVICE_NAME}
    stringData:
      database-password: ${REDIS_PASSWORD}
  - apiVersion: v1
    kind: Service
    metadata:
      annotations:
        template.openshift.io/expose-uri: redis://{.spec.clusterIP}:{.spec.ports[?(.name=="redis")].port}
      creationTimestamp: null
      name: ${DATABASE_SERVICE_NAME}
    spec:
      ports:
      - name: redis
        nodePort: 0
        port: 6379
        protocol: TCP
        targetPort: 6379
      selector:
        name: ${DATABASE_SERVICE_NAME}
      sessionAffinity: None
      type: ClusterIP
    status:
      loadBalancer: {}
  - apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      annotations:
        volume.beta.kubernetes.io/storage-class: slow
      name: ${DATABASE_SERVICE_NAME}
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: ${VOLUME_CAPACITY}
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      creationTimestamp: null
      name: ${DATABASE_SERVICE_NAME}
    spec:
      replicas: 1
      selector:
        name: ${DATABASE_SERVICE_NAME}
      strategy:
        type: Recreate
      template:
        metadata:
          creationTimestamp: null
          labels:
            name: ${DATABASE_SERVICE_NAME}
        spec:
          containers:
          - capabilities: {}
            env:
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: database-password
                  name: ${DATABASE_SERVICE_NAME}
            image: ' ${SOURCE_IMAGE}'
            imagePullPolicy: Always
            livenessProbe:
              initialDelaySeconds: 30
              tcpSocket:
                port: 6379
              timeoutSeconds: 1
            name: ${DATABASE_SERVICE_NAME}
            ports:
            - containerPort: 6379
              protocol: TCP
            readinessProbe:
              exec:
                command:
                - /bin/sh
                - -i
                - -c
                - test "$(redis-cli -h 127.0.0.1 -a $REDIS_PASSWORD ping)" == "PONG"
              initialDelaySeconds: 10
              timeoutSeconds: 1
            resources:
              limits:
                cpu: ${RESOURCE_CPU_LIMIT}
                memory: ${RESOURCE_MEMORY_LIMIT}G
              requests:
                cpu: 300m
                memory: 300Mi
            securityContext:
              capabilities: {}
              privileged: false
            terminationMessagePath: /dev/termination-log
            volumeMounts:
            - mountPath: /var/lib/redis/data
              name: data
          dnsPolicy: ClusterFirst
          restartPolicy: Always
          terminationGracePeriodSeconds: 10
      triggers:
      - type: ConfigChange
      volumeClaimTemplates:
      - metadata:
          annotations:
            volume.alpha.kubernetes.io/storage-class: slow
          name: data
        spec:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: ${PV_SIZE}Gi
    status: {}
  parameters:
  - description: The name of the OpenShift Service exposed for the database.
    displayName: Database Service Name
    name: DATABASE_SERVICE_NAME
    required: true
    value: redis
  - description: Password for the Redis connection user.
    displayName: Redis Connection Password
    from: '[a-zA-Z0-9]{16}'
    generate: expression
    name: REDIS_PASSWORD
    required: true
  - description: persistentVolume_size_GB
    name: PV_SIZE
    required: true
    value: "10"
  - description: Container image source.
    name: SOURCE_IMAGE
    required: true
    value: redis:3.2
  - description: The limits for memory resource.GB
    name: RESOURCE_MEMORY_LIMIT
    value: "4"
  - description: The limits for CPU resource.
    name: RESOURCE_CPU_LIMIT
    value: "1"
- apiVersion: v1
  kind: Template
  labels:
    app: rediscluster
    template: redis-cluster-inmemory
  metadata:
    annotations:
      description: Create a redis cluster,Data in memory PARTITION_REDUNDANT,By Geode
      iconClass: icon-database
      openshift.io/display-name: RedisCluster (inmemory)
      tags: database,rediscluster
    creationTimestamp: 2017-12-22T02:58:38Z
    name: rediscluster-inmemory
    namespace: openshift
    resourceVersion: "29145613"
    selfLink: /oapi/v1/namespaces/openshift/templates/rediscluster-inmemory
    uid: 027fe864-e6c4-11e7-86b8-364284a56e59
  objects:
  - apiVersion: v1
    kind: Service
    metadata:
      labels:
        name: locator
      name: locator
    spec:
      clusterIP: None
      ports:
      - name: locatorport
        port: 10334
        protocol: TCP
        targetPort: 10334
      selector:
        name: locator
  - apiVersion: v1
    kind: Service
    metadata:
      labels:
        name: ${NAME}
      name: redis
    spec:
      clusterIP: None
      ports:
      - name: redisport
        port: 6379
        protocol: TCP
        targetPort: 6379
      selector:
        name: ${NAME}
  - apiVersion: apps/v1beta1
    kind: StatefulSet
    metadata:
      labels:
        name: locator
      name: locator
    spec:
      replicas: 2
      serviceName: locator
      template:
        metadata:
          labels:
            name: locator
        spec:
          containers:
          - command:
            - /bin/sh
            - -ecx
            - |-
              rm -rf /data/$HOSTNAME/vf.gf.locator.pid

              gfsh start locator --name=$HOSTNAME --port=10334 \
                                 --mcast-port=0 --dir=/data/$HOSTNAME/ \
                                 --bind-address=$MY_POD_IP \
                                 --locators=locator-0.locator[10334],locator-1.locator[10334]

              #Do not delete it
              tail -f /data/$HOSTNAME/$HOSTNAME.log
            env:
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            - name: HOSTNAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
            image: ${REDIS_IMAGE}
            imagePullPolicy: Always
            livenessProbe:
              failureThreshold: 3
              initialDelaySeconds: 30
              periodSeconds: 10
              successThreshold: 1
              tcpSocket:
                port: 10334
              timeoutSeconds: 5
            name: locator
            ports:
            - containerPort: 10334
              protocol: TCP
            resources:
              limits:
                cpu: ${LOCATOR_CPU_LIMIT}
                memory: ${LOCATOR_MEM_LIMIT}G
              requests:
                cpu: 200m
                memory: 200M
            securityContext:
              privileged: false
              runAsUser: 0
            volumeMounts:
            - mountPath: /data
              name: storage
          terminationGracePeriodSeconds: 10
      volumeClaimTemplates:
      - metadata:
          annotations:
            volume.alpha.kubernetes.io/storage-class: slow
          name: storage
        spec:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: ${LOCATOR_PV_SIZE}Gi
  - apiVersion: apps/v1beta1
    kind: StatefulSet
    metadata:
      labels:
        name: ${NAME}
      name: ${NAME}
    spec:
      replicas: ${REPLICAS}
      serviceName: redis
      template:
        metadata:
          labels:
            name: ${NAME}
        spec:
          affinity:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: name
                      operator: In
                      values:
                      - ${NAME}
                  topologyKey: kubernetes.io/hostname
                weight: 100
          containers:
          - command:
            - /bin/sh
            - -ecx
            - "while true; do\n echo \"Waiting for locator-0.locator to come up\"\n
              ping -W 1 -c 1 locator-0.locator > /dev/null && break\n sleep 1s\ndone
              \ \n\nmkdir -p /data/$HOSTNAME  \n\nrm -rf /data/$HOSTNAME/vf.gf.server.pid\n\ngfsh
              start server \\\n      --name=\"$HOSTNAME\" --locator-wait-time=10 \\\n
              \     --dir=/data/$HOSTNAME/ \\\n      --mcast-port=0 --locators=locator-0.locator[10334],locator-1.locator[10334]
              \\\n      --redis-bind-address=$MY_POD_IP --redis-port=$REDIS_PORT \\\n
              \     --J=-Dgemfireredis.regiontype=$REGION_TYPE \\\n      --max-heap=8G
              --initial-heap=8G --max-connections=60000 \\\n      --max-threads=60000
              --off-heap-memory-size=52G --socket-buffer-size=1000000 \n      \n#Do
              not delete it   \n\ntail -f /data/$HOSTNAME/$HOSTNAME.log"
            env:
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: HOSTNAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: MAX_HEAP
              value: ${MAX_HEAP}
            - name: INITIAL_HEAP
              value: ${INITIAL_HEAP}
            - name: OFF_HEAP_MEMORY_SIZE
              value: ${OFF_HEAP_MEMORY_SIZE}
            - name: REGION_TYPE
              value: ${REGION_TYPE}
            - name: REDIS_PORT
              value: "6379"
            image: ${REDIS_IMAGE}
            imagePullPolicy: Always
            livenessProbe:
              failureThreshold: 3
              initialDelaySeconds: 50
              periodSeconds: 10
              successThreshold: 1
              tcpSocket:
                port: 6379
              timeoutSeconds: 3
            name: ${NAME}
            ports:
            - containerPort: 6379
              protocol: TCP
            resources:
              limits:
                cpu: ${REDIS_CPU_LIMIT}
                memory: ${REDIS_MEM_LIMIT}G
              requests:
                cpu: 200m
                memory: 200M
            securityContext:
              privileged: false
              runAsUser: 0
          terminationGracePeriodSeconds: 10
  parameters:
  - description: Name of the REDIS image BY GEODE_IMAGE
    name: REDIS_IMAGE
    required: true
    value: apachegeode/geode:1.3.0
  - description: name used as a service name and a selector
    name: NAME
    value: rediscluster
  - description: Number of Data Server replicas.
    name: REPLICAS
    required: true
    value: "2"
  - description: LOCATOR_CPU_LIMIT_CORES
    name: LOCATOR_CPU_LIMIT
    required: true
    value: "2"
  - description: LOCATOR_MEM_LIMIT_GB
    name: LOCATOR_MEM_LIMIT
    required: true
    value: "4"
  - description: REDIS_CPU_LIMIT_CORES
    name: REDIS_CPU_LIMIT
    required: true
    value: "8"
  - description: REDIS_MEM_LIMIT_GB
    name: REDIS_MEM_LIMIT
    required: true
    value: "64"
  - description: REDIS_MAX_HEAP_GB
    name: MAX_HEAP
    required: true
    value: "8"
  - description: REDIS_initial-heap_GB
    name: INITIAL_HEAP
    required: true
    value: "8"
  - description: REDIS_off-heap-memory-size_GB
    name: OFF_HEAP_MEMORY_SIZE
    required: true
    value: "52"
  - description: LOCATOR_persistentVolume_size_Gi
    name: LOCATOR_PV_SIZE
    required: true
    value: "1"
  - description: REGION_TYPE
    name: REGION_TYPE
    required: true
    value: REPLICATE
- apiVersion: v1
  kind: Template
  labels:
    app: rediscluster
    template: redis-cluster-persistent
  metadata:
    annotations:
      description: Create a redis cluster, with persistent storage,By Geode
      iconClass: icon-database
      openshift.io/display-name: RedisCluster (Persistent)
      tags: database,rediscluster
    creationTimestamp: 2017-12-22T02:55:39Z
    name: rediscluster-persistent
    namespace: openshift
    resourceVersion: "29146992"
    selfLink: /oapi/v1/namespaces/openshift/templates/rediscluster-persistent
    uid: 97d67405-e6c3-11e7-86b8-364284a56e59
  objects:
  - apiVersion: v1
    kind: Service
    metadata:
      labels:
        name: locator
      name: locator
    spec:
      clusterIP: None
      ports:
      - name: locatorport
        port: 10334
        protocol: TCP
        targetPort: 10334
      selector:
        name: locator
  - apiVersion: v1
    kind: Service
    metadata:
      labels:
        name: ${NAME}
      name: redis
    spec:
      clusterIP: None
      ports:
      - name: redisport
        port: 6379
        protocol: TCP
        targetPort: 6379
      selector:
        name: ${NAME}
  - apiVersion: apps/v1beta1
    kind: StatefulSet
    metadata:
      labels:
        name: locator
      name: locator
    spec:
      replicas: 2
      serviceName: locator
      template:
        metadata:
          labels:
            name: locator
        spec:
          containers:
          - command:
            - /bin/sh
            - -ecx
            - |-
              rm -rf /data/$HOSTNAME/vf.gf.locator.pid

              gfsh start locator --name=$HOSTNAME --port=10334 \
                                 --mcast-port=0 --dir=/data/$HOSTNAME/ \
                                 --bind-address=$MY_POD_IP \
                                 --locators=locator-0.locator[10334],locator-1.locator[10334]

              #Do not delete it
              tail -f /data/$HOSTNAME/$HOSTNAME.log
            env:
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            - name: HOSTNAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
            image: ${REDIS_IMAGE}
            imagePullPolicy: Always
            livenessProbe:
              failureThreshold: 3
              initialDelaySeconds: 30
              periodSeconds: 10
              successThreshold: 1
              tcpSocket:
                port: 10334
              timeoutSeconds: 5
            name: locator
            ports:
            - containerPort: 10334
              protocol: TCP
            resources:
              limits:
                cpu: ${LOCATOR_CPU_LIMIT}
                memory: ${LOCATOR_MEM_LIMIT}G
              requests:
                cpu: 200m
                memory: 200M
            securityContext:
              privileged: false
              runAsUser: 0
            volumeMounts:
            - mountPath: /data
              name: storage
          terminationGracePeriodSeconds: 10
      volumeClaimTemplates:
      - metadata:
          annotations:
            volume.alpha.kubernetes.io/storage-class: slow
          name: storage
        spec:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: ${LOCATOR_PV_SIZE}Gi
  - apiVersion: apps/v1beta1
    kind: StatefulSet
    metadata:
      labels:
        name: ${NAME}
      name: ${NAME}
    spec:
      replicas: ${REPLICAS}
      serviceName: redis
      template:
        metadata:
          labels:
            name: ${NAME}
        spec:
          affinity:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: name
                      operator: In
                      values:
                      - ${NAME}
                  topologyKey: kubernetes.io/hostname
                weight: 100
          containers:
          - command:
            - /bin/sh
            - -ecx
            - "while true; do\n echo \"Waiting for locator-0.locator to come up\"\n
              ping -W 1 -c 1 locator-0.locator > /dev/null && break\n sleep 1s\ndone\n\nmkdir
              -p /data/$HOSTNAME  \n\nrm -rf /data/$HOSTNAME/vf.gf.server.pid\n\ngfsh
              start server \\\n      --name=\"$HOSTNAME\" --locator-wait-time=10 \\\n
              \     --dir=/data/$HOSTNAME/ \\\n      --mcast-port=0 --locators=locator-0.locator[10334],locator-1.locator[10334]
              \\\n      --redis-bind-address=$MY_POD_IP --redis-port=$REDIS_PORT \\\n
              \     --J=-Dgemfireredis.regiontype=$REGION_TYPE \\\n      --max-heap=8G
              --initial-heap=8G --max-connections=60000 \\\n      --max-threads=60000
              --off-heap-memory-size=52G --socket-buffer-size=1000000 \n      \n#Do
              not delete it   \n\ntail -f /data/$HOSTNAME/$HOSTNAME.log"
            env:
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: HOSTNAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: MAX_HEAP
              value: ${MAX_HEAP}
            - name: INITIAL_HEAP
              value: ${INITIAL_HEAP}
            - name: OFF_HEAP_MEMORY_SIZE
              value: ${OFF_HEAP_MEMORY_SIZE}
            - name: REGION_TYPE
              value: ${REGION_TYPE}
            - name: REDIS_PORT
              value: "6379"
            image: ${REDIS_IMAGE}
            imagePullPolicy: Always
            livenessProbe:
              failureThreshold: 3
              initialDelaySeconds: 50
              periodSeconds: 10
              successThreshold: 1
              tcpSocket:
                port: 6379
              timeoutSeconds: 3
            name: ${NAME}
            ports:
            - containerPort: 6379
              protocol: TCP
            resources:
              limits:
                cpu: ${REDIS_CPU_LIMIT}
                memory: ${REDIS_MEM_LIMIT}G
              requests:
                cpu: 200m
                memory: 200M
            securityContext:
              privileged: false
              runAsUser: 0
            volumeMounts:
            - mountPath: /data
              name: redisdata
          terminationGracePeriodSeconds: 10
      volumeClaimTemplates:
      - metadata:
          annotations:
            volume.alpha.kubernetes.io/storage-class: slow
          name: redisdata
        spec:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: ${PV_SIZE}Gi
  parameters:
  - description: Name of the REDIS image BY GEODE_IMAGE
    name: REDIS_IMAGE
    required: true
    value: apachegeode/geode:1.3.0
  - description: name used as a service name and a selector
    name: NAME
    value: rediscluster
  - description: Number of Data Server replicas.
    name: REPLICAS
    required: true
    value: "2"
  - description: LOCATOR_CPU_LIMIT_CORES
    name: LOCATOR_CPU_LIMIT
    required: true
    value: "2"
  - description: LOCATOR_MEM_LIMIT_GB
    name: LOCATOR_MEM_LIMIT
    required: true
    value: "4"
  - description: REDIS_CPU_LIMIT_CORES
    name: REDIS_CPU_LIMIT
    required: true
    value: "8"
  - description: REDIS_MEM_LIMIT_GB
    name: REDIS_MEM_LIMIT
    required: true
    value: "64"
  - description: REDIS_MAX_HEAP_GB
    name: MAX_HEAP
    required: true
    value: "8"
  - description: REDIS_initial-heap_GB
    name: INITIAL_HEAP
    required: true
    value: "8"
  - description: REDIS_off-heap-memory-size_GB
    name: OFF_HEAP_MEMORY_SIZE
    required: true
    value: "52"
  - description: LOCATOR_persistentVolume_size_Gi
    name: LOCATOR_PV_SIZE
    required: true
    value: "1"
  - description: persistentVolume_size_Gi
    name: PV_SIZE
    required: true
    value: "64"
  - description: REGION_TYPE
    name: REGION_TYPE
    required: true
    value: REPLICATE_PERSISTENT
- apiVersion: v1
  kind: Template
  labels:
    createdBy: registry-console-template
  metadata:
    annotations:
      description: Template for deploying registry web console. Requires cluster-admin.
      tags: infrastructure
    creationTimestamp: 2017-08-23T07:04:18Z
    name: registry-console
    namespace: openshift
    resourceVersion: "876"
    selfLink: /oapi/v1/namespaces/openshift/templates/registry-console
    uid: 4855a6fa-87d1-11e7-9a67-ecf4bbe2f144
  objects:
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      labels:
        name: registry-console
      name: registry-console
    spec:
      replicas: 1
      selector:
        name: registry-console
      template:
        metadata:
          labels:
            name: registry-console
        spec:
          containers:
          - env:
            - name: OPENSHIFT_OAUTH_PROVIDER_URL
              value: ${OPENSHIFT_OAUTH_PROVIDER_URL}
            - name: OPENSHIFT_OAUTH_CLIENT_ID
              value: ${OPENSHIFT_OAUTH_CLIENT_ID}
            - name: KUBERNETES_INSECURE
              value: "false"
            - name: COCKPIT_KUBE_INSECURE
              value: "false"
            - name: REGISTRY_ONLY
              value: "true"
            - name: REGISTRY_HOST
              value: ${REGISTRY_HOST}
            image: ${IMAGE_NAME}:${IMAGE_VERSION}
            livenessProbe:
              failureThreshold: 3
              httpGet:
                path: /ping
                port: 9090
                scheme: HTTP
              initialDelaySeconds: 10
              periodSeconds: 10
              successThreshold: 1
              timeoutSeconds: 5
            name: registry-console
            ports:
            - containerPort: 9090
              protocol: TCP
            readinessProbe:
              failureThreshold: 3
              httpGet:
                path: /ping
                port: 9090
                scheme: HTTP
              periodSeconds: 10
              successThreshold: 1
              timeoutSeconds: 5
      triggers:
      - type: ConfigChange
  - apiVersion: v1
    kind: Service
    metadata:
      labels:
        name: registry-console
      name: registry-console
    spec:
      ports:
      - name: registry-console
        port: 9000
        protocol: TCP
        targetPort: 9090
      selector:
        name: registry-console
      type: ClusterIP
  - apiVersion: v1
    kind: ImageStream
    metadata:
      annotations:
        description: Atomic Registry console
      name: registry-console
    spec:
      tags:
      - annotations: null
        from:
          kind: DockerImage
          name: ${IMAGE_NAME}
        name: ${IMAGE_VERSION}
  - apiVersion: v1
    kind: OAuthClient
    metadata:
      name: ${OPENSHIFT_OAUTH_CLIENT_ID}
      respondWithChallenges: false
    redirectURIs:
    - ${COCKPIT_KUBE_URL}
    secret: ${OPENSHIFT_OAUTH_CLIENT_SECRET}
  parameters:
  - description: Container image name
    name: IMAGE_NAME
    value: cockpit/kubernetes
  - description: Specify image version; e.g. for "cockpit/kubernetes:latest", set
      version "latest"
    name: IMAGE_VERSION
    value: latest
  - description: The public URL for the Openshift OAuth Provider, e.g. https://openshift.example.com:8443
    name: OPENSHIFT_OAUTH_PROVIDER_URL
    required: true
  - description: The registry console URL. This should be created beforehand using
      'oc create route passthrough --service registry-console --port registry-console
      -n default', e.g. https://registry-console-default.example.com
    name: COCKPIT_KUBE_URL
    required: true
  - description: Oauth client secret
    from: user[a-zA-Z0-9]{64}
    generate: expression
    name: OPENSHIFT_OAUTH_CLIENT_SECRET
  - description: Oauth client id
    name: OPENSHIFT_OAUTH_CLIENT_ID
    value: cockpit-oauth-client
  - description: The integrated registry hostname exposed via route, e.g. registry.example.com
    name: REGISTRY_HOST
    required: true
- apiVersion: v1
  kind: Template
  labels:
    app: rocketmqcluster
    template: rocketmq-cluster-persistent
  metadata:
    annotations:
      description: Create a rocketmq cluster, with persistent storage,By two master
      iconClass: icon-database
      openshift.io/display-name: RocketmqCluster (Persistent)
      tags: database,rocketmqcluster
    creationTimestamp: 2017-12-26T09:09:04Z
    name: rocketmqcluster-persistent
    namespace: openshift
    resourceVersion: "29619788"
    selfLink: /oapi/v1/namespaces/openshift/templates/rocketmqcluster-persistent
    uid: 6baa2d76-ea1c-11e7-86b8-364284a56e59
  objects:
  - apiVersion: v1
    kind: Service
    metadata:
      labels:
        name: nameserver
      name: nameserver
    spec:
      clusterIP: None
      ports:
      - name: 9876-tcp
        port: 9876
        protocol: TCP
        targetPort: 9876
      - name: 10911-tcp
        port: 10911
        protocol: TCP
        targetPort: 10911
      - name: 10912-tcp
        port: 10912
        protocol: TCP
        targetPort: 10912
      selector:
        name: nameserver
  - apiVersion: v1
    kind: Service
    metadata:
      labels:
        name: ${NAME}
      name: broker
    spec:
      clusterIP: None
      ports:
      - name: 10909-tcp
        port: 10909
        protocol: TCP
        targetPort: 10909
      - name: 10911-tcp
        port: 10911
        protocol: TCP
        targetPort: 10911
      - name: 10912-tcp
        port: 10912
        protocol: TCP
        targetPort: 10912
      selector:
        name: ${NAME}
  - apiVersion: apps/v1beta1
    kind: StatefulSet
    metadata:
      labels:
        name: nameserver
      name: nameserver
    spec:
      replicas: 2
      serviceName: nameserver
      template:
        metadata:
          labels:
            name: nameserver
        spec:
          containers:
          - command:
            - /bin/sh
            - -ecx
            - |-
              export JAVA_OPT=" -Duser.home=/opt/mqdata"
              sed -i "s/-Xms4g -Xmx4g -Xmn2g/$NAMESERVER_JAVA_OPT/g" $ROCKETMQ_HOME/bin/runserver.sh

              cd $ROCKETMQ_HOME/bin && sh mqnamesrv
            env:
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            - name: HOSTNAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
            - name: NAMESERVER_JAVA_OPT
              value: ${NAMESERVER_JAVA_OPT}
            - name: NAMESRV_ADDR
              value: nameserver-0.nameserver:9876;nameserver-1.nameserver:9876
            image: ${NAMESERVER_IMAGE}
            imagePullPolicy: Always
            livenessProbe:
              failureThreshold: 3
              initialDelaySeconds: 30
              periodSeconds: 10
              successThreshold: 1
              tcpSocket:
                port: 9876
              timeoutSeconds: 5
            name: locator
            ports:
            - containerPort: 9876
              name: nameserver
            - containerPort: 10911
              name: broker
            - containerPort: 10912
              name: brokerreplica
            resources:
              limits:
                cpu: ${NAMESERVER_CPU_LIMIT}
                memory: ${NAMESERVER_MEM_LIMIT}G
              requests:
                cpu: 200m
                memory: 200M
            securityContext:
              privileged: false
              runAsUser: 0
          terminationGracePeriodSeconds: 10
  - apiVersion: apps/v1beta1
    kind: StatefulSet
    metadata:
      labels:
        name: ${NAME}
      name: ${NAME}
    spec:
      replicas: ${REPLICAS}
      serviceName: broker
      template:
        metadata:
          labels:
            name: ${NAME}
        spec:
          affinity:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: name
                      operator: In
                      values:
                      - ${NAME}
                  topologyKey: kubernetes.io/hostname
                weight: 100
          containers:
          - command:
            - /bin/sh
            - -ecx
            - |-
              while true; do
               echo "Waiting for nameserver-0.nameserver to come up"
               ping -W 1 -c 1 nameserver-0.nameserver > /dev/null && break
               sleep 1s
              done
              export JAVA_OPT=" -Duser.home=/opt/mqdata"
              sed -i "s/-Xms8g -Xmx8g -Xmn4g/$BROKER_JAVA_OPT/g" $ROCKETMQ_HOME/bin/runbroker.sh

              cd $ROCKETMQ_HOME/bin && sh mqbroker
            env:
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: HOSTNAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: BROKER_JAVA_OPT
              value: ${BROKER_JAVA_OPT}
            - name: NAMESRV_ADDR
              value: nameserver-0.nameserver:9876;nameserver-1.nameserver:9876
            image: ${BROKERSERVER_IMAGE}
            imagePullPolicy: Always
            livenessProbe:
              failureThreshold: 3
              initialDelaySeconds: 40
              periodSeconds: 10
              successThreshold: 1
              tcpSocket:
                port: 10912
              timeoutSeconds: 3
            name: ${NAME}
            ports:
            - containerPort: 10909
              name: nameserver
            - containerPort: 10911
              name: broker
            - containerPort: 10912
              name: brokerreplica
            resources:
              limits:
                cpu: ${BROKER_CPU_LIMIT}
                memory: ${BROKER_MEM_LIMIT}G
              requests:
                cpu: 200m
                memory: 200M
            securityContext:
              privileged: false
              runAsUser: 0
            volumeMounts:
            - mountPath: /opt/mqdata
              name: mqdata
          terminationGracePeriodSeconds: 10
      volumeClaimTemplates:
      - metadata:
          annotations:
            volume.alpha.kubernetes.io/storage-class: slow
          name: mqdata
        spec:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: ${PV_SIZE}Gi
  parameters:
  - description: Name of the NAMESERVER_IMAGE
    name: NAMESERVER_IMAGE
    required: true
    value: rocketmq-namesrv:4.2.0
  - description: Name of the BROKERSERVER_IMAGE
    name: BROKERSERVER_IMAGE
    required: true
    value: rocketmq-broker:4.2.0
  - description: broker name used as a service name
    name: NAME
    value: rocketmqbroker
  - description: Number of Broker Master Server replicas.
    name: REPLICAS
    required: true
    value: "2"
  - description: NAMESERVER_CPU_LIMIT_CORES
    name: NAMESERVER_CPU_LIMIT
    required: true
    value: "2"
  - description: NAMESERVER_MEM_LIMIT_GB
    name: NAMESERVER_MEM_LIMIT
    required: true
    value: "4"
  - description: BROKER_CPU_LIMIT_CORES
    name: BROKER_CPU_LIMIT
    required: true
    value: "8"
  - description: BROKER_MEM_LIMIT_GB
    name: BROKER_MEM_LIMIT
    required: true
    value: "16"
  - description: Java Memory Setting for NameServer
    displayName: Java OPT for NameServer
    name: NAMESERVER_JAVA_OPT
    required: true
    value: -Xms3g -Xmx3g -Xmn2g
  - description: Java Memory Setting for BROKER
    displayName: Java OPT for Broker
    name: BROKER_JAVA_OPT
    required: true
    value: -Xms12g -Xmx12g -Xmn6g
  - description: BROKER_persistentVolume_size_Gi
    name: PV_SIZE
    required: true
    value: "32"
- apiVersion: v1
  kind: Template
  labels:
    app: sparkcluster
    template: sparkcluster-persistent
  metadata:
    annotations:
      description: Create a Spark cluster, with persistent storage.
      iconClass: icon-database
      openshift.io/display-name: SparkCluster (Persistent)
      tags: computing,spark
    creationTimestamp: 2017-09-21T08:02:20Z
    name: sparkcluster-persistent
    namespace: openshift
    resourceVersion: "27449625"
    selfLink: /oapi/v1/namespaces/openshift/templates/sparkcluster-persistent
    uid: 31402a14-9ea3-11e7-b83c-ecf4bbe2f144
  objects:
  - apiVersion: v1
    data:
      log4j.properties: |-
        #
        # Licensed to the Apache Software Foundation (ASF) under one or more
        # contributor license agreements.  See the NOTICE file distributed with
        # this work for additional information regarding copyright ownership.
        # The ASF licenses this file to You under the Apache License, Version 2.0
        # (the "License"); you may not use this file except in compliance with
        # the License.  You may obtain a copy of the License at
        #
        #    http://www.apache.org/licenses/LICENSE-2.0
        #
        # Unless required by applicable law or agreed to in writing, software
        # distributed under the License is distributed on an "AS IS" BASIS,
        # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
        # See the License for the specific language governing permissions and
        # limitations under the License.
        #

        # Set everything to be logged to the console
        log4j.rootCategory=INFO, console
        log4j.appender.console=org.apache.log4j.ConsoleAppender
        log4j.appender.console.target=System.err
        log4j.appender.console.layout=org.apache.log4j.PatternLayout
        log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n

        # Set the default spark-shell log level to WARN. When running the spark-shell, the
        # log level for this class is used to overwrite the root logger's log level, so that
        # the user can have different defaults for the shell and regular Spark apps.
        log4j.logger.org.apache.spark.repl.Main=WARN

        # Settings to quiet third party logs that are too verbose
        log4j.logger.org.spark_project.jetty=WARN
        log4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle=ERROR
        log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO
        log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO
        log4j.logger.org.apache.parquet=ERROR
        log4j.logger.parquet=ERROR

        # SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive support
        log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler=FATAL
        log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry=ERROR
      log4j.properties.template: |-
        #
        # Licensed to the Apache Software Foundation (ASF) under one or more
        # contributor license agreements.  See the NOTICE file distributed with
        # this work for additional information regarding copyright ownership.
        # The ASF licenses this file to You under the Apache License, Version 2.0
        # (the "License"); you may not use this file except in compliance with
        # the License.  You may obtain a copy of the License at
        #
        #    http://www.apache.org/licenses/LICENSE-2.0
        #
        # Unless required by applicable law or agreed to in writing, software
        # distributed under the License is distributed on an "AS IS" BASIS,
        # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
        # See the License for the specific language governing permissions and
        # limitations under the License.
        #

        # Set everything to be logged to the console
        log4j.rootCategory=INFO, console
        log4j.appender.console=org.apache.log4j.ConsoleAppender
        log4j.appender.console.target=System.err
        log4j.appender.console.layout=org.apache.log4j.PatternLayout
        log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n

        # Set the default spark-shell log level to WARN. When running the spark-shell, the
        # log level for this class is used to overwrite the root logger's log level, so that
        # the user can have different defaults for the shell and regular Spark apps.
        log4j.logger.org.apache.spark.repl.Main=WARN

        # Settings to quiet third party logs that are too verbose
        log4j.logger.org.spark_project.jetty=WARN
        log4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle=ERROR
        log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO
        log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO
        log4j.logger.org.apache.parquet=ERROR
        log4j.logger.parquet=ERROR

        # SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive support
        log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler=FATAL
        log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry=ERROR
      metrics.properties: |-
        *.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink
        master.source.jvm.class=org.apache.spark.metrics.source.JvmSource
        worker.source.jvm.class=org.apache.spark.metrics.source.JvmSource
        driver.source.jvm.class=org.apache.spark.metrics.source.JvmSource
        executor.source.jvm.class=org.apache.spark.metrics.source.JvmSource
      metrics.properties.template: |-
        #
        # Licensed to the Apache Software Foundation (ASF) under one or more
        # contributor license agreements.  See the NOTICE file distributed with
        # this work for additional information regarding copyright ownership.
        # The ASF licenses this file to You under the Apache License, Version 2.0
        # (the "License"); you may not use this file except in compliance with
        # the License.  You may obtain a copy of the License at
        #
        #    http://www.apache.org/licenses/LICENSE-2.0
        #
        # Unless required by applicable law or agreed to in writing, software
        # distributed under the License is distributed on an "AS IS" BASIS,
        # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
        # See the License for the specific language governing permissions and
        # limitations under the License.
        #

        #  syntax: [instance].sink|source.[name].[options]=[value]

        #  This file configures Spark's internal metrics system. The metrics system is
        #  divided into instances which correspond to internal components.
        #  Each instance can be configured to report its metrics to one or more sinks.
        #  Accepted values for [instance] are "master", "worker", "executor", "driver",
        #  and "applications". A wildcard "*" can be used as an instance name, in
        #  which case all instances will inherit the supplied property.
        #
        #  Within an instance, a "source" specifies a particular set of grouped metrics.
        #  there are two kinds of sources:
        #    1. Spark internal sources, like MasterSource, WorkerSource, etc, which will
        #    collect a Spark component's internal state. Each instance is paired with a
        #    Spark source that is added automatically.
        #    2. Common sources, like JvmSource, which will collect low level state.
        #    These can be added through configuration options and are then loaded
        #    using reflection.
        #
        #  A "sink" specifies where metrics are delivered to. Each instance can be
        #  assigned one or more sinks.
        #
        #  The sink|source field specifies whether the property relates to a sink or
        #  source.
        #
        #  The [name] field specifies the name of source or sink.
        #
        #  The [options] field is the specific property of this source or sink. The
        #  source or sink is responsible for parsing this property.
        #
        #  Notes:
        #    1. To add a new sink, set the "class" option to a fully qualified class
        #    name (see examples below).
        #    2. Some sinks involve a polling period. The minimum allowed polling period
        #    is 1 second.
        #    3. Wildcard properties can be overridden by more specific properties.
        #    For example, master.sink.console.period takes precedence over
        #    *.sink.console.period.
        #    4. A metrics specific configuration
        #    "spark.metrics.conf=${SPARK_HOME}/conf/metrics.properties" should be
        #    added to Java properties using -Dspark.metrics.conf=xxx if you want to
        #    customize metrics system. You can also put the file in ${SPARK_HOME}/conf
        #    and it will be loaded automatically.
        #    5. The MetricsServlet sink is added by default as a sink in the master,
        #    worker and driver, and you can send HTTP requests to the "/metrics/json"
        #    endpoint to get a snapshot of all the registered metrics in JSON format.
        #    For master, requests to the "/metrics/master/json" and
        #    "/metrics/applications/json" endpoints can be sent separately to get
        #    metrics snapshots of the master instance and applications. This
        #    MetricsServlet does not have to be configured.

        ## List of available common sources and their properties.

        # org.apache.spark.metrics.source.JvmSource
        #   Note: Currently, JvmSource is the only available common source.
        #         It can be added to an instance by setting the "class" option to its
        #         fully qualified class name (see examples below).

        ## List of available sinks and their properties.

        # org.apache.spark.metrics.sink.ConsoleSink
        #   Name:   Default:   Description:
        #   period  10         Poll period
        #   unit    seconds    Unit of the poll period

        # org.apache.spark.metrics.sink.CSVSink
        #   Name:     Default:   Description:
        #   period    10         Poll period
        #   unit      seconds    Unit of the poll period
        #   directory /tmp       Where to store CSV files

        # org.apache.spark.metrics.sink.GangliaSink
        #   Name:     Default:   Description:
        #   host      NONE       Hostname or multicast group of the Ganglia server,
        #                        must be set
        #   port      NONE       Port of the Ganglia server(s), must be set
        #   period    10         Poll period
        #   unit      seconds    Unit of the poll period
        #   ttl       1          TTL of messages sent by Ganglia
        #   dmax      0          Lifetime in seconds of metrics (0 never expired)
        #   mode      multicast  Ganglia network mode ('unicast' or 'multicast')

        # org.apache.spark.metrics.sink.JmxSink

        # org.apache.spark.metrics.sink.MetricsServlet
        #   Name:     Default:   Description:
        #   path      VARIES*    Path prefix from the web server root
        #   sample    false      Whether to show entire set of samples for histograms
        #                        ('false' or 'true')
        #
        # * Default path is /metrics/json for all instances except the master. The
        #   master has two paths:
        #     /metrics/applications/json # App information
        #     /metrics/master/json       # Master information

        # org.apache.spark.metrics.sink.GraphiteSink
        #   Name:     Default:      Description:
        #   host      NONE          Hostname of the Graphite server, must be set
        #   port      NONE          Port of the Graphite server, must be set
        #   period    10            Poll period
        #   unit      seconds       Unit of the poll period
        #   prefix    EMPTY STRING  Prefix to prepend to every metric's name
        #   protocol  tcp           Protocol ("tcp" or "udp") to use

        ## Examples
        # Enable JmxSink for all instances by class name
        #*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink

        # Enable ConsoleSink for all instances by class name
        #*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink

        # Polling period for the ConsoleSink
        #*.sink.console.period=10
        # Unit of the polling period for the ConsoleSink
        #*.sink.console.unit=seconds

        # Polling period for the ConsoleSink specific for the master instance
        #master.sink.console.period=15
        # Unit of the polling period for the ConsoleSink specific for the master
        # instance
        #master.sink.console.unit=seconds

        # Enable CsvSink for all instances by class name
        #*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink

        # Polling period for the CsvSink
        #*.sink.csv.period=1
        # Unit of the polling period for the CsvSink
        #*.sink.csv.unit=minutes

        # Polling directory for CsvSink
        #*.sink.csv.directory=/tmp/

        # Polling period for the CsvSink specific for the worker instance
        #worker.sink.csv.period=10
        # Unit of the polling period for the CsvSink specific for the worker instance
        #worker.sink.csv.unit=minutes

        # Enable Slf4jSink for all instances by class name
        #*.sink.slf4j.class=org.apache.spark.metrics.sink.Slf4jSink

        # Polling period for the Slf4JSink
        #*.sink.slf4j.period=1
        # Unit of the polling period for the Slf4jSink
        #*.sink.slf4j.unit=minutes

        # Enable JvmSource for instance master, worker, driver and executor
        #master.source.jvm.class=org.apache.spark.metrics.source.JvmSource

        #worker.source.jvm.class=org.apache.spark.metrics.source.JvmSource

        #driver.source.jvm.class=org.apache.spark.metrics.source.JvmSource

        #executor.source.jvm.class=org.apache.spark.metrics.source.JvmSource
      slaves.template: |-
        #
        # Licensed to the Apache Software Foundation (ASF) under one or more
        # contributor license agreements.  See the NOTICE file distributed with
        # this work for additional information regarding copyright ownership.
        # The ASF licenses this file to You under the Apache License, Version 2.0
        # (the "License"); you may not use this file except in compliance with
        # the License.  You may obtain a copy of the License at
        #
        #    http://www.apache.org/licenses/LICENSE-2.0
        #
        # Unless required by applicable law or agreed to in writing, software
        # distributed under the License is distributed on an "AS IS" BASIS,
        # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
        # See the License for the specific language governing permissions and
        # limitations under the License.
        #

        # A Spark Worker will be started on each of the machines listed below.
        localhost
      spark-defaults.conf: |-
        #
        # Licensed to the Apache Software Foundation (ASF) under one or more
        # contributor license agreements.  See the NOTICE file distributed with
        # this work for additional information regarding copyright ownership.
        # The ASF licenses this file to You under the Apache License, Version 2.0
        # (the "License"); you may not use this file except in compliance with
        # the License.  You may obtain a copy of the License at
        #
        #    http://www.apache.org/licenses/LICENSE-2.0
        #
        # Unless required by applicable law or agreed to in writing, software
        # distributed under the License is distributed on an "AS IS" BASIS,
        # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
        # See the License for the specific language governing permissions and
        # limitations under the License.
        #

        # Default system properties included when running spark-submit.
        # This is useful for setting default environmental settings.

        # Example:
        # spark.master                     spark://master:7077
        # spark.eventLog.enabled           true
        # spark.eventLog.dir               hdfs://namenode:8021/directory
        # spark.serializer                 org.apache.spark.serializer.KryoSerializer
        # spark.driver.memory              5g
        # spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"
        #spark.ui.reverseProxy              true
        #spark.ui.reverseProxyUrl           /
        spark.driver.extraClassPath /opt/spark/jars/alluxio-1.5.0-spark-client.jar
        spark.executor.extraClassPath /opt/spark/jars/alluxio-1.5.0-spark-client.jar
      spark-defaults.conf.template: |-
        #
        # Licensed to the Apache Software Foundation (ASF) under one or more
        # contributor license agreements.  See the NOTICE file distributed with
        # this work for additional information regarding copyright ownership.
        # The ASF licenses this file to You under the Apache License, Version 2.0
        # (the "License"); you may not use this file except in compliance with
        # the License.  You may obtain a copy of the License at
        #
        #    http://www.apache.org/licenses/LICENSE-2.0
        #
        # Unless required by applicable law or agreed to in writing, software
        # distributed under the License is distributed on an "AS IS" BASIS,
        # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
        # See the License for the specific language governing permissions and
        # limitations under the License.
        #

        # Default system properties included when running spark-submit.
        # This is useful for setting default environmental settings.

        # Example:
        # spark.master                     spark://master:7077
        # spark.eventLog.enabled           true
        # spark.eventLog.dir               hdfs://namenode:8021/directory
        # spark.serializer                 org.apache.spark.serializer.KryoSerializer
        # spark.driver.memory              5g
        # spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"
      spark-env.sh: |-
        #!/usr/bin/env bash

        #
        # Licensed to the Apache Software Foundation (ASF) under one or more
        # contributor license agreements.  See the NOTICE file distributed with
        # this work for additional information regarding copyright ownership.
        # The ASF licenses this file to You under the Apache License, Version 2.0
        # (the "License"); you may not use this file except in compliance with
        # the License.  You may obtain a copy of the License at
        #
        #    http://www.apache.org/licenses/LICENSE-2.0
        #
        # Unless required by applicable law or agreed to in writing, software
        # distributed under the License is distributed on an "AS IS" BASIS,
        # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
        # See the License for the specific language governing permissions and
        # limitations under the License.
        #

        # This file is sourced when running various Spark programs.
        # Copy it as spark-env.sh and edit that to configure Spark for your site.

        # Options read when launching programs locally with
        # ./bin/run-example or ./bin/spark-submit
        # - HADOOP_CONF_DIR, to point Spark towards Hadoop configuration files
        # - SPARK_LOCAL_IP, to set the IP address Spark binds to on this node
        # - SPARK_PUBLIC_DNS, to set the public dns name of the driver program
        # - SPARK_CLASSPATH, default classpath entries to append

        # Options read by executors and drivers running inside the cluster
        # - SPARK_LOCAL_IP, to set the IP address Spark binds to on this node
        # - SPARK_PUBLIC_DNS, to set the public DNS name of the driver program
        # - SPARK_CLASSPATH, default classpath entries to append
        # - SPARK_LOCAL_DIRS, storage directories to use on this node for shuffle and RDD data
        # - MESOS_NATIVE_JAVA_LIBRARY, to point to your libmesos.so if you use Mesos

        # Options read in YARN client mode
        # - HADOOP_CONF_DIR, to point Spark towards Hadoop configuration files
        # - SPARK_EXECUTOR_INSTANCES, Number of executors to start (Default: 2)
        # - SPARK_EXECUTOR_CORES, Number of cores for the executors (Default: 1).
        # - SPARK_EXECUTOR_MEMORY, Memory per Executor (e.g. 1000M, 2G) (Default: 1G)
        # - SPARK_DRIVER_MEMORY, Memory for Driver (e.g. 1000M, 2G) (Default: 1G)

        # Options for the daemons used in the standalone deploy mode
        # - SPARK_MASTER_HOST, to bind the master to a different IP address or hostname
        # - SPARK_MASTER_PORT / SPARK_MASTER_WEBUI_PORT, to use non-default ports for the master
        # - SPARK_MASTER_OPTS, to set config properties only for the master (e.g. "-Dx=y")
        # - SPARK_WORKER_CORES, to set the number of cores to use on this machine
        # - SPARK_WORKER_MEMORY, to set how much total memory workers have to give executors (e.g. 1000m, 2g)
        # - SPARK_WORKER_PORT / SPARK_WORKER_WEBUI_PORT, to use non-default ports for the worker
        # - SPARK_WORKER_INSTANCES, to set the number of worker processes per node
        # - SPARK_WORKER_DIR, to set the working directory of worker processes
        # - SPARK_WORKER_OPTS, to set config properties only for the worker (e.g. "-Dx=y")
        # - SPARK_DAEMON_MEMORY, to allocate to the master, worker and history server themselves (default: 1g).
        # - SPARK_HISTORY_OPTS, to set config properties only for the history server (e.g. "-Dx=y")
        # - SPARK_SHUFFLE_OPTS, to set config properties only for the external shuffle service (e.g. "-Dx=y")
        # - SPARK_DAEMON_JAVA_OPTS, to set config properties for all daemons (e.g. "-Dx=y")
        # - SPARK_PUBLIC_DNS, to set the public dns name of the master or workers

        # Generic options for the daemons used in the standalone deploy mode
        # - SPARK_CONF_DIR      Alternate conf dir. (Default: ${SPARK_HOME}/conf)
        # - SPARK_LOG_DIR       Where log files are stored.  (Default: ${SPARK_HOME}/logs)
        # - SPARK_PID_DIR       Where the pid file is stored. (Default: /tmp)
        # - SPARK_IDENT_STRING  A string representing this instance of spark. (Default: $USER)
        # - SPARK_NICENESS      The scheduling priority for daemons. (Default: 0)
        # - SPARK_NO_DAEMONIZE  Run the proposed command in the foreground. It will not output a PID file.
      spark-env.sh.template: |-
        #!/usr/bin/env bash

        #
        # Licensed to the Apache Software Foundation (ASF) under one or more
        # contributor license agreements.  See the NOTICE file distributed with
        # this work for additional information regarding copyright ownership.
        # The ASF licenses this file to You under the Apache License, Version 2.0
        # (the "License"); you may not use this file except in compliance with
        # the License.  You may obtain a copy of the License at
        #
        #    http://www.apache.org/licenses/LICENSE-2.0
        #
        # Unless required by applicable law or agreed to in writing, software
        # distributed under the License is distributed on an "AS IS" BASIS,
        # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
        # See the License for the specific language governing permissions and
        # limitations under the License.
        #

        # This file is sourced when running various Spark programs.
        # Copy it as spark-env.sh and edit that to configure Spark for your site.

        # Options read when launching programs locally with
        # ./bin/run-example or ./bin/spark-submit
        # - HADOOP_CONF_DIR, to point Spark towards Hadoop configuration files
        # - SPARK_LOCAL_IP, to set the IP address Spark binds to on this node
        # - SPARK_PUBLIC_DNS, to set the public dns name of the driver program
        # - SPARK_CLASSPATH, default classpath entries to append

        # Options read by executors and drivers running inside the cluster
        # - SPARK_LOCAL_IP, to set the IP address Spark binds to on this node
        # - SPARK_PUBLIC_DNS, to set the public DNS name of the driver program
        # - SPARK_CLASSPATH, default classpath entries to append
        # - SPARK_LOCAL_DIRS, storage directories to use on this node for shuffle and RDD data
        # - MESOS_NATIVE_JAVA_LIBRARY, to point to your libmesos.so if you use Mesos

        # Options read in YARN client mode
        # - HADOOP_CONF_DIR, to point Spark towards Hadoop configuration files
        # - SPARK_EXECUTOR_INSTANCES, Number of executors to start (Default: 2)
        # - SPARK_EXECUTOR_CORES, Number of cores for the executors (Default: 1).
        # - SPARK_EXECUTOR_MEMORY, Memory per Executor (e.g. 1000M, 2G) (Default: 1G)
        # - SPARK_DRIVER_MEMORY, Memory for Driver (e.g. 1000M, 2G) (Default: 1G)

        # Options for the daemons used in the standalone deploy mode
        # - SPARK_MASTER_HOST, to bind the master to a different IP address or hostname
        # - SPARK_MASTER_PORT / SPARK_MASTER_WEBUI_PORT, to use non-default ports for the master
        # - SPARK_MASTER_OPTS, to set config properties only for the master (e.g. "-Dx=y")
        # - SPARK_WORKER_CORES, to set the number of cores to use on this machine
        # - SPARK_WORKER_MEMORY, to set how much total memory workers have to give executors (e.g. 1000m, 2g)
        # - SPARK_WORKER_PORT / SPARK_WORKER_WEBUI_PORT, to use non-default ports for the worker
        # - SPARK_WORKER_INSTANCES, to set the number of worker processes per node
        # - SPARK_WORKER_DIR, to set the working directory of worker processes
        # - SPARK_WORKER_OPTS, to set config properties only for the worker (e.g. "-Dx=y")
        # - SPARK_DAEMON_MEMORY, to allocate to the master, worker and history server themselves (default: 1g).
        # - SPARK_HISTORY_OPTS, to set config properties only for the history server (e.g. "-Dx=y")
        # - SPARK_SHUFFLE_OPTS, to set config properties only for the external shuffle service (e.g. "-Dx=y")
        # - SPARK_DAEMON_JAVA_OPTS, to set config properties for all daemons (e.g. "-Dx=y")
        # - SPARK_PUBLIC_DNS, to set the public dns name of the master or workers

        # Generic options for the daemons used in the standalone deploy mode
        # - SPARK_CONF_DIR      Alternate conf dir. (Default: ${SPARK_HOME}/conf)
        # - SPARK_LOG_DIR       Where log files are stored.  (Default: ${SPARK_HOME}/logs)
        # - SPARK_PID_DIR       Where the pid file is stored. (Default: /tmp)
        # - SPARK_IDENT_STRING  A string representing this instance of spark. (Default: $USER)
        # - SPARK_NICENESS      The scheduling priority for daemons. (Default: 0)
        # - SPARK_NO_DAEMONIZE  Run the proposed command in the foreground. It will not output a PID file.
    kind: ConfigMap
    metadata:
      name: spark-master-config
  - apiVersion: v1
    data:
      log4j.properties: |-
        #
        # Licensed to the Apache Software Foundation (ASF) under one or more
        # contributor license agreements.  See the NOTICE file distributed with
        # this work for additional information regarding copyright ownership.
        # The ASF licenses this file to You under the Apache License, Version 2.0
        # (the "License"); you may not use this file except in compliance with
        # the License.  You may obtain a copy of the License at
        #
        #    http://www.apache.org/licenses/LICENSE-2.0
        #
        # Unless required by applicable law or agreed to in writing, software
        # distributed under the License is distributed on an "AS IS" BASIS,
        # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
        # See the License for the specific language governing permissions and
        # limitations under the License.
        #

        # Set everything to be logged to the console
        log4j.rootCategory=INFO, console
        log4j.appender.console=org.apache.log4j.ConsoleAppender
        log4j.appender.console.target=System.err
        log4j.appender.console.layout=org.apache.log4j.PatternLayout
        log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n

        # Set the default spark-shell log level to WARN. When running the spark-shell, the
        # log level for this class is used to overwrite the root logger's log level, so that
        # the user can have different defaults for the shell and regular Spark apps.
        log4j.logger.org.apache.spark.repl.Main=WARN

        # Settings to quiet third party logs that are too verbose
        log4j.logger.org.spark_project.jetty=WARN
        log4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle=ERROR
        log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO
        log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO
        log4j.logger.org.apache.parquet=ERROR
        log4j.logger.parquet=ERROR

        # SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive support
        log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler=FATAL
        log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry=ERROR
      log4j.properties.template: |-
        #
        # Licensed to the Apache Software Foundation (ASF) under one or more
        # contributor license agreements.  See the NOTICE file distributed with
        # this work for additional information regarding copyright ownership.
        # The ASF licenses this file to You under the Apache License, Version 2.0
        # (the "License"); you may not use this file except in compliance with
        # the License.  You may obtain a copy of the License at
        #
        #    http://www.apache.org/licenses/LICENSE-2.0
        #
        # Unless required by applicable law or agreed to in writing, software
        # distributed under the License is distributed on an "AS IS" BASIS,
        # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
        # See the License for the specific language governing permissions and
        # limitations under the License.
        #

        # Set everything to be logged to the console
        log4j.rootCategory=INFO, console
        log4j.appender.console=org.apache.log4j.ConsoleAppender
        log4j.appender.console.target=System.err
        log4j.appender.console.layout=org.apache.log4j.PatternLayout
        log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n

        # Set the default spark-shell log level to WARN. When running the spark-shell, the
        # log level for this class is used to overwrite the root logger's log level, so that
        # the user can have different defaults for the shell and regular Spark apps.
        log4j.logger.org.apache.spark.repl.Main=WARN

        # Settings to quiet third party logs that are too verbose
        log4j.logger.org.spark_project.jetty=WARN
        log4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle=ERROR
        log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO
        log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO
        log4j.logger.org.apache.parquet=ERROR
        log4j.logger.parquet=ERROR

        # SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive support
        log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler=FATAL
        log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry=ERROR
      metrics.properties: |-
        *.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink
        master.source.jvm.class=org.apache.spark.metrics.source.JvmSource
        worker.source.jvm.class=org.apache.spark.metrics.source.JvmSource
        driver.source.jvm.class=org.apache.spark.metrics.source.JvmSource
        executor.source.jvm.class=org.apache.spark.metrics.source.JvmSource
      metrics.properties.template: |-
        #
        # Licensed to the Apache Software Foundation (ASF) under one or more
        # contributor license agreements.  See the NOTICE file distributed with
        # this work for additional information regarding copyright ownership.
        # The ASF licenses this file to You under the Apache License, Version 2.0
        # (the "License"); you may not use this file except in compliance with
        # the License.  You may obtain a copy of the License at
        #
        #    http://www.apache.org/licenses/LICENSE-2.0
        #
        # Unless required by applicable law or agreed to in writing, software
        # distributed under the License is distributed on an "AS IS" BASIS,
        # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
        # See the License for the specific language governing permissions and
        # limitations under the License.
        #

        #  syntax: [instance].sink|source.[name].[options]=[value]

        #  This file configures Spark's internal metrics system. The metrics system is
        #  divided into instances which correspond to internal components.
        #  Each instance can be configured to report its metrics to one or more sinks.
        #  Accepted values for [instance] are "master", "worker", "executor", "driver",
        #  and "applications". A wildcard "*" can be used as an instance name, in
        #  which case all instances will inherit the supplied property.
        #
        #  Within an instance, a "source" specifies a particular set of grouped metrics.
        #  there are two kinds of sources:
        #    1. Spark internal sources, like MasterSource, WorkerSource, etc, which will
        #    collect a Spark component's internal state. Each instance is paired with a
        #    Spark source that is added automatically.
        #    2. Common sources, like JvmSource, which will collect low level state.
        #    These can be added through configuration options and are then loaded
        #    using reflection.
        #
        #  A "sink" specifies where metrics are delivered to. Each instance can be
        #  assigned one or more sinks.
        #
        #  The sink|source field specifies whether the property relates to a sink or
        #  source.
        #
        #  The [name] field specifies the name of source or sink.
        #
        #  The [options] field is the specific property of this source or sink. The
        #  source or sink is responsible for parsing this property.
        #
        #  Notes:
        #    1. To add a new sink, set the "class" option to a fully qualified class
        #    name (see examples below).
        #    2. Some sinks involve a polling period. The minimum allowed polling period
        #    is 1 second.
        #    3. Wildcard properties can be overridden by more specific properties.
        #    For example, master.sink.console.period takes precedence over
        #    *.sink.console.period.
        #    4. A metrics specific configuration
        #    "spark.metrics.conf=${SPARK_HOME}/conf/metrics.properties" should be
        #    added to Java properties using -Dspark.metrics.conf=xxx if you want to
        #    customize metrics system. You can also put the file in ${SPARK_HOME}/conf
        #    and it will be loaded automatically.
        #    5. The MetricsServlet sink is added by default as a sink in the master,
        #    worker and driver, and you can send HTTP requests to the "/metrics/json"
        #    endpoint to get a snapshot of all the registered metrics in JSON format.
        #    For master, requests to the "/metrics/master/json" and
        #    "/metrics/applications/json" endpoints can be sent separately to get
        #    metrics snapshots of the master instance and applications. This
        #    MetricsServlet does not have to be configured.

        ## List of available common sources and their properties.

        # org.apache.spark.metrics.source.JvmSource
        #   Note: Currently, JvmSource is the only available common source.
        #         It can be added to an instance by setting the "class" option to its
        #         fully qualified class name (see examples below).

        ## List of available sinks and their properties.

        # org.apache.spark.metrics.sink.ConsoleSink
        #   Name:   Default:   Description:
        #   period  10         Poll period
        #   unit    seconds    Unit of the poll period

        # org.apache.spark.metrics.sink.CSVSink
        #   Name:     Default:   Description:
        #   period    10         Poll period
        #   unit      seconds    Unit of the poll period
        #   directory /tmp       Where to store CSV files

        # org.apache.spark.metrics.sink.GangliaSink
        #   Name:     Default:   Description:
        #   host      NONE       Hostname or multicast group of the Ganglia server,
        #                        must be set
        #   port      NONE       Port of the Ganglia server(s), must be set
        #   period    10         Poll period
        #   unit      seconds    Unit of the poll period
        #   ttl       1          TTL of messages sent by Ganglia
        #   dmax      0          Lifetime in seconds of metrics (0 never expired)
        #   mode      multicast  Ganglia network mode ('unicast' or 'multicast')

        # org.apache.spark.metrics.sink.JmxSink

        # org.apache.spark.metrics.sink.MetricsServlet
        #   Name:     Default:   Description:
        #   path      VARIES*    Path prefix from the web server root
        #   sample    false      Whether to show entire set of samples for histograms
        #                        ('false' or 'true')
        #
        # * Default path is /metrics/json for all instances except the master. The
        #   master has two paths:
        #     /metrics/applications/json # App information
        #     /metrics/master/json       # Master information

        # org.apache.spark.metrics.sink.GraphiteSink
        #   Name:     Default:      Description:
        #   host      NONE          Hostname of the Graphite server, must be set
        #   port      NONE          Port of the Graphite server, must be set
        #   period    10            Poll period
        #   unit      seconds       Unit of the poll period
        #   prefix    EMPTY STRING  Prefix to prepend to every metric's name
        #   protocol  tcp           Protocol ("tcp" or "udp") to use

        ## Examples
        # Enable JmxSink for all instances by class name
        #*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink

        # Enable ConsoleSink for all instances by class name
        #*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink

        # Polling period for the ConsoleSink
        #*.sink.console.period=10
        # Unit of the polling period for the ConsoleSink
        #*.sink.console.unit=seconds

        # Polling period for the ConsoleSink specific for the master instance
        #master.sink.console.period=15
        # Unit of the polling period for the ConsoleSink specific for the master
        # instance
        #master.sink.console.unit=seconds

        # Enable CsvSink for all instances by class name
        #*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink

        # Polling period for the CsvSink
        #*.sink.csv.period=1
        # Unit of the polling period for the CsvSink
        #*.sink.csv.unit=minutes

        # Polling directory for CsvSink
        #*.sink.csv.directory=/tmp/

        # Polling period for the CsvSink specific for the worker instance
        #worker.sink.csv.period=10
        # Unit of the polling period for the CsvSink specific for the worker instance
        #worker.sink.csv.unit=minutes

        # Enable Slf4jSink for all instances by class name
        #*.sink.slf4j.class=org.apache.spark.metrics.sink.Slf4jSink

        # Polling period for the Slf4JSink
        #*.sink.slf4j.period=1
        # Unit of the polling period for the Slf4jSink
        #*.sink.slf4j.unit=minutes

        # Enable JvmSource for instance master, worker, driver and executor
        #master.source.jvm.class=org.apache.spark.metrics.source.JvmSource

        #worker.source.jvm.class=org.apache.spark.metrics.source.JvmSource

        #driver.source.jvm.class=org.apache.spark.metrics.source.JvmSource

        #executor.source.jvm.class=org.apache.spark.metrics.source.JvmSource
      slaves.template: |-
        #
        # Licensed to the Apache Software Foundation (ASF) under one or more
        # contributor license agreements.  See the NOTICE file distributed with
        # this work for additional information regarding copyright ownership.
        # The ASF licenses this file to You under the Apache License, Version 2.0
        # (the "License"); you may not use this file except in compliance with
        # the License.  You may obtain a copy of the License at
        #
        #    http://www.apache.org/licenses/LICENSE-2.0
        #
        # Unless required by applicable law or agreed to in writing, software
        # distributed under the License is distributed on an "AS IS" BASIS,
        # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
        # See the License for the specific language governing permissions and
        # limitations under the License.
        #

        # A Spark Worker will be started on each of the machines listed below.
        localhost
      spark-defaults.conf: |-
        #
        # Licensed to the Apache Software Foundation (ASF) under one or more
        # contributor license agreements.  See the NOTICE file distributed with
        # this work for additional information regarding copyright ownership.
        # The ASF licenses this file to You under the Apache License, Version 2.0
        # (the "License"); you may not use this file except in compliance with
        # the License.  You may obtain a copy of the License at
        #
        #    http://www.apache.org/licenses/LICENSE-2.0
        #
        # Unless required by applicable law or agreed to in writing, software
        # distributed under the License is distributed on an "AS IS" BASIS,
        # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
        # See the License for the specific language governing permissions and
        # limitations under the License.
        #

        # Default system properties included when running spark-submit.
        # This is useful for setting default environmental settings.

        # Example:
        # spark.master                     spark://master:7077
        # spark.eventLog.enabled           true
        # spark.eventLog.dir               hdfs://namenode:8021/directory
        # spark.serializer                 org.apache.spark.serializer.KryoSerializer
        # spark.driver.memory              5g
        # spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"
        #spark.ui.reverseProxy              true
        #spark.ui.reverseProxyUrl           /
        spark.driver.extraClassPath /opt/spark/jars/alluxio-1.5.0-spark-client.jar
        spark.executor.extraClassPath /opt/spark/jars/alluxio-1.5.0-spark-client.jar
      spark-defaults.conf.template: |-
        #
        # Licensed to the Apache Software Foundation (ASF) under one or more
        # contributor license agreements.  See the NOTICE file distributed with
        # this work for additional information regarding copyright ownership.
        # The ASF licenses this file to You under the Apache License, Version 2.0
        # (the "License"); you may not use this file except in compliance with
        # the License.  You may obtain a copy of the License at
        #
        #    http://www.apache.org/licenses/LICENSE-2.0
        #
        # Unless required by applicable law or agreed to in writing, software
        # distributed under the License is distributed on an "AS IS" BASIS,
        # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
        # See the License for the specific language governing permissions and
        # limitations under the License.
        #

        # Default system properties included when running spark-submit.
        # This is useful for setting default environmental settings.

        # Example:
        # spark.master                     spark://master:7077
        # spark.eventLog.enabled           true
        # spark.eventLog.dir               hdfs://namenode:8021/directory
        # spark.serializer                 org.apache.spark.serializer.KryoSerializer
        # spark.driver.memory              5g
        # spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"
      spark-env.sh: |-
        #!/usr/bin/env bash

        #
        # Licensed to the Apache Software Foundation (ASF) under one or more
        # contributor license agreements.  See the NOTICE file distributed with
        # this work for additional information regarding copyright ownership.
        # The ASF licenses this file to You under the Apache License, Version 2.0
        # (the "License"); you may not use this file except in compliance with
        # the License.  You may obtain a copy of the License at
        #
        #    http://www.apache.org/licenses/LICENSE-2.0
        #
        # Unless required by applicable law or agreed to in writing, software
        # distributed under the License is distributed on an "AS IS" BASIS,
        # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
        # See the License for the specific language governing permissions and
        # limitations under the License.
        #

        # This file is sourced when running various Spark programs.
        # Copy it as spark-env.sh and edit that to configure Spark for your site.

        # Options read when launching programs locally with
        # ./bin/run-example or ./bin/spark-submit
        # - HADOOP_CONF_DIR, to point Spark towards Hadoop configuration files
        # - SPARK_LOCAL_IP, to set the IP address Spark binds to on this node
        # - SPARK_PUBLIC_DNS, to set the public dns name of the driver program
        # - SPARK_CLASSPATH, default classpath entries to append

        # Options read by executors and drivers running inside the cluster
        # - SPARK_LOCAL_IP, to set the IP address Spark binds to on this node
        # - SPARK_PUBLIC_DNS, to set the public DNS name of the driver program
        # - SPARK_CLASSPATH, default classpath entries to append
        # - SPARK_LOCAL_DIRS, storage directories to use on this node for shuffle and RDD data
        # - MESOS_NATIVE_JAVA_LIBRARY, to point to your libmesos.so if you use Mesos

        # Options read in YARN client mode
        # - HADOOP_CONF_DIR, to point Spark towards Hadoop configuration files
        # - SPARK_EXECUTOR_INSTANCES, Number of executors to start (Default: 2)
        # - SPARK_EXECUTOR_CORES, Number of cores for the executors (Default: 1).
        # - SPARK_EXECUTOR_MEMORY, Memory per Executor (e.g. 1000M, 2G) (Default: 1G)
        # - SPARK_DRIVER_MEMORY, Memory for Driver (e.g. 1000M, 2G) (Default: 1G)

        # Options for the daemons used in the standalone deploy mode
        # - SPARK_MASTER_HOST, to bind the master to a different IP address or hostname
        # - SPARK_MASTER_PORT / SPARK_MASTER_WEBUI_PORT, to use non-default ports for the master
        # - SPARK_MASTER_OPTS, to set config properties only for the master (e.g. "-Dx=y")
        # - SPARK_WORKER_CORES, to set the number of cores to use on this machine
        # - SPARK_WORKER_MEMORY, to set how much total memory workers have to give executors (e.g. 1000m, 2g)
        # - SPARK_WORKER_PORT / SPARK_WORKER_WEBUI_PORT, to use non-default ports for the worker
        # - SPARK_WORKER_INSTANCES, to set the number of worker processes per node
        # - SPARK_WORKER_DIR, to set the working directory of worker processes
        # - SPARK_WORKER_OPTS, to set config properties only for the worker (e.g. "-Dx=y")
        # - SPARK_DAEMON_MEMORY, to allocate to the master, worker and history server themselves (default: 1g).
        # - SPARK_HISTORY_OPTS, to set config properties only for the history server (e.g. "-Dx=y")
        # - SPARK_SHUFFLE_OPTS, to set config properties only for the external shuffle service (e.g. "-Dx=y")
        # - SPARK_DAEMON_JAVA_OPTS, to set config properties for all daemons (e.g. "-Dx=y")
        # - SPARK_PUBLIC_DNS, to set the public dns name of the master or workers

        # Generic options for the daemons used in the standalone deploy mode
        # - SPARK_CONF_DIR      Alternate conf dir. (Default: ${SPARK_HOME}/conf)
        # - SPARK_LOG_DIR       Where log files are stored.  (Default: ${SPARK_HOME}/logs)
        # - SPARK_PID_DIR       Where the pid file is stored. (Default: /tmp)
        # - SPARK_IDENT_STRING  A string representing this instance of spark. (Default: $USER)
        # - SPARK_NICENESS      The scheduling priority for daemons. (Default: 0)
        # - SPARK_NO_DAEMONIZE  Run the proposed command in the foreground. It will not output a PID file.
      spark-env.sh.template: |-
        #!/usr/bin/env bash

        #
        # Licensed to the Apache Software Foundation (ASF) under one or more
        # contributor license agreements.  See the NOTICE file distributed with
        # this work for additional information regarding copyright ownership.
        # The ASF licenses this file to You under the Apache License, Version 2.0
        # (the "License"); you may not use this file except in compliance with
        # the License.  You may obtain a copy of the License at
        #
        #    http://www.apache.org/licenses/LICENSE-2.0
        #
        # Unless required by applicable law or agreed to in writing, software
        # distributed under the License is distributed on an "AS IS" BASIS,
        # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
        # See the License for the specific language governing permissions and
        # limitations under the License.
        #

        # This file is sourced when running various Spark programs.
        # Copy it as spark-env.sh and edit that to configure Spark for your site.

        # Options read when launching programs locally with
        # ./bin/run-example or ./bin/spark-submit
        # - HADOOP_CONF_DIR, to point Spark towards Hadoop configuration files
        # - SPARK_LOCAL_IP, to set the IP address Spark binds to on this node
        # - SPARK_PUBLIC_DNS, to set the public dns name of the driver program
        # - SPARK_CLASSPATH, default classpath entries to append

        # Options read by executors and drivers running inside the cluster
        # - SPARK_LOCAL_IP, to set the IP address Spark binds to on this node
        # - SPARK_PUBLIC_DNS, to set the public DNS name of the driver program
        # - SPARK_CLASSPATH, default classpath entries to append
        # - SPARK_LOCAL_DIRS, storage directories to use on this node for shuffle and RDD data
        # - MESOS_NATIVE_JAVA_LIBRARY, to point to your libmesos.so if you use Mesos

        # Options read in YARN client mode
        # - HADOOP_CONF_DIR, to point Spark towards Hadoop configuration files
        # - SPARK_EXECUTOR_INSTANCES, Number of executors to start (Default: 2)
        # - SPARK_EXECUTOR_CORES, Number of cores for the executors (Default: 1).
        # - SPARK_EXECUTOR_MEMORY, Memory per Executor (e.g. 1000M, 2G) (Default: 1G)
        # - SPARK_DRIVER_MEMORY, Memory for Driver (e.g. 1000M, 2G) (Default: 1G)

        # Options for the daemons used in the standalone deploy mode
        # - SPARK_MASTER_HOST, to bind the master to a different IP address or hostname
        # - SPARK_MASTER_PORT / SPARK_MASTER_WEBUI_PORT, to use non-default ports for the master
        # - SPARK_MASTER_OPTS, to set config properties only for the master (e.g. "-Dx=y")
        # - SPARK_WORKER_CORES, to set the number of cores to use on this machine
        # - SPARK_WORKER_MEMORY, to set how much total memory workers have to give executors (e.g. 1000m, 2g)
        # - SPARK_WORKER_PORT / SPARK_WORKER_WEBUI_PORT, to use non-default ports for the worker
        # - SPARK_WORKER_INSTANCES, to set the number of worker processes per node
        # - SPARK_WORKER_DIR, to set the working directory of worker processes
        # - SPARK_WORKER_OPTS, to set config properties only for the worker (e.g. "-Dx=y")
        # - SPARK_DAEMON_MEMORY, to allocate to the master, worker and history server themselves (default: 1g).
        # - SPARK_HISTORY_OPTS, to set config properties only for the history server (e.g. "-Dx=y")
        # - SPARK_SHUFFLE_OPTS, to set config properties only for the external shuffle service (e.g. "-Dx=y")
        # - SPARK_DAEMON_JAVA_OPTS, to set config properties for all daemons (e.g. "-Dx=y")
        # - SPARK_PUBLIC_DNS, to set the public dns name of the master or workers

        # Generic options for the daemons used in the standalone deploy mode
        # - SPARK_CONF_DIR      Alternate conf dir. (Default: ${SPARK_HOME}/conf)
        # - SPARK_LOG_DIR       Where log files are stored.  (Default: ${SPARK_HOME}/logs)
        # - SPARK_PID_DIR       Where the pid file is stored. (Default: /tmp)
        # - SPARK_IDENT_STRING  A string representing this instance of spark. (Default: $USER)
        # - SPARK_NICENESS      The scheduling priority for daemons. (Default: 0)
        # - SPARK_NO_DAEMONIZE  Run the proposed command in the foreground. It will not output a PID file.
    kind: ConfigMap
    metadata:
      name: spark-worker-config
  - apiVersion: v1
    kind: Service
    metadata:
      labels:
        spark-name: ${MASTER_NAME}
      name: ${MASTER_NAME}
    spec:
      clusterIP: None
      ports:
      - port: 7077
        protocol: TCP
        targetPort: 7077
      selector:
        spark-name: ${MASTER_NAME}
  - apiVersion: v1
    kind: Service
    metadata:
      labels:
        spark-name: ${WORKER_NAME}
      name: ${WORKER_NAME}
    spec:
      clusterIP: None
      ports:
      - name: workerport
        port: 7078
        protocol: TCP
        targetPort: 7078
      - name: workerwebport
        port: 11008
        protocol: TCP
        targetPort: 11008
      selector:
        spark-name: ${WORKER_NAME}
  - apiVersion: v1
    kind: Service
    metadata:
      labels:
        spark-name: ${MASTER_NAME}
      name: ${MASTER_NAME}-webui
    spec:
      ports:
      - port: 10080
        protocol: TCP
        targetPort: 10080
      selector:
        spark-name: ${MASTER_NAME}
  - apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      annotations:
        volume.beta.kubernetes.io/storage-class: nfs
        volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/glusterfs
      name: ${MASTER_NAME}
    spec:
      accessModes:
      - ReadWriteMany
      resources:
        requests:
          storage: ${PV_SIZE}Gi
  - apiVersion: apps/v1beta1
    kind: StatefulSet
    metadata:
      labels:
        spark-name: ${MASTER_NAME}
      name: ${MASTER_NAME}
    spec:
      replicas: 1
      selector:
        matchLabels:
          spark-name: ${MASTER_NAME}
      serviceName: ${MASTER_NAME}
      template:
        metadata:
          labels:
            spark-name: ${MASTER_NAME}
        spec:
          affinity:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: spark-name
                      operator: In
                      values:
                      - ${MASTER_NAME}
                  topologyKey: kubernetes.io/hostname
                weight: 100
          containers:
          - env:
            - name: SPARK_MASTER_PORT
              value: "7077"
            - name: SPARK_MASTER_WEBUI_PORT
              value: "10080"
            - name: SPARK_DAEMON_JAVA_OPTS
              value: -Dspark.deploy.recoveryMode=FILESYSTEM -Dspark.deploy.recoveryDirectory=/var/sparkdata/recovery
            image: ${SPARK_IMAGE}
            imagePullPolicy: Always
            name: ${MASTER_NAME}
            ports:
            - containerPort: 7077
              protocol: TCP
            - containerPort: 8080
              protocol: TCP
            resources:
              limits:
                cpu: ${MASTER_CPU_LIMIT}
                memory: ${MASTER_MEM_LIMIT}G
              requests:
                cpu: 200m
                memory: 200M
            securityContext:
              privileged: false
              runAsUser: 0
            volumeMounts:
            - mountPath: /var/sparkdata
              name: sparkdata
            - mountPath: /opt/spark/conf/
              name: config-volume
          terminationGracePeriodSeconds: 10
          volumes:
          - configMap:
              name: spark-master-config
            name: config-volume
          - name: sparkdata
            persistentVolumeClaim:
              claimName: ${MASTER_NAME}
  - apiVersion: apps/v1beta1
    kind: StatefulSet
    metadata:
      labels:
        spark-name: ${WORKER_NAME}
      name: ${WORKER_NAME}
    spec:
      replicas: ${REPLICAS}
      selector:
        matchLabels:
          spark-name: ${WORKER_NAME}
      serviceName: ${WORKER_NAME}
      template:
        metadata:
          labels:
            spark-name: ${WORKER_NAME}
        spec:
          affinity:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: spark-name
                      operator: In
                      values:
                      - ${WORKER_NAME}
                  topologyKey: kubernetes.io/hostname
                weight: 100
          containers:
          - env:
            - name: SPARK_MASTER_ADDRESS
              value: spark://${MASTER_NAME}-0.${MASTER_NAME}:7077
            - name: SPARK_MASTER_UI_ADDRESS
              value: http://${MASTER_NAME}-0.${MASTER_NAME}:10080
            - name: SPARK_WORKER_PORT
              value: "7078"
            - name: SPARK_WORKER_WEBUI_PORT
              value: "11008"
            image: ${SPARK_IMAGE}
            imagePullPolicy: Always
            name: ${WORKER_NAME}
            ports:
            - containerPort: 7078
              protocol: TCP
            - containerPort: 11008
              protocol: TCP
            resources:
              limits:
                cpu: ${WORKER_CPU_LIMIT}
                memory: ${WORKER_MEM_LIMIT}G
              requests:
                cpu: 200m
                memory: 200M
            securityContext:
              privileged: false
              runAsUser: 0
            volumeMounts:
            - mountPath: /var/sparkdata
              name: sparkdata
            - mountPath: /opt/spark/conf/
              name: config-volume
          volumes:
          - name: sparkdata
            persistentVolumeClaim:
              claimName: ${MASTER_NAME}
          - configMap:
              name: spark-worker-config
            name: config-volume
  parameters:
  - description: Name of the Spark master/worker image
    name: SPARK_IMAGE
    required: true
    value: openshift/spark:v2.1
  - description: master name used as a service name and a selector
    name: MASTER_NAME
    required: true
    value: sparkmaster
  - description: worker name used as a selector
    name: WORKER_NAME
    required: true
    value: sparkworker
  - description: persistentVolume_size_Gi
    name: PV_SIZE
    required: true
    value: "10"
  - description: MASTER_NODE_CPU_LIMIT_CORES
    name: MASTER_CPU_LIMIT
    required: true
    value: "2"
  - description: MASTER_NODE_MEM_LIMIT_GB
    name: MASTER_MEM_LIMIT
    required: true
    value: "2"
  - description: WORKER_CPU_LIMIT_CORES
    name: WORKER_CPU_LIMIT
    required: true
    value: "8"
  - description: WORKER_MEM_LIMIT_GB
    name: WORKER_MEM_LIMIT
    required: true
    value: "24"
  - description: Number of replicas.
    name: REPLICAS
    required: true
    value: "3"
- apiVersion: v1
  kind: Template
  labels:
    template: storm-monitor
  metadata:
    annotations:
      description: Storm Monitor
      iconClass: icon-database
      openshift.io/display-name: Storm-Monitor
      tags: database,storm
    creationTimestamp: 2017-09-07T09:24:29Z
    name: storm-monitor
    namespace: openshift
    resourceVersion: "27449664"
    selfLink: /oapi/v1/namespaces/openshift/templates/storm-monitor
    uid: 59587dc4-93ae-11e7-a1ba-ecf4bbe2f144
  objects:
  - apiVersion: v1
    data:
      storm.yaml: |
        ${CONFIG_FILE_CONTENS}
        # Generated properties
        ui.port: ${UI_PORT}
        nimbus.seeds : [${NIMBUS_SEEDS}]
        storm.local.dir: "${DATA_DIR}"
        storm.log.dir: "${LOG_DIR}"
    kind: ConfigMap
    metadata:
      labels:
        app: ${NAME}
      name: ${NAME}-config
  - apiVersion: v1
    kind: Service
    metadata:
      labels:
        app: ${NAME}
      name: ${NAME}
    spec:
      clusterIP: None
      ports:
      - name: ui
        port: ${UI_PORT}
      selector:
        app: ${NAME}
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      labels:
        app: ${NAME}
      name: ${NAME}
    spec:
      replicas: 1
      strategy:
        type: Rolling
      template:
        metadata:
          labels:
            app: ${NAME}
        spec:
          containers:
          - args:
            - storm --config ${CONF_DIR}/storm.yaml ui
            command:
            - /bin/bash
            - -c
            image: storm:${VERSION}
            imagePullPolicy: IfNotPresent
            livenessProbe:
              initialDelaySeconds: 20
              periodSeconds: 20
              tcpSocket:
                port: ui
            name: ${NAME}-ui
            ports:
            - containerPort: ${UI_PORT}
              name: ui
              protocol: TCP
            readinessProbe:
              initialDelaySeconds: 20
              periodSeconds: 5
              tcpSocket:
                port: ui
            terminationMessagePath: /dev/termination-log
            volumeMounts:
            - mountPath: ${CONF_DIR}
              name: config-volume
            - mountPath: ${DATA_DIR}
              name: data-volume
            - mountPath: ${LOG_DIR}
              name: log-volume
          restartPolicy: Always
          terminationGracePeriodSeconds: 10
          volumes:
          - configMap:
              name: ${NAME}-config
            name: config-volume
          - emptyDir: {}
            name: data-volume
          - emptyDir: {}
            name: log-volume
      triggers:
      - type: ConfigChange
  - apiVersion: v1
    kind: Route
    metadata:
      labels:
        app: ${NAME}
      name: ${NAME}
    spec:
      host: ${NAME}-ui.${ROUTE_SERVICE_DOMAIN}
      port:
        targetPort: ui
      to:
        kind: Service
        name: ${NAME}
        weight: 100
      wildcardPolicy: None
  parameters:
  - name: NAME
    required: true
    value: storm-monitor
  - name: VERSION
    required: true
    value: 1.1.0
  - description: Conf directory
    name: CONF_DIR
    required: true
    value: /conf
  - description: Data directory
    name: DATA_DIR
    required: true
    value: /data
  - description: Log Directory
    name: LOG_DIR
    required: true
    value: /logs
  - description: Webconsole port
    name: UI_PORT
    required: true
    value: "8080"
  - description: |
      "Nimbus seeds, FQDN values for all seeds in the cluster,
       format must be: \"storm-nimbus-0.storm-nimbus.storm.svc.cluster.local\", \"storm-nimbus-1.storm-nimbus.storm.svc.cluster.local\" (Note quote marks)."
    name: NIMBUS_SEEDS
    required: true
    value: '"storm-nimbus-0.storm-nimbus.storm.svc.cluster.local", "storm-nimbus-1.storm-nimbus.storm.svc.cluster.local"'
  - description: Add storm.yaml contents
    name: CONFIG_FILE_CONTENS
    required: true
    value: |
      # These properties will be generated automatically by provided params
      # storm.zookeeper.servers
      # nimbus.thrift.port
      # nimbus.seeds
      # ui.port
      # logviewer.port
      # storm.log.dir
      # storm.local.dir
      # Add any other properties here
      ui.host: 0.0.0.0
      ui.childopts: "-Xmx768m"
      ui.actions.enabled: true
      ui.filter: null
      ui.filter.params: null
      ui.users: null
      ui.header.buffer.bytes: 4096
      ui.http.creds.plugin: org.apache.storm.security.auth.DefaultHttpCredentialsPlugin
  - description: The memory resource request.
    name: RESOURCE_MEMORY_REQ
    required: true
    value: 512Mi
  - description: The limits for memory resource.
    name: RESOURCE_MEMORY_LIMIT
    required: true
    value: 512Mi
  - description: The CPU resource request.
    name: RESOURCE_CPU_REQ
    required: true
    value: 250m
  - description: The limits for CPU resource.
    name: RESOURCE_CPU_LIMIT
    required: true
    value: 500m
  - description: 'Service domain for external routes. The route URL will be: NAME.ROUTE_SERVICE_DOMAIN'
    name: ROUTE_SERVICE_DOMAIN
    required: true
    value: svc.public.com
- apiVersion: v1
  kind: Template
  labels:
    template: storm-nimbus
  metadata:
    annotations:
      description: Storm Nimbus
      iconClass: icon-database
      openshift.io/display-name: Storm-Nimbus
      tags: database,storm
    creationTimestamp: 2017-09-07T09:24:03Z
    name: storm-nimbus
    namespace: openshift
    resourceVersion: "27449680"
    selfLink: /oapi/v1/namespaces/openshift/templates/storm-nimbus
    uid: 4a58ecbd-93ae-11e7-a1ba-ecf4bbe2f144
  objects:
  - apiVersion: v1
    data:
      storm.yaml: |
        ${CONFIG_FILE_CONTENS}
        # Generated properties
        storm.zookeeper.servers: [${ZK_SERVERS}]
        nimbus.thrift.port: ${PORT}
        storm.local.dir: "${DATA_DIR}"
        storm.log.dir: "${LOG_DIR}"
        logviewer.port: ${LOGVIEWER_PORT}
    kind: ConfigMap
    metadata:
      labels:
        app: ${NAME}
      name: ${NAME}-config
  - apiVersion: v1
    kind: Service
    metadata:
      annotations:
        service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
      labels:
        app: ${NAME}
      name: ${NAME}
    spec:
      clusterIP: None
      ports:
      - name: nimbus
        port: ${PORT}
      - name: logviewer
        port: ${LOGVIEWER_PORT}
      selector:
        app: ${NAME}
  - apiVersion: apps/v1beta1
    kind: StatefulSet
    metadata:
      annotations:
        scheduler.alpha.kubernetes.io/affinity: |
          {
            "podAntiAffinity": {
              "requiredDuringSchedulingIgnoredDuringExecution": [{
                "labelSelector": {
                  "matchExpressions": [{
                    "key": "app",
                    "operator": "In",
                    "values": ["${NAME}"]
                  }]
                },
                "topologyKey": "kubernetes.io/hostname"
              }]
            }
          }
      labels:
        app: ${NAME}
      name: ${NAME}
    spec:
      replicas: ${REPLICAS}
      selector:
        matchLabels:
          app: ${NAME}
      serviceName: ${NAME}
      template:
        metadata:
          labels:
            app: ${NAME}
        spec:
          containers:
          - args:
            - storm --config ${CONF_DIR}/storm.yaml nimbus
            command:
            - /bin/bash
            - -c
            image: storm:${VERSION}
            imagePullPolicy: IfNotPresent
            livenessProbe:
              initialDelaySeconds: 30
              periodSeconds: 15
              tcpSocket:
                port: nimbus
            name: ${NAME}
            ports:
            - containerPort: ${PORT}
              name: nimbus
              protocol: TCP
            readinessProbe:
              initialDelaySeconds: 30
              periodSeconds: 15
              tcpSocket:
                port: nimbus
            resources:
              limits:
                cpu: ${RESOURCE_CPU_LIMIT}
                memory: ${RESOURCE_MEMORY_LIMIT}
              requests:
                cpu: ${RESOURCE_CPU_REQ}
                memory: ${RESOURCE_MEMORY_REQ}
            terminationMessagePath: /dev/termination-log
            volumeMounts:
            - mountPath: ${CONF_DIR}
              name: config-volume
            - mountPath: ${DATA_DIR}
              name: data-volume
            - mountPath: ${LOG_DIR}
              name: log-volume
          - args:
            - storm --config ${CONF_DIR}/storm.yaml logviewer
            command:
            - /bin/bash
            - -c
            image: storm:${VERSION}
            imagePullPolicy: IfNotPresent
            livenessProbe:
              initialDelaySeconds: 30
              periodSeconds: 20
              tcpSocket:
                port: logviewer
            name: ${NAME}-logviewer
            ports:
            - containerPort: ${LOGVIEWER_PORT}
              name: logviewer
              protocol: TCP
            readinessProbe:
              initialDelaySeconds: 30
              periodSeconds: 5
              tcpSocket:
                port: logviewer
            terminationMessagePath: /dev/termination-log
            volumeMounts:
            - mountPath: ${CONF_DIR}
              name: config-volume
            - mountPath: ${DATA_DIR}
              name: data-volume
            - mountPath: ${LOG_DIR}
              name: log-volume
              restartPolicy: Always
          terminationGracePeriodSeconds: 10
          volumes:
          - configMap:
              name: ${NAME}-config
            name: config-volume
          - emptyDir: {}
            name: data-volume
          - emptyDir: {}
            name: log-volume
  parameters:
  - name: NAME
    required: true
    value: storm-nimbus
  - description: Number of replicas
    name: REPLICAS
    required: true
    value: "1"
  - name: VERSION
    required: true
    value: 1.1.0
  - description: Conf directory
    name: CONF_DIR
    required: true
    value: /conf
  - description: Data directory
    name: DATA_DIR
    required: true
    value: /data
  - description: Log Directory
    name: LOG_DIR
    required: true
    value: /logs
  - description: Nimbus thrift port
    name: PORT
    required: true
    value: "6627"
  - description: Nimbus thrift port
    name: LOGVIEWER_PORT
    required: true
    value: "8000"
  - description: 'Zookeeper servers, value format must be: "zk-server1", "zk-server2"
      (Note quote marks)'
    name: ZK_SERVERS
    required: true
    value: '"zk-server1", "zk-server2"'
  - description: Add storm.yaml contents
    name: CONFIG_FILE_CONTENS
    required: true
    value: |
      # These properties will be generated automatically by provided params
      # storm.zookeeper.servers
      # nimbus.thrift.port
      # nimbus.seeds
      # storm.log.dir
      # storm.local.dir
      # Add any other properties here
      # LogViewer
      logviewer.childopts: "-Xmx128m"
      logviewer.cleanup.age.mins: 10080
      logviewer.appender.name: "A1"
      logviewer.max.sum.worker.logs.size.mb: 4096
      logviewer.max.per.worker.logs.size.mb: 2048
      logs.users: null
  - description: The memory resource request.
    name: RESOURCE_MEMORY_REQ
    value: 512Mi
  - description: The limits for memory resource.
    name: RESOURCE_MEMORY_LIMIT
    value: 1Gi
  - description: The CPU resource request.
    name: RESOURCE_CPU_REQ
    value: 500m
  - description: The limits for CPU resource.
    name: RESOURCE_CPU_LIMIT
    value: "1"
- apiVersion: v1
  kind: Template
  labels:
    template: storm-worker
  metadata:
    annotations:
      description: Storm Worker. Number of replicas will be managed by an horizontal
        por autoscaler
      iconClass: icon-database
      openshift.io/display-name: Storm-Worker
      tags: database,storm
    creationTimestamp: 2017-09-07T09:23:34Z
    name: storm-worker
    namespace: openshift
    resourceVersion: "27449690"
    selfLink: /oapi/v1/namespaces/openshift/templates/storm-worker
    uid: 391c3cb3-93ae-11e7-a1ba-ecf4bbe2f144
  objects:
  - apiVersion: v1
    data:
      storm.yaml: |
        ${CONFIG_FILE_CONTENS}
        # Generated properties
        storm.zookeeper.servers: [${ZK_SERVERS}]
        nimbus.thrift.port: ${NIMBUS_THRIFT_PORT}
        nimbus.seeds : [${NIMBUS_SEEDS}]
        drpc.servers: [${NIMBUS_SEEDS}]
        supervisor.slots.ports: [${SUPERVISOR_PORTS}]
        storm.local.dir: "${DATA_DIR}"
        storm.log.dir: "${LOG_DIR}"
    kind: ConfigMap
    metadata:
      labels:
        app: ${NAME}
      name: ${NAME}-config
  - apiVersion: v1
    kind: Service
    metadata:
      labels:
        app: ${NAME}
      name: ${NAME}
    spec:
      clusterIP: None
      ports:
      - name: "3700"
        port: 6700
      - name: "3701"
        port: 6701
      - name: "3702"
        port: 6702
      - name: "3703"
        port: 6703
      - name: "3772"
        port: 3772
      - name: "3773"
        port: 3773
      - name: "3774"
        port: 3774
      selector:
        app: ${NAME}
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      annotations:
        billing: "true"
      labels:
        app: ${NAME}
      name: ${NAME}
    spec:
      replicas: ${MIN_REPLICAS}
      strategy:
        type: Rolling
      template:
        metadata:
          labels:
            app: ${NAME}
        spec:
          affinity:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: app
                      operator: In
                      values:
                      - ${NAME}
                  topologyKey: kubernetes.io/hostname
                weight: 100
          containers:
          - args:
            - storm --config ${CONF_DIR}/storm.yaml supervisor
            command:
            - /bin/bash
            - -c
            image: storm:${VERSION}
            imagePullPolicy: IfNotPresent
            livenessProbe:
              exec:
                command:
                - /bin/bash
                - -c
                - storm --config ${CONF_DIR}/storm.yaml node-health-check
                initialDelaySeconds: 15
                periodSeconds: 5
            name: ${NAME}
            ports:
            - containerPort: 6700
              protocol: TCP
            - containerPort: 6701
              protocol: TCP
            - containerPort: 6702
              protocol: TCP
            - containerPort: 6703
              protocol: TCP
            - containerPort: 3772
              protocol: TCP
            - containerPort: 3773
              protocol: TCP
            - containerPort: 3774
              protocol: TCP
            resources:
              limits:
                cpu: ${RESOURCE_CPU_LIMIT}
                memory: ${RESOURCE_MEMORY_LIMIT}
              requests:
                cpu: ${RESOURCE_CPU_REQ}
                memory: ${RESOURCE_MEMORY_REQ}
            terminationMessagePath: /dev/termination-log
            volumeMounts:
            - mountPath: ${CONF_DIR}
              name: config-volume
            - mountPath: ${DATA_DIR}
              name: data-volume
            - mountPath: ${LOG_DIR}
              name: log-volume
          restartPolicy: Always
          terminationGracePeriodSeconds: 10
          volumes:
          - configMap:
              name: ${NAME}-config
            name: config-volume
          - emptyDir: {}
            name: data-volume
          - emptyDir: {}
            name: log-volume
      triggers:
      - type: ConfigChange
  - apiVersion: autoscaling/v1
    kind: HorizontalPodAutoscaler
    metadata:
      labels:
        app: ${NAME}
      name: ${NAME}
    spec:
      maxReplicas: ${MAX_REPLICAS}
      minReplicas: ${MIN_REPLICAS}
      scaleTargetRef:
        apiVersion: v1
        kind: DeploymentConfig
        name: ${NAME}
      targetCPUUtilizationPercentage: ${CPU_SCALE_TARGET}
  parameters:
  - name: NAME
    required: true
    value: storm-worker
  - description: Min number of replicas
    name: MIN_REPLICAS
    required: true
    value: "1"
  - description: Max number of replicas
    name: MAX_REPLICAS
    required: true
    value: "1"
  - description: Target CPU utilization (percentage) to scale up or down
    name: CPU_SCALE_TARGET
    required: true
    value: "80"
  - name: VERSION
    required: true
    value: 1.1.0
  - description: Conf directory
    name: CONF_DIR
    required: true
    value: /conf
  - description: Data directory
    name: DATA_DIR
    required: true
    value: /data
  - description: Log Directory
    name: LOG_DIR
    required: true
    value: /logs
  - description: Nimbus thrift port
    name: NIMBUS_THRIFT_PORT
    required: true
    value: "6627"
  - description: |
      "Zookeeper servers, value format must be: \"zk-0.zk\", \"zk-1.zk\", \"zk-2.zk\" (Note quote marks)."
    name: ZK_SERVERS
    required: true
    value: '"zk-0.zk", "zk-1.zk","zk-2.zk"'
  - description: |
      "Nimbus seeds, FQDN values for all seeds in the cluster, format must be: \"storm-nimbus-0.storm-nimbus.storm.svc.cluster.local\", \"storm-nimbus-1.storm-nimbus.storm.svc.cluster.local\" (Note quote marks)."
    name: NIMBUS_SEEDS
    required: true
    value: '"storm-nimbus-0.storm-nimbus.storm.svc.cluster.local", "storm-nimbus-1.storm-nimbus.storm.svc.cluster.local"'
  - description: |
      "Supervisor ports, value format must be: port1, port2 (Note colon mark)."
    name: SUPERVISOR_PORTS
    required: true
    value: 6700, 6701, 6702, 6703
  - description: Add custom contents to configuration file storm.yaml
    name: CONFIG_FILE_CONTENS
    required: true
    value: |
      # These properties will be generated automatically by provided params
      # storm.zookeeper.servers
      # nimbus.thrift.port
      # nimbus.seeds
      # storm.log.dir
      # storm.local.dir
      # Add any other properties here
      # Pacemaker
      #pacemaker.servers: []
      #pacemaker.port: 6699
      #storm.cluster.state.store: "org.apache.storm.pacemaker.pacemaker_state_factory"
  - description: The memory resource request.
    name: RESOURCE_MEMORY_REQ
    value: 512Mi
  - description: The limits for memory resource.
    name: RESOURCE_MEMORY_LIMIT
    value: 1Gi
  - description: The CPU resource request.
    name: RESOURCE_CPU_REQ
    value: 500m
  - description: The limits for CPU resource.
    name: RESOURCE_CPU_LIMIT
    value: "1"
- apiVersion: v1
  kind: Template
  message: Login on https://${TENANT_NAME}-admin.${WILDCARD_DOMAIN} as ${ADMIN_USERNAME}/${ADMIN_PASSWORD}
  metadata:
    creationTimestamp: 2017-08-23T07:04:10Z
    name: system
    namespace: openshift
    resourceVersion: "856"
    selfLink: /oapi/v1/namespaces/openshift/templates/system
    uid: 434c9977-87d1-11e7-9a67-ecf4bbe2f144
  objects:
  - apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: system-storage
    spec:
      accessModes:
      - ReadWriteMany
      resources:
        requests:
          storage: 100Mi
  - apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: mysql-storage
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 1Gi
  - apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: system-redis-storage
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 1Gi
  - apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: backend-redis-storage
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 1Gi
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      name: backend-cron
    spec:
      replicas: 1
      selector:
        name: backend-cron
      strategy:
        rollingParams:
          intervalSeconds: 1
          maxSurge: 25%
          maxUnavailable: 25%
          timeoutSeconds: 600
          updatePeriodSeconds: 1
        type: Rolling
      template:
        metadata:
          labels:
            name: backend-cron
        spec:
          containers:
          - args:
            - backend-cron
            env:
            - name: CONFIG_REDIS_PROXY
              value: backend-redis:6379
            - name: CONFIG_QUEUES_MASTER_NAME
              value: backend-redis:6379/1
            - name: RACK_ENV
              value: production
            image: 3scale-amp20/backend:1.0-2
            imagePullPolicy: IfNotPresent
            name: backend-cron
      triggers:
      - type: ConfigChange
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      name: backend-redis
    spec:
      replicas: 1
      selector:
        name: backend-redis
      strategy:
        type: Recreate
      template:
        metadata:
          labels:
            name: backend-redis
        spec:
          containers:
          - image: ${REDIS_IMAGE}
            imagePullPolicy: IfNotPresent
            livenessProbe:
              initialDelaySeconds: 10
              periodSeconds: 10
              tcpSocket:
                port: 6379
            name: backend-redis
            readinessProbe:
              exec:
                command:
                - container-entrypoint
                - bash
                - -c
                - redis-cli set liveness-probe "`date`" | grep OK
              initialDelaySeconds: 10
              periodSeconds: 30
              timeoutSeconds: 1
            volumeMounts:
            - mountPath: /var/lib/redis/data
              name: backend-redis-storage
            - mountPath: /etc/redis.conf
              name: redis-config
              subPath: redis.conf
          volumes:
          - name: backend-redis-storage
            persistentVolumeClaim:
              claimName: backend-redis-storage
          - configMap:
              items:
              - key: redis.conf
                path: redis.conf
              name: redis-config
            name: redis-config
      triggers:
      - type: ConfigChange
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      name: backend-listener
    spec:
      replicas: 1
      selector:
        name: backend-listener
      strategy:
        rollingParams:
          intervalSeconds: 1
          maxSurge: 25%
          maxUnavailable: 25%
          timeoutSeconds: 600
          updatePeriodSeconds: 1
        type: Rolling
      template:
        metadata:
          labels:
            name: backend-listener
        spec:
          containers:
          - args:
            - 3scale_backend
            - start
            - -e
            - production
            - -p
            - "3000"
            - -x
            - /dev/stdout
            env:
            - name: CONFIG_REDIS_PROXY
              value: backend-redis:6379
            - name: CONFIG_QUEUES_MASTER_NAME
              value: backend-redis:6379/1
            - name: RACK_ENV
              value: production
            - name: CONFIG_INTERNAL_API_USER
              value: ${SYSTEM_BACKEND_USERNAME}
            - name: CONFIG_INTERNAL_API_PASSWORD
              value: ${SYSTEM_BACKEND_PASSWORD}
            image: 3scale-amp20/backend:1.0-2
            imagePullPolicy: IfNotPresent
            livenessProbe:
              initialDelaySeconds: 30
              periodSeconds: 10
              tcpSocket:
                port: 3000
            name: backend-listener
            ports:
            - containerPort: 3000
              protocol: TCP
            readinessProbe:
              httpGet:
                path: /status
                port: 3000
              initialDelaySeconds: 30
              timeoutSeconds: 5
      triggers:
      - type: ConfigChange
  - apiVersion: v1
    kind: Service
    metadata:
      name: backend-redis
    spec:
      ports:
      - port: 6379
        protocol: TCP
        targetPort: 6379
      selector:
        name: backend-redis
  - apiVersion: v1
    kind: Service
    metadata:
      name: backend-listener
    spec:
      ports:
      - name: http
        port: 3000
        protocol: TCP
        targetPort: 3000
      selector:
        name: backend-listener
  - apiVersion: v1
    kind: Service
    metadata:
      name: system-provider
    spec:
      ports:
      - name: http
        port: 3000
        protocol: TCP
        targetPort: provider
      selector:
        name: system-app
  - apiVersion: v1
    kind: Service
    metadata:
      name: system-developer
    spec:
      ports:
      - name: http
        port: 3000
        protocol: TCP
        targetPort: developer
      selector:
        name: system-app
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      name: backend-worker
    spec:
      replicas: 1
      selector:
        name: backend-worker
      strategy:
        rollingParams:
          intervalSeconds: 1
          maxSurge: 25%
          maxUnavailable: 25%
          timeoutSeconds: 600
          updatePeriodSeconds: 1
        type: Rolling
      template:
        metadata:
          labels:
            name: backend-worker
        spec:
          containers:
          - args:
            - 3scale_backend_worker
            - run
            env:
            - name: CONFIG_REDIS_PROXY
              value: backend-redis:6379
            - name: CONFIG_QUEUES_MASTER_NAME
              value: backend-redis:6379/1
            - name: RACK_ENV
              value: production
            - name: CONFIG_EVENTS_HOOK
              value: http://system-provider:3000/master/events/import
            - name: CONFIG_EVENTS_HOOK_SHARED_SECRET
              value: ${SYSTEM_BACKEND_SHARED_SECRET}
            image: 3scale-amp20/backend:1.0-2
            imagePullPolicy: IfNotPresent
            name: backend-worker
      triggers:
      - type: ConfigChange
  - apiVersion: v1
    kind: Service
    metadata:
      name: system-mysql
    spec:
      ports:
      - name: system-mysql
        nodePort: 0
        port: 3306
        protocol: TCP
        targetPort: 3306
      selector:
        name: system-mysql
  - apiVersion: v1
    kind: Service
    metadata:
      name: system-redis
    spec:
      ports:
      - name: redis
        port: 6379
        protocol: TCP
        targetPort: 6379
      selector:
        name: system-redis
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      name: system-redis
    spec:
      replicas: 1
      selector:
        name: system-redis
      strategy:
        type: Recreate
      template:
        metadata:
          labels:
            name: system-redis
        spec:
          containers:
          - args: null
            image: ${REDIS_IMAGE}
            imagePullPolicy: IfNotPresent
            livenessProbe:
              initialDelaySeconds: 10
              periodSeconds: 5
              tcpSocket:
                port: 6379
            name: system-redis
            readinessProbe:
              exec:
                command:
                - container-entrypoint
                - bash
                - -c
                - redis-cli set liveness-probe "`date`" | grep OK
              initialDelaySeconds: 30
              periodSeconds: 10
              timeoutSeconds: 5
            terminationMessagePath: /dev/termination-log
            volumeMounts:
            - mountPath: /var/lib/redis/data
              name: system-redis-storage
            - mountPath: /etc/redis.conf
              name: redis-config
              subPath: redis.conf
          volumes:
          - name: system-redis-storage
            persistentVolumeClaim:
              claimName: system-redis-storage
          - configMap:
              items:
              - key: redis.conf
                path: redis.conf
              name: redis-config
            name: redis-config
      triggers:
      - type: ConfigChange
  - apiVersion: v1
    kind: Service
    metadata:
      name: system-sphinx
    spec:
      ports:
      - name: sphinx
        port: 9306
        protocol: TCP
        targetPort: 9306
      selector:
        name: system-sphinx
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      name: system-sphinx
    spec:
      replicas: 1
      selector:
        name: system-sphinx
      strategy:
        rollingParams:
          intervalSeconds: 1
          maxSurge: 25%
          maxUnavailable: 25%
          timeoutSeconds: 600
          updatePeriodSeconds: 1
        type: Rolling
      template:
        metadata:
          labels:
            name: system-sphinx
        spec:
          containers:
          - args:
            - rake
            - openshift:thinking_sphinx:start
            env:
            - name: RAILS_ENV
              value: production
            - name: DATABASE_URL
              value: mysql2://root:${MYSQL_ROOT_PASSWORD}@system-mysql/${MYSQL_DATABASE}
            - name: THINKING_SPHINX_ADDRESS
              value: 0.0.0.0
            - name: THINKING_SPHINX_CONFIGURATION_FILE
              value: db/sphinx/production.conf
            - name: THINKING_SPHINX_PID_FILE
              value: db/sphinx/searchd.pid
            - name: DELTA_INDEX_INTERVAL
              value: "5"
            - name: FULL_REINDEX_INTERVAL
              value: "60"
            image: 3scale-amp20/system:1.0-2
            imagePullPolicy: IfNotPresent
            livenessProbe:
              initialDelaySeconds: 60
              periodSeconds: 10
              tcpSocket:
                port: 9306
            name: system-sphinx
            volumeMounts:
            - mountPath: /opt/system/db/sphinx
              name: system-sphinx-database
          volumes:
          - emptyDir: {}
            name: system-sphinx-database
      triggers:
      - type: ConfigChange
  - apiVersion: v1
    kind: Service
    metadata:
      name: system-memcache
    spec:
      ports:
      - name: memcache
        port: 11211
        protocol: TCP
        targetPort: 11211
      selector:
        name: system-memcache
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      name: system-memcache
    spec:
      replicas: 1
      selector:
        name: system-memcache
      strategy:
        rollingParams:
          intervalSeconds: 1
          maxSurge: 25%
          maxUnavailable: 25%
          timeoutSeconds: 600
          updatePeriodSeconds: 1
        type: Rolling
      template:
        metadata:
          labels:
            name: system-memcache
        spec:
          containers:
          - args: null
            command:
            - memcached
            - -m
            - "64"
            env: null
            image: 3scale-amp20/memcached:1.4.15-7
            imagePullPolicy: IfNotPresent
            livenessProbe:
              initialDelaySeconds: 10
              periodSeconds: 10
              tcpSocket:
                port: 11211
            name: memcache
            readinessProbe:
              exec:
                command:
                - sh
                - -c
                - echo version | nc $HOSTNAME 11211 | grep VERSION
              initialDelaySeconds: 10
              periodSeconds: 30
              timeoutSeconds: 5
          ports:
          - containerPort: 6379
            protocol: TCP
      triggers:
      - type: ConfigChange
  - apiVersion: v1
    kind: Route
    metadata:
      labels:
        app: system-route
      name: system-provider-admin-route
    spec:
      host: ${TENANT_NAME}-admin.${WILDCARD_DOMAIN}
      port:
        targetPort: http
      tls:
        insecureEdgeTerminationPolicy: Allow
        termination: edge
      to:
        kind: Service
        name: system-provider
  - apiVersion: v1
    kind: Route
    metadata:
      labels:
        app: system-route
      name: backend-route
    spec:
      host: backend-${TENANT_NAME}.${WILDCARD_DOMAIN}
      port:
        targetPort: http
      tls:
        insecureEdgeTerminationPolicy: Allow
        termination: edge
      to:
        kind: Service
        name: backend-listener
  - apiVersion: v1
    kind: Route
    metadata:
      labels:
        app: system-route
      name: system-developer-route
    spec:
      host: ${TENANT_NAME}.${WILDCARD_DOMAIN}
      port:
        targetPort: http
      tls:
        insecureEdgeTerminationPolicy: Allow
        termination: edge
      to:
        kind: Service
        name: system-developer
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      name: apicast-staging
    spec:
      replicas: 1
      selector:
        deploymentconfig: apicast-staging
      strategy:
        rollingParams:
          intervalSeconds: 1
          maxSurge: 25%
          maxUnavailable: 25%
          timeoutSeconds: 1800
          updatePeriodSeconds: 1
        type: Rolling
      template:
        metadata:
          labels:
            deploymentconfig: apicast-staging
        spec:
          containers:
          - env:
            - name: THREESCALE_PORTAL_ENDPOINT
              value: http://${APICAST_ACCESS_TOKEN}@system-provider:3000
            - name: APICAST_CONFIGURATION_LOADER
              value: lazy
            - name: APICAST_CONFIGURATION_CACHE
              value: "0"
            - name: THREESCALE_DEPLOYMENT_ENV
              value: sandbox
            - name: APICAST_MANAGEMENT_API
              value: ${APICAST_MANAGEMENT_API}
            - name: BACKEND_ENDPOINT_OVERRIDE
              value: http://backend-listener:3000
            - name: OPENSSL_VERIFY
              value: ${APICAST_OPENSSL_VERIFY}
            - name: APICAST_RESPONSE_CODES
              value: ${APICAST_RESPONSE_CODES}
            - name: REDIS_URL
              value: redis://system-redis:6379/2
            image: 3scale-amp20/apicast-gateway:1.0-3
            imagePullPolicy: IfNotPresent
            livenessProbe:
              httpGet:
                path: /status/live
                port: 8090
              initialDelaySeconds: 10
              periodSeconds: 10
              timeoutSeconds: 5
            name: apicast-staging
            ports:
            - containerPort: 8080
              protocol: TCP
            - containerPort: 8090
              protocol: TCP
            readinessProbe:
              httpGet:
                path: /status/ready
                port: 8090
              initialDelaySeconds: 15
              periodSeconds: 30
              timeoutSeconds: 5
      triggers:
      - type: ConfigChange
  - apiVersion: v1
    kind: Service
    metadata:
      name: apicast-staging
    spec:
      ports:
      - name: gateway
        port: 8080
        protocol: TCP
        targetPort: 8080
      - name: management
        port: 8090
        protocol: TCP
        targetPort: 8090
      selector:
        deploymentconfig: apicast-staging
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      name: apicast-production
    spec:
      replicas: 1
      selector:
        deploymentconfig: apicast-production
      strategy:
        rollingParams:
          intervalSeconds: 1
          maxSurge: 25%
          maxUnavailable: 25%
          timeoutSeconds: 1800
          updatePeriodSeconds: 1
        type: Rolling
      template:
        metadata:
          labels:
            deploymentconfig: apicast-production
        spec:
          containers:
          - env:
            - name: THREESCALE_PORTAL_ENDPOINT
              value: http://${APICAST_ACCESS_TOKEN}@system-provider:3000
            - name: APICAST_CONFIGURATION_LOADER
              value: boot
            - name: APICAST_CONFIGURATION_CACHE
              value: "300"
            - name: THREESCALE_DEPLOYMENT_ENV
              value: production
            - name: APICAST_MANAGEMENT_API
              value: ${APICAST_MANAGEMENT_API}
            - name: BACKEND_ENDPOINT_OVERRIDE
              value: http://backend-listener:3000
            - name: OPENSSL_VERIFY
              value: ${APICAST_OPENSSL_VERIFY}
            - name: APICAST_RESPONSE_CODES
              value: ${APICAST_RESPONSE_CODES}
            - name: REDIS_URL
              value: redis://system-redis:6379/1
            image: 3scale-amp20/apicast-gateway:1.0-3
            imagePullPolicy: IfNotPresent
            livenessProbe:
              httpGet:
                path: /status/live
                port: 8090
              initialDelaySeconds: 10
              periodSeconds: 10
              timeoutSeconds: 5
            name: apicast-production
            ports:
            - containerPort: 8080
              protocol: TCP
            - containerPort: 8090
              protocol: TCP
            readinessProbe:
              httpGet:
                path: /status/ready
                port: 8090
              initialDelaySeconds: 15
              periodSeconds: 30
              timeoutSeconds: 5
      triggers:
      - type: ConfigChange
  - apiVersion: v1
    kind: Service
    metadata:
      name: apicast-production
    spec:
      ports:
      - name: gateway
        port: 8080
        protocol: TCP
        targetPort: 8080
      - name: management
        port: 8090
        protocol: TCP
        targetPort: 8090
      selector:
        deploymentconfig: apicast-production
  - apiVersion: v1
    kind: Route
    metadata:
      labels:
        app: apicast-staging
      name: api-apicast-staging-route
    spec:
      host: api-${TENANT_NAME}-apicast-staging.${WILDCARD_DOMAIN}
      port:
        targetPort: gateway
      tls:
        insecureEdgeTerminationPolicy: Allow
        termination: edge
      to:
        kind: Service
        name: apicast-staging
  - apiVersion: v1
    kind: Route
    metadata:
      labels:
        app: apicast-production
      name: api-apicast-production-route
    spec:
      host: api-${TENANT_NAME}-apicast-production.${WILDCARD_DOMAIN}
      port:
        targetPort: gateway
      tls:
        insecureEdgeTerminationPolicy: Allow
        termination: edge
      to:
        kind: Service
        name: apicast-production
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      name: system-app
    spec:
      replicas: 1
      selector:
        name: system-app
      strategy:
        rollingParams:
          intervalSeconds: 1
          maxSurge: 25%
          maxUnavailable: 25%
          post:
            execNewPod:
              command:
              - bash
              - -c
              - bundle exec rake boot openshift:post_deploy
              containerName: system-provider
            failurePolicy: Abort
          pre:
            execNewPod:
              command:
              - bash
              - -c
              - bundle exec rake boot openshift:deploy
              containerName: system-provider
              env:
              - name: RAILS_ENV
                value: production
              - name: DATABASE_URL
                value: mysql2://root:${MYSQL_ROOT_PASSWORD}@system-mysql/${MYSQL_DATABASE}
              - name: FORCE_SSL
                value: "true"
              - name: THREESCALE_SUPERDOMAIN
                value: ${WILDCARD_DOMAIN}
              - name: TENANT_NAME
                value: ${TENANT_NAME}
              - name: APICAST_ACCESS_TOKEN
                value: ${APICAST_ACCESS_TOKEN}
              - name: ADMIN_ACCESS_TOKEN
                value: ${ADMIN_ACCESS_TOKEN}
              - name: PROVIDER_PLAN
                value: enterprise
              - name: USER_LOGIN
                value: ${ADMIN_USERNAME}
              - name: USER_PASSWORD
                value: ${ADMIN_PASSWORD}
              - name: RAILS_LOG_TO_STDOUT
                value: "true"
              - name: RAILS_LOG_LEVEL
                value: info
              - name: THINKING_SPHINX_ADDRESS
                value: system-sphinx
              - name: THINKING_SPHINX_PORT
                value: "9306"
              - name: THINKING_SPHINX_CONFIGURATION_FILE
                value: /tmp/sphinx.conf
              - name: EVENTS_SHARED_SECRET
                value: ${SYSTEM_BACKEND_SHARED_SECRET}
              - name: THREESCALE_SANDBOX_PROXY_OPENSSL_VERIFY_MODE
                value: VERIFY_NONE
              - name: APICAST_BACKEND_ROOT_ENDPOINT
                value: https://backend-${TENANT_NAME}.${WILDCARD_DOMAIN}
              - name: CONFIG_INTERNAL_API_USER
                value: ${SYSTEM_BACKEND_USERNAME}
              - name: CONFIG_INTERNAL_API_PASSWORD
                value: ${SYSTEM_BACKEND_PASSWORD}
              - name: SECRET_KEY_BASE
                value: ${SYSTEM_APP_SECRET_KEY_BASE}
              - name: AMP_RELEASE
                value: ${AMP_RELEASE}
              - name: SMTP_ADDRESS
                valueFrom:
                  configMapKeyRef:
                    key: address
                    name: smtp
              - name: SMTP_USER_NAME
                valueFrom:
                  configMapKeyRef:
                    key: username
                    name: smtp
              - name: SMTP_PASSWORD
                valueFrom:
                  configMapKeyRef:
                    key: password
                    name: smtp
              - name: SMTP_DOMAIN
                valueFrom:
                  configMapKeyRef:
                    key: domain
                    name: smtp
              - name: SMTP_PORT
                valueFrom:
                  configMapKeyRef:
                    key: port
                    name: smtp
              - name: SMTP_AUTHENTICATION
                valueFrom:
                  configMapKeyRef:
                    key: authentication
                    name: smtp
              - name: SMTP_OPENSSL_VERIFY_MODE
                valueFrom:
                  configMapKeyRef:
                    key: openssl.verify.mode
                    name: smtp
              - name: BACKEND_ROUTE
                value: https://backend-${TENANT_NAME}.${WILDCARD_DOMAIN}
              volumes:
              - system-storage
            failurePolicy: Retry
          timeoutSeconds: 600
          updatePeriodSeconds: 1
        type: Rolling
      template:
        metadata:
          labels:
            name: system-app
        spec:
          containers:
          - args: null
            command:
            - env
            - TENANT_MODE=provider
            - PORT=3000
            - container-entrypoint
            - bundle
            - exec
            - unicorn
            - -c
            - config/unicorn.rb
            env:
            - name: RAILS_ENV
              value: production
            - name: DATABASE_URL
              value: mysql2://root:${MYSQL_ROOT_PASSWORD}@system-mysql/${MYSQL_DATABASE}
            - name: FORCE_SSL
              value: "true"
            - name: THREESCALE_SUPERDOMAIN
              value: ${WILDCARD_DOMAIN}
            - name: TENANT_NAME
              value: ${TENANT_NAME}
            - name: APICAST_ACCESS_TOKEN
              value: ${APICAST_ACCESS_TOKEN}
            - name: ADMIN_ACCESS_TOKEN
              value: ${ADMIN_ACCESS_TOKEN}
            - name: PROVIDER_PLAN
              value: enterprise
            - name: USER_LOGIN
              value: ${ADMIN_USERNAME}
            - name: USER_PASSWORD
              value: ${ADMIN_PASSWORD}
            - name: RAILS_LOG_TO_STDOUT
              value: "true"
            - name: RAILS_LOG_LEVEL
              value: info
            - name: THINKING_SPHINX_ADDRESS
              value: system-sphinx
            - name: THINKING_SPHINX_PORT
              value: "9306"
            - name: THINKING_SPHINX_CONFIGURATION_FILE
              value: /tmp/sphinx.conf
            - name: EVENTS_SHARED_SECRET
              value: ${SYSTEM_BACKEND_SHARED_SECRET}
            - name: THREESCALE_SANDBOX_PROXY_OPENSSL_VERIFY_MODE
              value: VERIFY_NONE
            - name: APICAST_BACKEND_ROOT_ENDPOINT
              value: https://backend-${TENANT_NAME}.${WILDCARD_DOMAIN}
            - name: CONFIG_INTERNAL_API_USER
              value: ${SYSTEM_BACKEND_USERNAME}
            - name: CONFIG_INTERNAL_API_PASSWORD
              value: ${SYSTEM_BACKEND_PASSWORD}
            - name: SECRET_KEY_BASE
              value: ${SYSTEM_APP_SECRET_KEY_BASE}
            - name: AMP_RELEASE
              value: ${AMP_RELEASE}
            - name: SMTP_ADDRESS
              valueFrom:
                configMapKeyRef:
                  key: address
                  name: smtp
            - name: SMTP_USER_NAME
              valueFrom:
                configMapKeyRef:
                  key: username
                  name: smtp
            - name: SMTP_PASSWORD
              valueFrom:
                configMapKeyRef:
                  key: password
                  name: smtp
            - name: SMTP_DOMAIN
              valueFrom:
                configMapKeyRef:
                  key: domain
                  name: smtp
            - name: SMTP_PORT
              valueFrom:
                configMapKeyRef:
                  key: port
                  name: smtp
            - name: SMTP_AUTHENTICATION
              valueFrom:
                configMapKeyRef:
                  key: authentication
                  name: smtp
            - name: SMTP_OPENSSL_VERIFY_MODE
              valueFrom:
                configMapKeyRef:
                  key: openssl.verify.mode
                  name: smtp
            - name: BACKEND_ROUTE
              value: https://backend-${TENANT_NAME}.${WILDCARD_DOMAIN}
            image: 3scale-amp20/system:1.0-2
            imagePullPolicy: IfNotPresent
            livenessProbe:
              initialDelaySeconds: 20
              periodSeconds: 10
              tcpSocket:
                port: provider
              timeoutSeconds: 10
            name: system-provider
            ports:
            - containerPort: 3000
              name: provider
              protocol: TCP
            readinessProbe:
              httpGet:
                httpHeaders:
                - name: X-Forwarded-Proto
                  value: https
                path: /check.txt
                port: provider
                scheme: HTTP
              initialDelaySeconds: 30
              periodSeconds: 30
              timeoutSeconds: 10
            volumeMounts:
            - mountPath: /opt/system/public/system
              name: system-storage
          - args: null
            command:
            - env
            - TENANT_MODE=developer
            - PORT=3001
            - container-entrypoint
            - bundle
            - exec
            - unicorn
            - -c
            - config/unicorn.rb
            env:
            - name: RAILS_ENV
              value: production
            - name: DATABASE_URL
              value: mysql2://root:${MYSQL_ROOT_PASSWORD}@system-mysql/${MYSQL_DATABASE}
            - name: FORCE_SSL
              value: "true"
            - name: THREESCALE_SUPERDOMAIN
              value: ${WILDCARD_DOMAIN}
            - name: TENANT_NAME
              value: ${TENANT_NAME}
            - name: APICAST_ACCESS_TOKEN
              value: ${APICAST_ACCESS_TOKEN}
            - name: ADMIN_ACCESS_TOKEN
              value: ${ADMIN_ACCESS_TOKEN}
            - name: PROVIDER_PLAN
              value: enterprise
            - name: USER_LOGIN
              value: ${ADMIN_USERNAME}
            - name: USER_PASSWORD
              value: ${ADMIN_PASSWORD}
            - name: RAILS_LOG_TO_STDOUT
              value: "true"
            - name: RAILS_LOG_LEVEL
              value: info
            - name: THINKING_SPHINX_ADDRESS
              value: system-sphinx
            - name: THINKING_SPHINX_PORT
              value: "9306"
            - name: THINKING_SPHINX_CONFIGURATION_FILE
              value: /tmp/sphinx.conf
            - name: EVENTS_SHARED_SECRET
              value: ${SYSTEM_BACKEND_SHARED_SECRET}
            - name: THREESCALE_SANDBOX_PROXY_OPENSSL_VERIFY_MODE
              value: VERIFY_NONE
            - name: APICAST_BACKEND_ROOT_ENDPOINT
              value: https://backend-${TENANT_NAME}.${WILDCARD_DOMAIN}
            - name: CONFIG_INTERNAL_API_USER
              value: ${SYSTEM_BACKEND_USERNAME}
            - name: CONFIG_INTERNAL_API_PASSWORD
              value: ${SYSTEM_BACKEND_PASSWORD}
            - name: SECRET_KEY_BASE
              value: ${SYSTEM_APP_SECRET_KEY_BASE}
            - name: AMP_RELEASE
              value: ${AMP_RELEASE}
            - name: SMTP_ADDRESS
              valueFrom:
                configMapKeyRef:
                  key: address
                  name: smtp
            - name: SMTP_USER_NAME
              valueFrom:
                configMapKeyRef:
                  key: username
                  name: smtp
            - name: SMTP_PASSWORD
              valueFrom:
                configMapKeyRef:
                  key: password
                  name: smtp
            - name: SMTP_DOMAIN
              valueFrom:
                configMapKeyRef:
                  key: domain
                  name: smtp
            - name: SMTP_PORT
              valueFrom:
                configMapKeyRef:
                  key: port
                  name: smtp
            - name: SMTP_AUTHENTICATION
              valueFrom:
                configMapKeyRef:
                  key: authentication
                  name: smtp
            - name: SMTP_OPENSSL_VERIFY_MODE
              valueFrom:
                configMapKeyRef:
                  key: openssl.verify.mode
                  name: smtp
            - name: BACKEND_ROUTE
              value: https://backend-${TENANT_NAME}.${WILDCARD_DOMAIN}
            image: 3scale-amp20/system:1.0-2
            imagePullPolicy: IfNotPresent
            livenessProbe:
              initialDelaySeconds: 20
              periodSeconds: 10
              tcpSocket:
                port: developer
              timeoutSeconds: 10
            name: system-developer
            ports:
            - containerPort: 3001
              name: developer
              protocol: TCP
            readinessProbe:
              httpGet:
                httpHeaders:
                - name: X-Forwarded-Proto
                  value: https
                path: /check.txt
                port: developer
                scheme: HTTP
              initialDelaySeconds: 30
              periodSeconds: 30
              timeoutSeconds: 10
            volumeMounts:
            - mountPath: /opt/system/public/system
              name: system-storage
              readOnly: true
          volumes:
          - name: system-storage
            persistentVolumeClaim:
              claimName: system-storage
      triggers:
      - type: ConfigChange
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      name: system-resque
    spec:
      replicas: 1
      selector:
        name: system-resque
      strategy:
        rollingParams:
          intervalSeconds: 1
          maxSurge: 25%
          maxUnavailable: 25%
          timeoutSeconds: 600
          updatePeriodSeconds: 1
        type: Rolling
      template:
        metadata:
          labels:
            name: system-resque
        spec:
          containers:
          - args:
            - rake
            - resque:work
            - QUEUE=*
            env:
            - name: RAILS_ENV
              value: production
            - name: DATABASE_URL
              value: mysql2://root:${MYSQL_ROOT_PASSWORD}@system-mysql/${MYSQL_DATABASE}
            - name: FORCE_SSL
              value: "true"
            - name: THREESCALE_SUPERDOMAIN
              value: ${WILDCARD_DOMAIN}
            - name: TENANT_NAME
              value: ${TENANT_NAME}
            - name: APICAST_ACCESS_TOKEN
              value: ${APICAST_ACCESS_TOKEN}
            - name: ADMIN_ACCESS_TOKEN
              value: ${ADMIN_ACCESS_TOKEN}
            - name: PROVIDER_PLAN
              value: enterprise
            - name: USER_LOGIN
              value: ${ADMIN_USERNAME}
            - name: USER_PASSWORD
              value: ${ADMIN_PASSWORD}
            - name: RAILS_LOG_TO_STDOUT
              value: "true"
            - name: RAILS_LOG_LEVEL
              value: info
            - name: THINKING_SPHINX_ADDRESS
              value: system-sphinx
            - name: THINKING_SPHINX_PORT
              value: "9306"
            - name: THINKING_SPHINX_CONFIGURATION_FILE
              value: /tmp/sphinx.conf
            - name: EVENTS_SHARED_SECRET
              value: ${SYSTEM_BACKEND_SHARED_SECRET}
            - name: THREESCALE_SANDBOX_PROXY_OPENSSL_VERIFY_MODE
              value: VERIFY_NONE
            - name: APICAST_BACKEND_ROOT_ENDPOINT
              value: https://backend-${TENANT_NAME}.${WILDCARD_DOMAIN}
            - name: CONFIG_INTERNAL_API_USER
              value: ${SYSTEM_BACKEND_USERNAME}
            - name: CONFIG_INTERNAL_API_PASSWORD
              value: ${SYSTEM_BACKEND_PASSWORD}
            - name: SECRET_KEY_BASE
              value: ${SYSTEM_APP_SECRET_KEY_BASE}
            - name: AMP_RELEASE
              value: ${AMP_RELEASE}
            - name: SMTP_ADDRESS
              valueFrom:
                configMapKeyRef:
                  key: address
                  name: smtp
            - name: SMTP_USER_NAME
              valueFrom:
                configMapKeyRef:
                  key: username
                  name: smtp
            - name: SMTP_PASSWORD
              valueFrom:
                configMapKeyRef:
                  key: password
                  name: smtp
            - name: SMTP_DOMAIN
              valueFrom:
                configMapKeyRef:
                  key: domain
                  name: smtp
            - name: SMTP_PORT
              valueFrom:
                configMapKeyRef:
                  key: port
                  name: smtp
            - name: SMTP_AUTHENTICATION
              valueFrom:
                configMapKeyRef:
                  key: authentication
                  name: smtp
            - name: SMTP_OPENSSL_VERIFY_MODE
              valueFrom:
                configMapKeyRef:
                  key: openssl.verify.mode
                  name: smtp
            - name: BACKEND_ROUTE
              value: https://backend-${TENANT_NAME}.${WILDCARD_DOMAIN}
            image: 3scale-amp20/system:1.0-2
            imagePullPolicy: IfNotPresent
            name: system-resque
            volumeMounts:
            - mountPath: /opt/system/public/system
              name: system-storage
          - args:
            - rake
            - resque:scheduler
            - QUEUE=*
            env:
            - name: RAILS_ENV
              value: production
            - name: DATABASE_URL
              value: mysql2://root:${MYSQL_ROOT_PASSWORD}@system-mysql/${MYSQL_DATABASE}
            - name: FORCE_SSL
              value: "true"
            - name: THREESCALE_SUPERDOMAIN
              value: ${WILDCARD_DOMAIN}
            - name: TENANT_NAME
              value: ${TENANT_NAME}
            - name: APICAST_ACCESS_TOKEN
              value: ${APICAST_ACCESS_TOKEN}
            - name: ADMIN_ACCESS_TOKEN
              value: ${ADMIN_ACCESS_TOKEN}
            - name: PROVIDER_PLAN
              value: enterprise
            - name: USER_LOGIN
              value: ${ADMIN_USERNAME}
            - name: USER_PASSWORD
              value: ${ADMIN_PASSWORD}
            - name: RAILS_LOG_TO_STDOUT
              value: "true"
            - name: RAILS_LOG_LEVEL
              value: info
            - name: THINKING_SPHINX_ADDRESS
              value: system-sphinx
            - name: THINKING_SPHINX_PORT
              value: "9306"
            - name: THINKING_SPHINX_CONFIGURATION_FILE
              value: /tmp/sphinx.conf
            - name: EVENTS_SHARED_SECRET
              value: ${SYSTEM_BACKEND_SHARED_SECRET}
            - name: THREESCALE_SANDBOX_PROXY_OPENSSL_VERIFY_MODE
              value: VERIFY_NONE
            - name: APICAST_BACKEND_ROOT_ENDPOINT
              value: https://backend-${TENANT_NAME}.${WILDCARD_DOMAIN}
            - name: CONFIG_INTERNAL_API_USER
              value: ${SYSTEM_BACKEND_USERNAME}
            - name: CONFIG_INTERNAL_API_PASSWORD
              value: ${SYSTEM_BACKEND_PASSWORD}
            - name: SECRET_KEY_BASE
              value: ${SYSTEM_APP_SECRET_KEY_BASE}
            - name: AMP_RELEASE
              value: ${AMP_RELEASE}
            - name: SMTP_ADDRESS
              valueFrom:
                configMapKeyRef:
                  key: address
                  name: smtp
            - name: SMTP_USER_NAME
              valueFrom:
                configMapKeyRef:
                  key: username
                  name: smtp
            - name: SMTP_PASSWORD
              valueFrom:
                configMapKeyRef:
                  key: password
                  name: smtp
            - name: SMTP_DOMAIN
              valueFrom:
                configMapKeyRef:
                  key: domain
                  name: smtp
            - name: SMTP_PORT
              valueFrom:
                configMapKeyRef:
                  key: port
                  name: smtp
            - name: SMTP_AUTHENTICATION
              valueFrom:
                configMapKeyRef:
                  key: authentication
                  name: smtp
            - name: SMTP_OPENSSL_VERIFY_MODE
              valueFrom:
                configMapKeyRef:
                  key: openssl.verify.mode
                  name: smtp
            - name: BACKEND_ROUTE
              value: https://backend-${TENANT_NAME}.${WILDCARD_DOMAIN}
            image: 3scale-amp20/system:1.0-2
            imagePullPolicy: IfNotPresent
            name: system-scheduler
          volumes:
          - name: system-storage
            persistentVolumeClaim:
              claimName: system-storage
      triggers:
      - type: ConfigChange
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      name: system-sidekiq
    spec:
      replicas: 1
      selector:
        name: system-sidekiq
      strategy:
        rollingParams:
          intervalSeconds: 1
          maxSurge: 25%
          maxUnavailable: 25%
          timeoutSeconds: 600
          updatePeriodSeconds: 1
        type: Rolling
      template:
        metadata:
          labels:
            name: system-sidekiq
        spec:
          containers:
          - args:
            - rake
            - sidekiq:worker
            env:
            - name: RAILS_ENV
              value: production
            - name: DATABASE_URL
              value: mysql2://root:${MYSQL_ROOT_PASSWORD}@system-mysql/${MYSQL_DATABASE}
            - name: FORCE_SSL
              value: "true"
            - name: THREESCALE_SUPERDOMAIN
              value: ${WILDCARD_DOMAIN}
            - name: TENANT_NAME
              value: ${TENANT_NAME}
            - name: APICAST_ACCESS_TOKEN
              value: ${APICAST_ACCESS_TOKEN}
            - name: ADMIN_ACCESS_TOKEN
              value: ${ADMIN_ACCESS_TOKEN}
            - name: PROVIDER_PLAN
              value: enterprise
            - name: USER_LOGIN
              value: ${ADMIN_USERNAME}
            - name: USER_PASSWORD
              value: ${ADMIN_PASSWORD}
            - name: RAILS_LOG_TO_STDOUT
              value: "true"
            - name: RAILS_LOG_LEVEL
              value: info
            - name: THINKING_SPHINX_ADDRESS
              value: system-sphinx
            - name: THINKING_SPHINX_PORT
              value: "9306"
            - name: THINKING_SPHINX_CONFIGURATION_FILE
              value: /tmp/sphinx.conf
            - name: EVENTS_SHARED_SECRET
              value: ${SYSTEM_BACKEND_SHARED_SECRET}
            - name: THREESCALE_SANDBOX_PROXY_OPENSSL_VERIFY_MODE
              value: VERIFY_NONE
            - name: APICAST_BACKEND_ROOT_ENDPOINT
              value: https://backend-${TENANT_NAME}.${WILDCARD_DOMAIN}
            - name: CONFIG_INTERNAL_API_USER
              value: ${SYSTEM_BACKEND_USERNAME}
            - name: CONFIG_INTERNAL_API_PASSWORD
              value: ${SYSTEM_BACKEND_PASSWORD}
            - name: SECRET_KEY_BASE
              value: ${SYSTEM_APP_SECRET_KEY_BASE}
            - name: AMP_RELEASE
              value: ${AMP_RELEASE}
            - name: SMTP_ADDRESS
              valueFrom:
                configMapKeyRef:
                  key: address
                  name: smtp
            - name: SMTP_USER_NAME
              valueFrom:
                configMapKeyRef:
                  key: username
                  name: smtp
            - name: SMTP_PASSWORD
              valueFrom:
                configMapKeyRef:
                  key: password
                  name: smtp
            - name: SMTP_DOMAIN
              valueFrom:
                configMapKeyRef:
                  key: domain
                  name: smtp
            - name: SMTP_PORT
              valueFrom:
                configMapKeyRef:
                  key: port
                  name: smtp
            - name: SMTP_AUTHENTICATION
              valueFrom:
                configMapKeyRef:
                  key: authentication
                  name: smtp
            - name: SMTP_OPENSSL_VERIFY_MODE
              valueFrom:
                configMapKeyRef:
                  key: openssl.verify.mode
                  name: smtp
            - name: BACKEND_ROUTE
              value: https://backend-${TENANT_NAME}.${WILDCARD_DOMAIN}
            image: 3scale-amp20/system:1.0-2
            imagePullPolicy: IfNotPresent
            name: system-sidekiq
            volumeMounts:
            - mountPath: /opt/system/public/system
              name: system-storage
          volumes:
          - name: system-storage
            persistentVolumeClaim:
              claimName: system-storage
      triggers:
      - type: ConfigChange
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      name: system-mysql
    spec:
      replicas: 1
      selector:
        name: system-mysql
      strategy:
        type: Recreate
      template:
        metadata:
          labels:
            name: system-mysql
        spec:
          containers:
          - env:
            - name: MYSQL_USER
              value: ${MYSQL_USER}
            - name: MYSQL_PASSWORD
              value: ${MYSQL_PASSWORD}
            - name: MYSQL_DATABASE
              value: ${MYSQL_DATABASE}
            - name: MYSQL_ROOT_PASSWORD
              value: ${MYSQL_ROOT_PASSWORD}
            - name: MYSQL_LOWER_CASE_TABLE_NAMES
              value: "1"
            image: ${MYSQL_IMAGE}
            imagePullPolicy: IfNotPresent
            livenessProbe:
              initialDelaySeconds: 30
              periodSeconds: 10
              tcpSocket:
                port: 3306
            name: system-mysql
            ports:
            - containerPort: 3306
              protocol: TCP
            readinessProbe:
              exec:
                command:
                - /bin/sh
                - -i
                - -c
                - MYSQL_PWD="$MYSQL_PASSWORD" mysql -h 127.0.0.1 -u $MYSQL_USER -D
                  $MYSQL_DATABASE -e 'SELECT 1'
              initialDelaySeconds: 10
              periodSeconds: 30
              timeoutSeconds: 5
            resources:
              limits:
                memory: 2Gi
              requests:
                cpu: "1"
                memory: 1Gi
            volumeMounts:
            - mountPath: /var/lib/mysql/data
              name: mysql-storage
          volumes:
          - name: mysql-storage
            persistentVolumeClaim:
              claimName: mysql-storage
      triggers:
      - type: ConfigChange
  - apiVersion: v1
    data:
      redis.conf: |
        protected-mode no

        port 6379

        timeout 0
        tcp-keepalive 300

        daemonize no
        supervised no

        loglevel notice

        databases 16

        save 900 1
        save 300 10
        save 60 10000

        stop-writes-on-bgsave-error yes

        rdbcompression yes
        rdbchecksum yes

        dbfilename dump.rdb

        slave-serve-stale-data yes
        slave-read-only yes

        repl-diskless-sync no
        repl-disable-tcp-nodelay no

        appendonly yes
        appendfilename "appendonly.aof"
        appendfsync everysec
        no-appendfsync-on-rewrite no
        auto-aof-rewrite-percentage 100
        auto-aof-rewrite-min-size 64mb
        aof-load-truncated yes

        lua-time-limit 5000

        activerehashing no

        aof-rewrite-incremental-fsync yes
        dir /var/lib/redis/data
    kind: ConfigMap
    metadata:
      name: redis-config
  - apiVersion: v1
    data:
      address: ""
      authentication: ""
      domain: ""
      openssl.verify.mode: ""
      password: ""
      port: ""
      username: ""
    kind: ConfigMap
    metadata:
      name: smtp
  parameters:
  - description: AMP release tag.
    name: AMP_RELEASE
    required: true
    value: 2.0.0-CR2-redhat-1
  - from: '[a-z0-9]{8}'
    generate: expression
    name: ADMIN_PASSWORD
    required: true
  - name: ADMIN_USERNAME
    required: true
    value: admin
  - description: Read Only Access Token that is APIcast going to use to download its
      configuration.
    from: '[a-z0-9]{8}'
    generate: expression
    name: APICAST_ACCESS_TOKEN
    required: true
  - description: Admin Access Token with all scopes and write permissions for API
      access.
    from: '[a-z0-9]{16}'
    generate: expression
    name: ADMIN_ACCESS_TOKEN
  - description: Root domain for the wildcard routes. Eg. example.com will generate
      3scale-admin.example.com.
    name: WILDCARD_DOMAIN
    required: true
  - description: Tenant name under the root that Admin UI will be available with -admin
      suffix.
    name: TENANT_NAME
    required: true
    value: 3scale
  - description: Username for MySQL user that will be used for accessing the database.
    displayName: MySQL User
    name: MYSQL_USER
    required: true
    value: mysql
  - description: Password for the MySQL user.
    displayName: MySQL Password
    from: '[a-z0-9]{8}'
    generate: expression
    name: MYSQL_PASSWORD
    required: true
  - description: Name of the MySQL database accessed.
    displayName: MySQL Database Name
    name: MYSQL_DATABASE
    required: true
    value: system
  - description: Password for Root user.
    displayName: MySQL Root password.
    from: '[a-z0-9]{8}'
    generate: expression
    name: MYSQL_ROOT_PASSWORD
    required: true
  - description: Internal 3scale API username for internal 3scale api auth.
    name: SYSTEM_BACKEND_USERNAME
    required: true
    value: 3scale_api_user
  - description: Internal 3scale API password for internal 3scale api auth.
    from: '[a-z0-9]{8}'
    generate: expression
    name: SYSTEM_BACKEND_PASSWORD
    required: true
  - description: Redis image to use
    name: REDIS_IMAGE
    required: true
    value: rhscl/redis-32-rhel7:3.2-5.7
  - description: Mysql image to use
    name: MYSQL_IMAGE
    required: true
    value: rhscl/mysql-56-rhel7:5.6-13.14
  - description: Shared secret to import events from backend to system.
    from: '[a-z0-9]{8}'
    generate: expression
    name: SYSTEM_BACKEND_SHARED_SECRET
    required: true
  - description: System application secret key base
    from: '[a-f0-9]{128}'
    generate: expression
    name: SYSTEM_APP_SECRET_KEY_BASE
    required: true
  - description: Scope of the APIcast Management API. Can be disabled, status or debug.
      At least status required for health checks.
    name: APICAST_MANAGEMENT_API
    value: status
  - description: Turn on/off the OpenSSL peer verification when downloading the configuration.
      Can be set to true/false.
    name: APICAST_OPENSSL_VERIFY
    value: "false"
  - description: Enable logging response codes in APIcast.
    name: APICAST_RESPONSE_CODES
    value: "true"
- apiVersion: v1
  kind: Template
  labels:
    app: tidb
    template: tidb-template
  metadata:
    annotations:
      description: Create a TiDB cluster
      iconClass: icon-database
      openshift.io/display-name: TiDB
      tags: database,tidb
    creationTimestamp: 2017-11-13T04:36:51Z
    name: tidb
    namespace: openshift
    resourceVersion: "26206397"
    selfLink: /oapi/v1/namespaces/openshift/templates/tidb
    uid: 44b3096d-c82c-11e7-845f-ecf4bbe2f144
  objects:
  - apiVersion: v1
    kind: Service
    metadata:
      labels:
        app: ${NAME}
      name: tidb
    spec:
      clusterIP: None
      ports:
      - name: mysql
        port: 4000
        protocol: TCP
        targetPort: 4000
      - name: status
        port: 10080
        protocol: TCP
        targetPort: 10080
      selector:
        app: ${NAME}
      sessionAffinity: None
      type: ClusterIP
  - apiVersion: v1
    kind: Service
    metadata:
      labels:
        app: ${NAME}
      name: tidb-out
    spec:
      ports:
      - name: mysql
        port: 4000
        protocol: TCP
        targetPort: 4000
      - name: status
        port: 10080
        protocol: TCP
        targetPort: 10080
      selector:
        app: ${NAME}
      sessionAffinity: ClientIP
      type: LoadBalancer
  - apiVersion: apps/v1beta1
    kind: StatefulSet
    metadata:
      labels:
        app: ${NAME}
      name: ${NAME}
    spec:
      replicas: ${REPLICAS}
      selector:
        matchLabels:
          app: ${NAME}
      serviceName: tidb
      template:
        metadata:
          labels:
            app: ${NAME}
        spec:
          affinity:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: app
                      operator: In
                      values:
                      - ${NAME}
                  topologyKey: kubernetes.io/hostname
                weight: 100
          containers:
          - args:
            - -L=info
            - --store=tikv
            - --path=${SERVER_PD_CONNECT}
            env:
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
            image: ${TIDB_IMAGE}
            imagePullPolicy: Always
            livenessProbe:
              failureThreshold: 3
              initialDelaySeconds: 30
              periodSeconds: 10
              successThreshold: 1
              tcpSocket:
                port: 4000
              timeoutSeconds: 8
            name: ${NAME}
            ports:
            - containerPort: 4000
              protocol: TCP
            - containerPort: 10080
              protocol: TCP
            resources:
              limits:
                cpu: ${RESOURCE_CPU_LIMIT}
                memory: ${RESOURCE_MEMORY_LIMIT}G
              requests:
                cpu: 300m
                memory: 300M
            securityContext:
              capabilities: {}
              privileged: true
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            volumeMounts:
            - mountPath: /etc/localtime
              name: timestamp
          dnsPolicy: ClusterFirst
          restartPolicy: Always
          schedulerName: default-scheduler
          securityContext: {}
          terminationGracePeriodSeconds: 10
          volumes:
          - hostPath:
              path: /etc/localtime
            name: timestamp
  parameters:
  - description: Name of the tidb image
    name: TIDB_IMAGE
    required: true
    value: pingcap/tidb:latest
  - description: tidb name used as a service name and a selector
    name: NAME
    required: true
    value: tidb
  - description: The limits for memory resource.
    name: RESOURCE_MEMORY_LIMIT
    value: "16"
  - description: The limits for CPU resource.
    name: RESOURCE_CPU_LIMIT
    value: "8"
  - description: Number of replicas.
    name: REPLICAS
    required: true
    value: "2"
  - description: pd cluster connect
    name: SERVER_PD_CONNECT
    required: true
    value: pd-0.pd:2379,pd-1.pd:2379,pd-2.pd:2379
- apiVersion: v1
  kind: Template
  labels:
    app: tikv
    template: tidb-kv-persistent
  metadata:
    annotations:
      description: Create a TiDB-KV cluster, with persistent storage.
      iconClass: icon-database
      openshift.io/display-name: TiDB-KV (Persistent)
      tags: database,tidb-kv
    creationTimestamp: 2017-11-13T04:35:55Z
    name: tidb-kv-persistent
    namespace: openshift
    resourceVersion: "26389426"
    selfLink: /oapi/v1/namespaces/openshift/templates/tidb-kv-persistent
    uid: 231eade1-c82c-11e7-845f-ecf4bbe2f144
  objects:
  - apiVersion: v1
    data:
      tikv-config.toml: |-
        # 日志级别，可选值为：trace，debug，info，warn，error，off
        log-level = "info"
        [server]
        # 监听地址
        # addr = "127.0.0.1:20160"

        # 建议使用默认值
        # notify-capacity = 40960
        # messages-per-tick = 4096

        # gRPC 线程池大小
        grpc-concurrency = 8
        # TiKV 每个实例之间的 gRPC 连接数
        # grpc-raft-conn-num = 10

        # TiDB 过来的大部分读请求都会发送到 TiKV 的 coprocessor 进行处理，该参数用于设置
        # coprocessor 线程的个数，如果业务是读请求比较多，增加 coprocessor 的线程数，但应比系统的
        # CPU 核数小。例如：TiKV 所在的机器有 32 core，在重读的场景下甚至可以将该参数设置为 30。在没有
        # 设置该参数的情况下，TiKV 会自动将该值设置为 CPU 总核数乘以 0.8。
        # end-point-concurrency = 8

        # 可以给 TiKV 实例打标签，用于副本的调度
        # labels = {zone = "cn-east-1", host = "118", disk = "ssd"}

        [storage]
        # 数据目录
        # data-dir = "/tmp/tikv/store"

        # 通常情况下使用默认值就可以了。在导数据的情况下建议将改参数设置为 1024000。
        # scheduler-concurrency = 102400
        # 该参数控制写入线程的个数，当写入操作比较频繁的时候，需要把该参数调大。使用 top -H -p tikv-pid
        # 发现名称为 sched-worker-pool 的线程都特别忙，这个时候就需要将 scheduler-worker-pool-size
        # 参数调大，增加写线程的个数。
        # scheduler-worker-pool-size = 4

        [pd]
        # pd 的地址
        # endpoints = "127.0.0.1:2379"

        [metric]
        # 将 metrics 推送给 Prometheus pushgateway 的时间间隔
        interval = "15s"
        # Prometheus pushgateway 的地址
        address = ""
        job = "tikv"

        [raftstore]
        # 默认为 true，表示强制将数据刷到磁盘上。如果是非金融安全级别的业务场景，建议设置成 false，
        # 以便获得更高的性能。
        sync-log = true

        # Raft RocksDB 目录。默认值是 [storage.data-dir] 的 raft 子目录。
        # 如果机器上有多块磁盘，可以将 Raft RocksDB 的数据放在不同的盘上，提高 TiKV 的性能。
        # raftdb-dir = "/tmp/tikv/store/raft"

        region-max-size = "384MB"
        # region 分裂阈值
        region-split-size = "256MB"
        # 当 region 写入的数据量超过该阈值的时候，TiKV 会检查该 region 是否需要分裂。为了减少检查过程
        # 中扫描数据的成本，数据过程中可以将该值设置为32MB，正常运行状态下使用默认值即可。
        region-split-check-diff = "32MB"

        [rocksdb]
        # RocksDB 进行后台任务的最大线程数，后台任务包括 compaction 和 flush。具体 RocksDB 为什么需要进行 compaction，
        # 请参考 RocksDB 的相关资料。在写流量比较大的时候（例如导数据），建议开启更多的线程，
        # 但应小于 CPU 的核数。例如在导数据的时候，32 核 CPU 的机器，可以设置成 28。
        # max-background-jobs = 8

        # RocksDB 能够打开的最大文件句柄数。
        # max-open-files = 40960

        # RocksDB MANIFEST 文件的大小限制.             # 更详细的信息请参考：https://github.com/facebook/rocksdb/wiki/MANIFEST
        max-manifest-file-size = "20MB"

        # RocksDB write-ahead logs 目录。如果机器上有两块盘，可以将 RocksDB 的数据和 WAL 日志放在
        # 不同的盘上，提高 TiKV 的性能。
        # wal-dir = "/tmp/tikv/store"

        # 下面两个参数用于怎样处理 RocksDB 归档 WAL。
        # 更多详细信息请参考：https://github.com/facebook/rocksdb/wiki/How-to-persist-in-memory-RocksDB-database%3F
        # wal-ttl-seconds = 0
        # wal-size-limit = 0

        # RocksDB WAL 日志的最大总大小，通常情况下使用默认值就可以了。
        # max-total-wal-size = "4GB"

        # 可以通过该参数打开或者关闭 RocksDB 的统计信息。
        # enable-statistics = true

        # 开启 RocksDB compaction 过程中的预读功能，如果使用的是机械磁盘，建议该值至少为2MB。
        # compaction-readahead-size = "2MB"

        [rocksdb.defaultcf]
        # 数据块大小。RocksDB 是按照 block 为单元对数据进行压缩的，同时 block 也是缓存在 block-cache
        # 中的最小单元（类似其他数据库的 page 概念）。
        block-size = "64KB"

        # RocksDB 每一层数据的压缩方式，可选的值为：no,snappy,zlib,bzip2,lz4,lz4hc,zstd。
        # no:no:lz4:lz4:lz4:zstd:zstd 表示 level0 和 level1 不压缩，level2 到 level4 采用 lz4 压缩算法,
        # level5 和 level6 采用 zstd 压缩算法,。
        # no 表示没有压缩，lz4 是速度和压缩比较为中庸的压缩算法，zlib 的压缩比很高，对存储空间比较友
        # 好，但是压缩速度比较慢，压缩的时候需要占用较多的 CPU 资源。不同的机器需要根据 CPU 以及 IO 资
        # 源情况来配置怎样的压缩方式。例如：如果采用的压缩方式为"no:no:lz4:lz4:lz4:zstd:zstd"，在大量
        # 写入数据的情况下（导数据），发现系统的 IO 压力很大（使用 iostat 发现 %util 持续 100% 或者使
        # 用 top 命令发现 iowait 特别多），而 CPU 的资源还比较充裕，这个时候可以考虑将 level0 和
        # level1 开启压缩，用 CPU 资源换取 IO 资源。如果采用的压缩方式
        # 为"no:no:lz4:lz4:lz4:zstd:zstd"，在大量写入数据的情况下，发现系统的 IO 压力不大，但是 CPU
        # 资源已经吃光了，top -H 发现有大量的 bg 开头的线程（RocksDB 的 compaction 线程）在运行，这
        # 个时候可以考虑用 IO 资源换取 CPU 资源，将压缩方式改成"no:no:no:lz4:lz4:zstd:zstd"。总之，目
        # 的是为了最大限度地利用系统的现有资源，使 TiKV 的性能在现有的资源情况下充分发挥。
        compression-per-level = ["no", "no", "lz4", "lz4", "lz4", "zstd", "zstd"]

        # RocksDB memtable 的大小。
        write-buffer-size = "128MB"

        # 最多允许几个 memtable 存在。写入到 RocksDB 的数据首先会记录到 WAL 日志里面，然后会插入到
        # memtable 里面，当 memtable 的大小到达了 write-buffer-size 限定的大小的时候，当前的
        # memtable 会变成只读的，然后生成一个新的 memtable 接收新的写入。只读的 memtable 会被
        # RocksDB 的 flush 线程（max-background-flushes 参数能够控制 flush 线程的最大个数）
        # flush 到磁盘，成为 level0 的一个 sst 文件。当 flush 线程忙不过来，导致等待 flush 到磁盘的
        # memtable 的数量到达 max-write-buffer-number 限定的个数的时候，RocksDB 会将新的写入
        # stall 住，stall 是 RocksDB 的一种流控机制。在导数据的时候可以将 max-write-buffer-number
        # 的值设置的更大一点，例如 10。
        max-write-buffer-number = 5

        # 当 level0 的 sst 文件个数到达 level0-slowdown-writes-trigger 指定的限度的时候，
        # RocksDB 会尝试减慢写入的速度。因为 level0 的 sst 太多会导致 RocksDB 的读放大上升。
        # level0-slowdown-writes-trigger 和 level0-stop-writes-trigger 是 RocksDB 进行流控的
        # 另一个表现。当 level0 的 sst 的文件个数到达 4（默认值），level0 的 sst 文件会和 level1 中
        # 有 overlap 的 sst 文件进行 compaction，缓解读放大的问题。
        level0-slowdown-writes-trigger = 20

        # 当 level0 的 sst 文件个数到达 level0-stop-writes-trigger 指定的限度的时候，RocksDB 会
        # stall 住新的写入。
        level0-stop-writes-trigger = 36

        # 当 level1 的数据量大小达到 max-bytes-for-level-base 限定的值的时候，会触发 level1 的
        # sst 和 level2 种有 overlap 的 sst 进行 compaction。
        # 黄金定律：max-bytes-for-level-base 的设置的第一参考原则就是保证和 level0 的数据量大致相
        # 等，这样能够减少不必要的 compaction。例如压缩方式为"no:no:lz4:lz4:lz4:lz4:lz4"，那么
        # max-bytes-for-level-base 的值应该是 write-buffer-size 的大小乘以 4，因为 level0 和
        # level1 都没有压缩，而且 level0 触发 compaction 的条件是 sst 的个数到达 4（默认值）。在
        # level0 和 level1 都采取了压缩的情况下，就需要分析下 RocksDB 的日志，看一个 memtable 的压
        # 缩成一个 sst 文件的大小大概是多少，例如 32MB，那么 max-bytes-for-level-base 的建议值就应
        # 该是 32MB * 4 = 128MB。
        max-bytes-for-level-base = "512MB"

        # sst 文件的大小。level0 的 sst 文件的大小受 write-buffer-size 和 level0 采用的压缩算法的
        # 影响，target-file-size-base 参数用于控制 level1-level6 单个 sst 文件的大小。
        target-file-size-base = "32MB"

        # 在不配置该参数的情况下，TiKV 会将该值设置为系统总内存量的 40%。如果需要在单个物理机上部署多个
        # TiKV 节点，需要显式配置该参数，否则 TiKV 容易出现 OOM 的问题。
        # block-cache-size = "1GB"

        [rocksdb.writecf]
        # 保持和 rocksdb.defaultcf.compression-per-level 一致。
        compression-per-level = ["no", "no", "lz4", "lz4", "lz4", "zstd", "zstd"]

        # 保持和 rocksdb.defaultcf.write-buffer-size 一致。
        write-buffer-size = "128MB"
        max-write-buffer-number = 5
        min-write-buffer-number-to-merge = 1

        # 保持和 rocksdb.defaultcf.max-bytes-for-level-base 一致。
        max-bytes-for-level-base = "512MB"
        target-file-size-base = "32MB"

        # 在不配置该参数的情况下，TiKV 会将该值设置为系统总内存量的 15%。如果需要在单个物理机上部署多个
        # TiKV 节点，需要显式配置该参数。版本信息（MVCC）相关的数据以及索引相关的数据都记录在 write 这
        # 个 cf 里面，如果业务的场景下单表索引较多，可以将该参数设置的更大一点。
        # block-cache-size = "256MB"

        [raftdb]
        # RaftDB 能够打开的最大文件句柄数。
        # max-open-files = 40960

        # 可以通过该参数打开或者关闭 RaftDB 的统计信息。
        # enable-statistics = true

        # 开启 RaftDB compaction 过程中的预读功能，如果使用的是机械磁盘，建议该值至少为2MB。
        # compaction-readahead-size = "2MB"

        [raftdb.defaultcf]
        # 保持和 rocksdb.defaultcf.compression-per-level 一致。
        compression-per-level = ["no", "no", "lz4", "lz4", "lz4", "zstd", "zstd"]

        # 保持和 rocksdb.defaultcf.write-buffer-size 一致。
        write-buffer-size = "128MB"
        max-write-buffer-number = 5
        min-write-buffer-number-to-merge = 1

        # 保持和 rocksdb.defaultcf.max-bytes-for-level-base 一致。
        max-bytes-for-level-base = "512MB"
        target-file-size-base = "32MB"

        # 通常配置在 256MB 到 2GB 之间，通常情况下使用默认值就可以了，但如果系统资源比较充足可以适当调大点。
        block-cache-size = "256MB"
    kind: ConfigMap
    metadata:
      creationTimestamp: null
      name: tidb-kv
  - apiVersion: v1
    kind: Service
    metadata:
      labels:
        app: ${NAME}
      name: tikv
    spec:
      clusterIP: None
      ports:
      - name: peer
        port: 20160
        protocol: TCP
        targetPort: 20160
      selector:
        app: ${NAME}
      sessionAffinity: None
      type: ClusterIP
  - apiVersion: apps/v1beta1
    kind: StatefulSet
    metadata:
      labels:
        app: ${NAME}
      name: ${NAME}
    spec:
      replicas: ${REPLICAS}
      selector:
        matchLabels:
          app: ${NAME}
      serviceName: tikv
      template:
        metadata:
          labels:
            app: ${NAME}
        spec:
          affinity:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: app
                      operator: In
                      values:
                      - ${NAME}
                  topologyKey: kubernetes.io/hostname
                weight: 100
          containers:
          - args:
            - --addr=0.0.0.0:20160
            - --advertise-addr=$(NODE_NAME).tikv:20160
            - --data-dir=/data/tikv
            - --config=/tikv-config.toml
            - -L=info
            - --pd=${SERVER_PD_CONNECT}
            env:
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
            image: ${TIKV_IMAGE}
            imagePullPolicy: Always
            livenessProbe:
              failureThreshold: 3
              initialDelaySeconds: 30
              periodSeconds: 10
              successThreshold: 1
              tcpSocket:
                port: 20160
              timeoutSeconds: 8
            name: ${NAME}
            ports:
            - containerPort: 20160
              name: peer
              protocol: TCP
            resources:
              limits:
                cpu: ${RESOURCE_CPU_LIMIT}
                memory: ${RESOURCE_MEMORY_LIMIT}G
              requests:
                cpu: 300m
                memory: 300M
            securityContext:
              capabilities: {}
              privileged: true
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            volumeMounts:
            - mountPath: /data
              name: storage
            - mountPath: /etc/localtime
              name: timestamp
            - mountPath: /tikv-config.toml
              name: conf-volume
              subPath: tikv-config.toml
          dnsPolicy: ClusterFirst
          name: conf-volume
          restartPolicy: Always
          securityContext: {}
          terminationGracePeriodSeconds: 10
          volumes:
          - hostPath:
              path: /etc/localtime
            name: timestamp
          - configMap:
              items:
              - key: tikv-config.toml
                path: tikv-config.toml
            name: tidb-kv
      volumeClaimTemplates:
      - metadata:
          annotations:
            volume.alpha.kubernetes.io/storage-class: slow
          creationTimestamp: null
          name: storage
        spec:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: ${PV_SIZE}Gi
  parameters:
  - description: Name of the tikv image
    name: TIKV_IMAGE
    required: true
    value: pingcap/tikv:latest
  - description: tikv name used as a service name and a selector
    name: NAME
    required: true
    value: tikv
  - description: persistentVolume_size_GB
    name: PV_SIZE
    required: true
    value: "50"
  - description: The limits for memory resource.
    name: RESOURCE_MEMORY_LIMIT
    value: "32"
  - description: The limits for CPU resource.
    name: RESOURCE_CPU_LIMIT
    value: "8"
  - description: Number of replicas.
    name: REPLICAS
    required: true
    value: "3"
  - description: pd cluster connect
    name: SERVER_PD_CONNECT
    required: true
    value: pd-0.pd:2379,pd-1.pd:2379,pd-2.pd:2379
- apiVersion: v1
  kind: Template
  labels:
    app: pd
    template: tidb-pd-persistent
  metadata:
    annotations:
      description: Create a TiDB-PD cluster, with persistent storage.
      iconClass: icon-database
      openshift.io/display-name: TiDB-PD (Persistent)
      tags: database,tidb-pd
    creationTimestamp: 2017-11-13T04:34:30Z
    name: tidb-pd-persistent
    namespace: openshift
    resourceVersion: "26389244"
    selfLink: /oapi/v1/namespaces/openshift/templates/tidb-pd-persistent
    uid: f0e75224-c82b-11e7-845f-ecf4bbe2f144
  objects:
  - apiVersion: v1
    data:
      pd-config.toml: |-
        [log]
        level = "info"

        # log format, one of json, text, console
        format = "json"

        # disable automatic timestamps in output
        #disable-timestamp = false

        # file logging
        [log.file]
        #filename = ""
        # max log file size in MB
        #max-size = 300
        # max log file keep days
        #max-days = 28
        # maximum number of old log files to retain
        #max-backups = 7
        # rotate log by day
        #log-rotate = true

        [metric]
        # prometheus client push interval, set "0s" to disable prometheus.
        interval = "15s"
        # prometheus pushgateway address, leaves it empty will disable prometheus.
        address = ""

        [schedule]
        max-snapshot-count = 3
        max-pending-peer-count = 16
        max-store-down-time = "1h"
        leader-schedule-limit = 64
        region-schedule-limit = 16
        replica-schedule-limit = 24
        tolerant-size-ratio = 2.5

        # customized schedulers, the format is as below
        # if empty, it will use balance-leader, balance-region, hot-region as default
        # [[schedule.schedulers]]
        # type = "evict-leader"
        # args = ["1"]
        [replication]
        # The number of replicas for each region.
        max-replicas = 3
        # The label keys specified the location of a store.
        # The placement priorities is implied by the order of label keys.
        # For example, ["zone", "rack"] means that we should place replicas to
        # different zones first, then to different racks if we don't have enough zones.
        location-labels = []
    kind: ConfigMap
    metadata:
      creationTimestamp: null
      name: tidb-pd
  - apiVersion: v1
    kind: Service
    metadata:
      labels:
        app: ${NAME}
      name: pd
    spec:
      clusterIP: None
      ports:
      - name: client
        port: 2379
        protocol: TCP
        targetPort: 2379
      - name: peer
        port: 2380
        protocol: TCP
        targetPort: 2380
      selector:
        app: ${NAME}
      sessionAffinity: None
      type: ClusterIP
  - apiVersion: apps/v1beta1
    kind: StatefulSet
    metadata:
      labels:
        app: ${NAME}
      name: pd
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: ${NAME}
      serviceName: pd
      template:
        metadata:
          labels:
            app: ${NAME}
        spec:
          affinity:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: app
                      operator: In
                      values:
                      - ${NAME}
                  topologyKey: kubernetes.io/hostname
                weight: 100
          containers:
          - args:
            - --name=$(NODE_NAME)
            - --data-dir=/data/pd
            - --client-urls=http://0.0.0.0:2379
            - --advertise-client-urls=http://$(NODE_NAME).pd:2379,http://127.0.0.1:2379
            - --peer-urls=http://0.0.0.0:2380
            - --advertise-peer-urls=http://$(NODE_NAME).pd:2380
            - --config=/pd-config.toml
            - -L=info
            - --initial-cluster
            - pd-0=http://pd-0.pd:2380,pd-1=http://pd-1.pd:2380,pd-2=http://pd-2.pd:2380
            env:
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
            image: ${PD_IMAGE}
            imagePullPolicy: Always
            livenessProbe:
              failureThreshold: 3
              initialDelaySeconds: 30
              periodSeconds: 10
              successThreshold: 1
              tcpSocket:
                port: 2379
              timeoutSeconds: 8
            name: ${NAME}
            ports:
            - containerPort: 2379
              name: etcd-server
              protocol: TCP
            - containerPort: 2380
              name: peer
              protocol: TCP
            resources:
              limits:
                cpu: ${RESOURCE_CPU_LIMIT}
                memory: ${RESOURCE_MEMORY_LIMIT}G
              requests:
                cpu: 300m
                memory: 300M
            securityContext:
              capabilities: {}
              privileged: true
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            volumeMounts:
            - mountPath: /data
              name: storage
            - mountPath: /etc/localtime
              name: timestamp
            - mountPath: /pd-config.toml
              name: conf-volume
              subPath: pd-config.toml
          dnsPolicy: ClusterFirst
          restartPolicy: Always
          schedulerName: default-scheduler
          securityContext: {}
          terminationGracePeriodSeconds: 10
          volumes:
          - hostPath:
              path: /etc/localtime
            name: timestamp
          - configMap:
              items:
              - key: pd-config.toml
                path: pd-config.toml
              name: tidb-pd
            name: conf-volume
      volumeClaimTemplates:
      - metadata:
          annotations:
            volume.alpha.kubernetes.io/storage-class: slow
          creationTimestamp: null
          name: storage
        spec:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: ${PV_SIZE}Gi
  parameters:
  - description: Name of the tipd image
    name: PD_IMAGE
    required: true
    value: pingcap/pd:latest
  - description: tipd name used as a service name and a selector
    name: NAME
    required: true
    value: pd
  - description: persistentVolume_size_GB
    name: PV_SIZE
    required: true
    value: "2"
  - description: The limits for memory resource.
    name: RESOURCE_MEMORY_LIMIT
    value: "4"
  - description: The limits for CPU resource.
    name: RESOURCE_CPU_LIMIT
    value: "2"
- apiVersion: v1
  kind: Template
  labels:
    application: tomcat-gradle
    createdBy: lidt
  metadata:
    annotations:
      description: jdk8+tomcat8+gradle2.6环境
      iconClass: icon-tomcat
      tags: java,tomcat-gradle
    creationTimestamp: 2017-10-31T02:42:30Z
    name: tomcat-gradle
    namespace: openshift
    resourceVersion: "27449152"
    selfLink: /oapi/v1/namespaces/openshift/templates/tomcat-gradle
    uid: 240797da-bde5-11e7-bc85-ecf4bbe2f144
  objects:
  - apiVersion: v1
    kind: BuildConfig
    metadata:
      name: ${APPLICATION_NAME}
    spec:
      output:
        to:
          kind: ImageStreamTag
          name: ${APPLICATION_NAME}:latest
      source:
        git:
          ref: ${GIT_REF}
          uri: ${GIT_URI}
        sourceSecret:
          name: gitbasicsecret
        type: Git
      strategy:
        dockerStrategy:
          from:
            kind: DockerImage
            name: ${IMAGE_NAME}
          pullSecret:
            name: dockerhub
        type: Docker
      triggers:
      - generic:
          secret: ${Generic_TRIGGER_SECRET}
        type: Generic
  - apiVersion: v1
    kind: ImageStream
    metadata:
      name: ${APPLICATION_NAME}
    spec:
      dockerImageRepository: ""
      tags:
      - name: latest
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      labels:
        deploymentConfig: ${APPLICATION_NAME}
      name: ${APPLICATION_NAME}
    spec:
      replicas: 1
      selector:
        deploymentConfig: ${APPLICATION_NAME}
      strategy:
        type: Recreate
      template:
        metadata:
          labels:
            deploymentConfig: ${APPLICATION_NAME}
        spec:
          containers:
          - env:
            - name: LOG_DIRECTORY
              value: ${LOG_DIRECTORY}
            - name: LOG_FILE_MATCH
              value: ${LOG_FILE_MATCH}
            - name: LOG_PRIORITY
              value: ${LOG_PRIORITY}
            image: ${APPLICATION_NAME}:latest
            imagePullPolicy: IfNotPresent
            name: ${APPLICATION_NAME}
            ports:
            - containerPort: 8080
              name: ${APPLICATION_NAME}
              protocol: TCP
            resources:
              limits:
                cpu: ${LIMIT_CPU}m
                memory: ${LIMIT_MEMORY}Mi
              requests:
                cpu: ${LIMIT_CPU}m
                memory: ${LIMIT_MEMORY}Mi
            securityContext:
              capabilities: {}
              privileged: false
            terminationMessagePath: /dev/termination-log
            volumeMounts:
            - mountPath: ${LOG_DIRECTORY}
              name: log
          dnsPolicy: ClusterFirst
          restartPolicy: Always
          terminationGracePeriodSeconds: 10
          volumes:
          - emptyDir: {}
            name: log
      triggers:
      - imageChangeParams:
          automatic: true
          containerNames:
          - ${APPLICATION_NAME}
          from:
            kind: ImageStreamTag
            name: ${APPLICATION_NAME}:latest
        type: ImageChange
      - type: ConfigChange
  - apiVersion: v1
    kind: Route
    metadata:
      annotations:
        description: 应用访问入口
      name: ${APPLICATION_NAME}-route
    spec:
      host: ${APPLICATION_HOSTNAME}
      to:
        kind: Service
        name: ${APPLICATION_NAME}-http
  - apiVersion: v1
    kind: Service
    metadata:
      annotations:
        description: 应用的web服务
      name: ${APPLICATION_NAME}-http
    spec:
      ports:
      - port: 8080
        protocol: TCP
        targetPort: 8080
      selector:
        deploymentConfig: ${APPLICATION_NAME}
  parameters:
  - description: 使用镜像名
    name: IMAGE_NAME
    value: ghoul008/tomcat8:testgradle
  - description: 应用名称
    name: APPLICATION_NAME
    value: tomcat-gradle
  - description: '应用访问域名. 若为空则由系统生成, e.g.: <application-name>.<project>.<default-domain-suffix>'
    name: APPLICATION_HOSTNAME
  - description: git源码路径
    name: GIT_URI
  - description: 选用的git分支，默认为master
    name: GIT_REF
    value: master
  - description: gitlab webhook授权密码，由8位数字字母组成，若为空则由系统生成
    from: '[a-zA-Z0-9]{8}'
    generate: expression
    name: Generic_TRIGGER_SECRET
  - description: 应用健康检查
    name: HEALTH_CHECK
    value: /gradle-demo-1.0/hello
  - description: 设置最大cpu资源,单位MHZ.
    name: LIMIT_CPU
    required: true
    value: "500"
  - description: 设置最大内存资源,单位MiB.
    name: LIMIT_MEMORY
    required: true
    value: "1024"
  - description: 日志监控路径
    name: LOG_DIRECTORY
    required: true
    value: /data/tomcat/apache-tomcat-8.0.33/logs
  - description: 日志监控file_match
    name: LOG_FILE_MATCH
    required: true
    value: catalina
  - description: 日志监控priority
    name: LOG_PRIORITY
    required: true
    value: 2016-06-02
- apiVersion: v1
  kind: Template
  labels:
    application: tomcat-maven
    createdBy: lidt
  metadata:
    annotations:
      description: jre8+tomcat8+maven3环境
      iconClass: icon-tomcat
    creationTimestamp: 2017-10-31T02:40:08Z
    name: tomcat-maven
    namespace: openshift
    resourceVersion: "27449236"
    selfLink: /oapi/v1/namespaces/openshift/templates/tomcat-maven
    uid: cf597e4e-bde4-11e7-bc85-ecf4bbe2f144
  objects:
  - apiVersion: v1
    kind: BuildConfig
    metadata:
      name: ${APPLICATION_NAME}
    spec:
      output:
        to:
          kind: ImageStreamTag
          name: ${APPLICATION_NAME}:latest
      source:
        git:
          ref: ${GIT_REF}
          uri: ${GIT_URI}
        sourceSecret:
          name: gitbasicsecret
        type: Git
      strategy:
        dockerStrategy:
          from:
            kind: DockerImage
            name: 10.1.245.137/tomcat-maven:latest
          pullSecret:
            name: dockerhub
        type: Docker
      triggers:
      - generic:
          secret: ${Generic_TRIGGER_SECRET}
        type: Generic
  - apiVersion: v1
    kind: ImageStream
    metadata:
      name: ${APPLICATION_NAME}
    spec:
      dockerImageRepository: ""
      tags:
      - name: latest
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      labels:
        deploymentConfig: ${APPLICATION_NAME}
      name: ${APPLICATION_NAME}
    spec:
      replicas: 1
      selector:
        deploymentConfig: ${APPLICATION_NAME}
      strategy:
        recreateParams:
          pre:
            execNewPod:
              command:
              - /usr/local/tomcat/bin/startup.sh
              containerName: ${APPLICATION_NAME}
              env: []
            failurePolicy: Abort
        type: Recreate
      template:
        metadata:
          labels:
            deploymentConfig: ${APPLICATION_NAME}
        spec:
          containers:
          - env: []
            image: ${APPLICATION_NAME}:latest
            imagePullPolicy: IfNotPresent
            name: ${APPLICATION_NAME}
            ports:
            - containerPort: 8080
              name: ${APPLICATION_NAME}
              protocol: TCP
            readinessProbe:
              failureThreshold: 3
              httpGet:
                path: ${HEALTH_CHECK}
                port: 8080
              initialDelaySeconds: 30
              periodSeconds: 10
              successThreshold: 1
              timeoutSeconds: 5
            resources:
              limits:
                cpu: ${LIMIT_CPU}m
                memory: ${LIMIT_MEMORY}Mi
              requests:
                cpu: ${LIMIT_CPU}m
                memory: ${LIMIT_MEMORY}Mi
            securityContext:
              capabilities: {}
              privileged: false
            terminationMessagePath: /dev/termination-log
          dnsPolicy: ClusterFirst
          restartPolicy: Always
          terminationGracePeriodSeconds: 10
      triggers:
      - imageChangeParams:
          automatic: true
          containerNames:
          - ${APPLICATION_NAME}
          from:
            kind: ImageStreamTag
            name: ${APPLICATION_NAME}:latest
        type: ImageChange
  - apiVersion: v1
    kind: Route
    metadata:
      annotations:
        description: 应用访问入口
      name: ${APPLICATION_NAME}-route
    spec:
      host: ${APPLICATION_HOSTNAME}
      to:
        kind: Service
        name: ${APPLICATION_NAME}-http
  - apiVersion: v1
    kind: Service
    metadata:
      annotations:
        description: 应用的web服务
      name: ${APPLICATION_NAME}-http
    spec:
      ports:
      - port: 8080
        protocol: TCP
        targetPort: 8080
      selector:
        deploymentConfig: ${APPLICATION_NAME}
  parameters:
  - description: 应用名称
    name: APPLICATION_NAME
    value: tomcat-maven
  - description: '应用访问域名. 若为空则由系统生成, e.g.: <application-name>.<project>.<default-domain-suffix>'
    name: APPLICATION_HOSTNAME
  - description: git源码路径
    name: GIT_URI
  - description: 选用的git分支，默认为master
    name: GIT_REF
    value: master
  - description: gitlab webhook授权密码，由8位数字字母组成，若为空则由系统生成
    from: '[a-zA-Z0-9]{8}'
    generate: expression
    name: Generic_TRIGGER_SECRET
  - description: 应用健康检查
    name: HEALTH_CHECK
    required: true
    value: /health
  - description: 设置最大cpu资源,单位MHZ.
    name: LIMIT_CPU
    required: true
    value: "500"
  - description: 设置最大内存资源,单位MiB.
    name: LIMIT_MEMORY
    required: true
    value: "1024"
- apiVersion: v1
  kind: Template
  labels:
    app: zeppelin
    template: zeppelin-persistent
  metadata:
    annotations:
      description: Create a zeppelin node, with persistent storage.
      iconClass: icon-database
      openshift.io/display-name: Zeppelin (Persistent)
      tags: computing,zeppelin
    creationTimestamp: 2017-10-18T02:53:22Z
    name: zeppelin-persistent
    namespace: openshift
    resourceVersion: "27449263"
    selfLink: /oapi/v1/namespaces/openshift/templates/zeppelin-persistent
    uid: 8116ff3c-b3af-11e7-9b95-ecf4bbe2f144
  objects:
  - apiVersion: v1
    kind: Service
    metadata:
      labels:
        name: ${NAME}
      name: zeppelin
    spec:
      ports:
      - name: httpport
        port: 8090
        protocol: TCP
        targetPort: 8090
      selector:
        name: ${NAME}
  - apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      annotations:
        volume.beta.kubernetes.io/storage-class: nfs
        volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/glusterfs
      labels:
        name: ${NAME}
      name: ${NAME}
    spec:
      accessModes:
      - ReadWriteMany
      resources:
        requests:
          storage: ${PV_SIZE}Gi
  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      labels:
        name: ${NAME}
      name: ${NAME}
    spec:
      replicas: 1
      selector:
        name: ${NAME}
      strategy:
        type: Rolling
      template:
        metadata:
          labels:
            name: ${NAME}
        spec:
          containers:
          - env:
            - name: ZEPPELIN_LOG_DIR
              value: /nfs/logs
            - name: ZEPPELIN_NOTEBOOK_DIR
              value: /nfs/notebook
            - name: ZEPPELIN_PORT
              value: "8090"
            image: ${SOURCE_IMAGE}
            imagePullPolicy: Always
            livenessProbe:
              initialDelaySeconds: 60
              tcpSocket:
                port: 8090
              timeoutSeconds: 5
            name: ${NAME}
            ports:
            - containerPort: 8090
              protocol: TCP
            resources:
              limits:
                cpu: ${RESOURCE_CPU_LIMIT}
                memory: ${RESOURCE_MEMORY_LIMIT}G
              requests:
                cpu: 300m
                memory: 300Mi
            securityContext:
              privileged: true
              runAsUser: 0
            terminationMessagePath: /dev/termination-log
            volumeMounts:
            - mountPath: /nfs
              name: datadir
          terminationGracePeriodSeconds: 10
          volumes:
          - name: datadir
            persistentVolumeClaim:
              claimName: ${NAME}
      triggers:
      - type: ConfigChange
  parameters:
  - description: Name.
    name: NAME
    required: true
    value: zeppelin
  - description: Container image source.
    name: SOURCE_IMAGE
    required: true
    value: apache/zeppelin:0.7.3
  - description: Data capacity GB.
    name: PV_SIZE
    required: true
    value: "5"
  - description: The limits for memory resource.GB
    name: RESOURCE_MEMORY_LIMIT
    value: "4"
  - description: The limits for CPU resource.
    name: RESOURCE_CPU_LIMIT
    value: "2"
- apiVersion: v1
  kind: Template
  labels:
    component: zk
    template: zk
  metadata:
    annotations:
      description: An example of a replicated Zookeeper server
      iconClass: icon-database
      openshift.io/display-name: Zookeeper
      tags: database,zookeeper
    creationTimestamp: 2017-09-07T08:59:20Z
    name: zookeeper
    namespace: openshift
    resourceVersion: "27912650"
    selfLink: /oapi/v1/namespaces/openshift/templates/zookeeper
    uid: d6335af4-93aa-11e7-a1ba-ecf4bbe2f144
  objects:
  - apiVersion: v1
    kind: Service
    metadata:
      labels:
        app: ${NAME}
      name: zk
    spec:
      clusterIP: None
      ports:
      - name: client
        port: ${ZOO_CLIENT_PORT}
      - name: server
        port: ${ZOO_SERVER_PORT}
      - name: election
        port: ${ZOO_ELECTION_PORT}
      selector:
        app: ${NAME}
  - apiVersion: apps/v1beta1
    kind: StatefulSet
    metadata:
      labels:
        app: ${NAME}
      name: zk
    spec:
      replicas: ${ZOO_REPLICAS}
      serviceName: zk
      template:
        metadata:
          labels:
            app: ${NAME}
        spec:
          affinity:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: app
                      operator: In
                      values:
                      - ${NAME}
                  topologyKey: kubernetes.io/hostname
                weight: 100
          containers:
          - env:
            - name: ZOO_REPLICAS
              value: ${ZOO_REPLICAS}
            - name: ZK_HEAP_SIZE
              value: ${ZOO_HEAP_SIZE}
            - name: ZK_tickTime
              value: ${ZOO_TICK_TIME}
            - name: ZK_initLimit
              value: ${ZOO_INIT_LIMIT}
            - name: ZK_syncLimit
              value: ${ZOO_SYNC_LIMIT}
            - name: ZK_maxClientCnxns
              value: ${ZOO_MAX_CLIENT_CNXNS}
            - name: ZK_autopurge_snapRetainCount
              value: ${ZOO_SNAP_RETAIN_COUNT}
            - name: ZK_autopurge_purgeInterval
              value: ${ZOO_PURGE_INTERVAL}
            - name: ZK_clientPort
              value: ${ZOO_CLIENT_PORT}
            - name: ZOO_SERVER_PORT
              value: ${ZOO_SERVER_PORT}
            - name: ZOO_ELECTION_PORT
              value: ${ZOO_ELECTION_PORT}
            - name: JAVA_ZK_JVMFLAG
              value: ${ZOO_HEAP_SIZE}
            image: ${SOURCE_IMAGE}
            imagePullPolicy: IfNotPresent
            livenessProbe:
              exec:
                command:
                - zk_status.sh
              initialDelaySeconds: 35
              timeoutSeconds: 10
            name: ${NAME}
            ports:
            - containerPort: ${ZOO_CLIENT_PORT}
              name: client
            - containerPort: ${ZOO_SERVER_PORT}
              name: server
            - containerPort: ${ZOO_ELECTION_PORT}
              name: election
            readinessProbe:
              exec:
                command:
                - zk_status.sh
              initialDelaySeconds: 15
              timeoutSeconds: 5
            resources:
              limits:
                cpu: ${RESOURCE_CPU_LIMIT}
                memory: ${RESOURCE_MEMORY_LIMIT}G
              requests:
                cpu: 300m
                memory: 300M
            securityContext:
              fsGroup: 1000
              runAsUser: 1000
            volumeMounts:
            - mountPath: /var/lib/zookeeper
              name: datadir
          terminationGracePeriodSeconds: 10
      volumeClaimTemplates:
      - metadata:
          annotations:
            volume.alpha.kubernetes.io/storage-class: slow
          labels:
            app: ${NAME}
          name: datadir
        spec:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: ${VOLUME_CAPACITY}Gi
  parameters:
  - name: NAME
    required: true
    value: zk
  - description: Container image source (by default from "engapa" dockerhub account)
    name: SOURCE_IMAGE
    value: engapa/zookeeper:3.4.10
  - description: Number of nodes
    name: ZOO_REPLICAS
    required: true
    value: "1"
  - description: Persistent volume capacity per pod, e.g. 512Mi, 2Gi
    name: VOLUME_CAPACITY
    required: true
    value: "2"
  - description: The number of milliseconds of each tick
    name: ZOO_TICK_TIME
    required: true
    value: "6000"
  - description: The number of ticks that the initial synchronization phase can take
    name: ZOO_INIT_LIMIT
    required: true
    value: "5"
  - description: The number of ticks that can pass between sending a request and getting
      an acknowledgement
    name: ZOO_SYNC_LIMIT
    required: true
    value: "2"
  - description: The port at which the clients will connect
    name: ZOO_CLIENT_PORT
    required: true
    value: "2181"
  - description: Server Port
    name: ZOO_SERVER_PORT
    required: true
    value: "2888"
  - description: Election Port
    name: ZOO_ELECTION_PORT
    required: true
    value: "3888"
  - description: The maximum number of client connections
    name: ZOO_MAX_CLIENT_CNXNS
    required: true
    value: "60"
  - description: The number of snapshots to retain in dataDir
    name: ZOO_SNAP_RETAIN_COUNT
    required: true
    value: "3"
  - description: Purge task interval in hours. Set to 0 to disable auto purge feature
    name: ZOO_PURGE_INTERVAL
    required: true
    value: "1"
  - description: JVM heap size
    displayName: JVM heap size
    name: ZOO_HEAP_SIZE
    required: true
    value: -Xmx1512M -Xms1512M
  - description: The limits for memory resource.
    name: RESOURCE_MEMORY_LIMIT
    value: "2"
  - description: The limits for CPU resource.
    name: RESOURCE_CPU_LIMIT
    value: "2"
kind: List
metadata: {}
resourceVersion: ""
selfLink: ""
